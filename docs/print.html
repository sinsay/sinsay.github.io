<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>SinSay's Note Book</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <base href="">

        <link rel="stylesheet" href="book.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <link rel="shortcut icon" href="favicon.png">

        <!-- Font Awesome -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">

        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        

    </head>
    <body class="light">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { } 
            if (theme === null || theme === undefined) { theme = 'light'; }
            document.body.className = theme;
            document.querySelector('html').className = theme + ' js';
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <ol class="chapter"><li><a href="about.html"><strong aria-hidden="true">1.</strong> About Me</a></li><li><a href="tokio.html"><strong aria-hidden="true">2.</strong> Tokio Tutirial</a></li><li><ol class="section"><li><a href="tokio/tokio - tutorial - hello.html"><strong aria-hidden="true">2.1.</strong> Hello</a></li><li><a href="tokio/tokio - tutorial - spawning.html"><strong aria-hidden="true">2.2.</strong> Spawning</a></li><li><a href="tokio/tokio - tutorial - shared state.html"><strong aria-hidden="true">2.3.</strong> Shared State</a></li><li><a href="tokio/tokio - tutorial - channel.html"><strong aria-hidden="true">2.4.</strong> Channel</a></li><li><a href="tokio/tokio - tutorial - io.html"><strong aria-hidden="true">2.5.</strong> I/O</a></li><li><a href="tokio/tokio - tutorial - framing.html"><strong aria-hidden="true">2.6.</strong> Framing</a></li></ol></li><li><a href="data_struct.html"><strong aria-hidden="true">3.</strong> DataStruct</a></li><li><ol class="section"><li><a href="redis/sds.html"><strong aria-hidden="true">3.1.</strong> sds</a></li><li><a href="redis/dict.html"><strong aria-hidden="true">3.2.</strong> dict</a></li><li><a href="redis/skiplist.html"><strong aria-hidden="true">3.3.</strong> skiplist</a></li><li><a href="redis/intset.html"><strong aria-hidden="true">3.4.</strong> intset</a></li><li><a href="redis/ziplist.html"><strong aria-hidden="true">3.5.</strong> ziplist</a></li></ol></li><li><a href="redis.html"><strong aria-hidden="true">4.</strong> Redis</a></li><li><ol class="section"><li><a href="redis/1. Redis 基本定义.html"><strong aria-hidden="true">4.1.</strong> 基本定义</a></li><li><a href="redis/2. Redis 分析起步.html"><strong aria-hidden="true">4.2.</strong> 分析起步</a></li><li><a href="redis/3. Redis 请求处理.html"><strong aria-hidden="true">4.3.</strong> 请求处理</a></li><li><a href="redis/4. Redis 执行命令.html"><strong aria-hidden="true">4.4.</strong> 执行命令</a></li></ol></li><li><a href="distributed.html"><strong aria-hidden="true">5.</strong> Distributed</a></li><li><ol class="section"><li><a href="distributed/mapreduce_note.html"><strong aria-hidden="true">5.1.</strong> MapReduce</a></li><li><a href="distributed/raft_note.html"><strong aria-hidden="true">5.2.</strong> Raft</a></li></ol></li></ol>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar" class="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div class="left-buttons">
                            <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                                <i class="fa fa-bars"></i>
                            </button>
                            <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                                <i class="fa fa-paint-brush"></i>
                            </button>
                            <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                                <li role="none"><button role="menuitem" class="theme" id="light">Light <span class="default">(default)</span></button></li>
                                <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            </ul>
                            
                            <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                                <i class="fa fa-search"></i>
                            </button>
                            
                        </div>

                        <h1 class="menu-title">SinSay's Note Book</h1> 

                        <div class="right-buttons">
                            <a href="print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button" class="fa fa-print"></i>
                            </a>
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <a class="header" href="print.html#about-me" id="about-me"><h1>About Me</h1></a>
<p>This has some record of my learning.
I will fill it soon!</p>
<a class="header" href="print.html#contact-me" id="contact-me"><h2>Contact Me</h2></a>
<p>for now, if you wanna discuss with me or have some issue for me, contact me with the following ways: <br/></p>
<p>wechat: SinSay</p>
<p>mail: SinSayChen@gmail.com</p>
<a class="header" href="print.html#tokio-tutirial" id="tokio-tutirial"><h1>Tokio Tutirial</h1></a>
<a class="header" href="print.html#hello-tokio" id="hello-tokio"><h1>Hello Tokio</h1></a>
<p>我们从写一个最基础的的 <code>Tokio</code> 程序开始，这个程序会连接到 <code>MiniRedis</code> 的服务端，然后设置一个 <code>key</code> 为 <code>hello</code>，<code>value</code> 为 <code>world</code> 的键值对，然后再把这个键值对读取回来。这些操作我们会使用名为 <code>Mini-Redis</code> 的客户端库来完成。</p>
<a class="header" href="print.html#the-code" id="the-code"><h2>The Code</h2></a>
<a class="header" href="print.html#a创建一个新程序-generate-a-new-crate" id="a创建一个新程序-generate-a-new-crate"><h3>创建一个新程序 Generate a new crate</h3></a>
<p>我们从创建一个新的 <code>Rust</code> 程序开始</p>
<pre><code class="language-shell">cargo new my-redis
cd my-redis
</code></pre>
<a class="header" href="print.html#a添加依赖-add-dependencies" id="a添加依赖-add-dependencies"><h3>添加依赖 Add dependencies</h3></a>
<p>接下来，打开 <code>Cargo.toml</code> ，并在 <code>[dependencies]</code> 后添加下面的代码</p>
<pre><code class="language-toml">tokio = { version = &quot;1&quot;, features = [&quot;full&quot;] }
mini-redis = &quot;0.4&quot;
</code></pre>
<a class="header" href="print.html#a开始写代码-write-the-code" id="a开始写代码-write-the-code"><h3>开始写代码 Write the code</h3></a>
<p>然后，打开 <code>main.rs</code> 并用下面的代码替换文件的内容</p>
<pre><pre class="playpen"><code class="language-rust">use mini_redis::{client, Result};

#[tokil::main]
pub async fn main() -&gt; Result&lt;()&gt; {
  // Open a connection to the mini-redis address.
  let mut client = client::connect(&quot;127.0.0.1:6379&quot;).await?;
  
  // Set the key &quot;hello&quot; with value &quot;world&quot;
  client.set(&quot;hello&quot;, &quot;world&quot;.into()).await?;
  
  // Get key &quot;hello&quot;
  let result = client.get(&quot;hello&quot;).await?;
  
  println!(&quot;got value from the server; result={:?}&quot;, result);
  
  Ok(())
}
</code></pre></pre>
<p>为了确保 <code>Mini-Redis</code> 的服务端处于运行状态，我们打开一个终端窗口，运行如下命令:</p>
<pre><code class="language-bash">mini-redis-server
</code></pre>
<p>接下来运行我们的 <code>mini-redis</code> 程序</p>
<pre><code class="language-bash">$ cargo run
got value from the server; result=Some(b&quot;world)
</code></pre>
<p>成功了！</p>
<blockquote>
<p>你可以从 <a href="https://github.com/tokio-rs/website/blob/master/tutorial-code/hello-tokio/src/main.rs">这里</a> 找到完整的源码.</p>
</blockquote>
<a class="header" href="print.html#break-it-down" id="break-it-down"><h2>Break it down</h2></a>
<p>接下来花点时间梳理下我们刚才做的事情。代码并不多，但其中却触发了许多的事情。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
let mut client = client::connect(&quot;127.0.0.1:6379&quot;).await?;
#}</code></pre></pre>
<p>函数 <code>client::connect</code> 是 <code>mini-redis</code> 这个包所提供的，他会使用指定的地址来异步的创建一个 <strong>TCP</strong> 连接，当这个连接建立成功时， <code>client</code> 则保存了该函数返回的结果。尽管这个操作是异步发生的，但代码 <strong>看起来</strong> 却是同步的。其中唯一指示了该操作为异步的只有 <code>.await</code> 操作符。</p>
<a class="header" href="print.html#a什么是异步编程-what-is-asynchronous-programming" id="a什么是异步编程-what-is-asynchronous-programming"><h3>什么是异步编程 What is asynchronous programming?</h3></a>
<p>大部分的电脑程序都按照他们代码所写的顺序执行，最前面的先执行，然后是下一行，然后一直执行下去。在同步编程中，当程序遇到了一个无法立即完成的操作时，他会堵塞在该位置一直到操作完成，举个例子，在创建 <strong>TCP</strong> 连接时连接双方需要在网络中交换一些信息，交换信息的操作需要花费相当的时间，而运行这段代码的线程在这个时间内将被阻塞。</p>
<p>在异步编程中，如果一个操作不能马上完成的话，他将被暂停然后切换到后台等待，执行的线程不会被阻塞，因此他可以继续执行其他的事情。当这个操作完成时，他又会被切换至前台并从之前中断的地方继续执行。我们刚刚实现的示例只启动了一个任务，所以在这个任务的操作被暂停时并没有发生任何其他的事情，但通常异步的编程会同时运行许多的任务。</p>
<p>尽管异步编程能够给我们带来更快的程序，与此同时他也为程序带来了更高的复杂度。开发人员为了能够在异步操作完成时将任务重新恢复执行，需要去跟进任务的运行状态。从历史经验上来看，这是一个乏味并且非常容易出错的工作。</p>
<a class="header" href="print.html#a编译时的绿色线程-compile-time-green-threading" id="a编译时的绿色线程-compile-time-green-threading"><h3>编译时的绿色线程 Compile-time green-threading</h3></a>
<p><strong>Rust</strong> 使用了 <code>async/await</code> 特性来实现了异步编程的功能。会执行异步操作的函数通过 <code>async</code> 关键字进行标识，在我们的示例中， <code>connect</code> 函数进行了如下的定义：</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use mini_redis::Result;
use mini_redis::client:Client;
use tokio::net::ToSocketAddrs;

pub async fn connect&lt;T: ToSocketAddrs&gt;(addr: T) -&gt; Result&lt;Client&gt; {
  // ...
}
#}</code></pre></pre>
<p><code>async fn</code> 的定义跟同步函数很类似，但他以异步的方式执行。<code>Rust</code> 在<strong>编译时</strong>将代码转换为异步的操作，所有使用 <code>.await</code> 调用并定义为 <code>async fn</code> 的操作将让出线程的执行权。这样该线程就能在异步操作被放到后台的期间做其他的事情。</p>
<blockquote>
<p>尽管也有一些其他的编程语言实现 <code>async/await</code> 的特性，但 <code>Rust</code> 使用了一个独立的方式，最主要的一点是 <code>Rust</code> 的异步操作是 <code>lazy</code> 的。这导致了运行时的语义跟其他编程语言的产生了区别。</p>
</blockquote>
<p>如果到现在还没弄得很明白，不用担心，我们还会继续在接下来的篇幅中探讨 <code>async/await</code>。</p>
<a class="header" href="print.html#a使用-asyncawait-using-asyncawait" id="a使用-asyncawait-using-asyncawait"><h3>使用 <code>async/await</code> Using <code>async/await</code></h3></a>
<p>异步函数能够与普通的 <code>Rust</code> 函数一样使用。但是，调用这些函数不意味着执行这些函数，调用 <code>async fn</code> 类型的函数返回的是一个代表该操作的标识。在概念上他跟一个无参的闭包函数类型。为了能够真正的执行它，你需要在函数返回的标识上使用 <code>.await</code> 操作。</p>
<p>我们来看看下面的例子</p>
<pre><pre class="playpen"><code class="language-rust">async fn say_world() {
  println!(&quot;world&quot;);
}

#[tokio::main]
async fn main() {
  // Calling `say_world()` does not execute the body of `say_world()`
  let op = say_hello();
  
  // This println! comes first
  println!(&quot;hello&quot;);
  
  // Calling `.await` on `op` starts executing `say_world`.
  op.await;
}
</code></pre></pre>
<p>输出</p>
<pre><code class="language-shell">hello
world
</code></pre>
<p><code>async fn</code> 函数的返回结果是一个实现了 <code>Future</code> trait 的匿名类型。</p>
<blockquote>
<p>所以这里到底是怎么执行的，还得看 <code>Rust</code> 最终转换出的代码及 <code>Future</code> 的定义，后续我会单独细讲</p>
</blockquote>
<a class="header" href="print.html#a异步的-main-函数-async-main-function" id="a异步的-main-函数-async-main-function"><h3>异步的 <code>main</code> 函数 Async <code>main</code> function</h3></a>
<p>用来启动程序的 <code>main</code> 函数其他普通的 <code>Rust</code> 程序的有所不同：</p>
<ol>
<li>被定义为 <code>async fn</code></li>
<li>添加了 <code>#[tokio::main]</code> 宏</li>
</ol>
<p><code>async fn</code> 函数在我们需要执行异步操作的上下文中被使用。然而，异步函数需要通过 <code>runtime</code> 来运行，<code>runtime</code> 中包含异步任务的调度器，他提供了事件驱动的 <strong>I/O</strong>、定时器等。<code>runtime</code> 并不会自动的运行，所以需要在主函数中运行它。</p>
<p>我们在 <code>async fn main()</code> 函数中添加的 <code>#[tokio::main]</code> 宏会将其转换为同步的 <code>fn main()</code> 函数，该函数会初始化 <code>runtime</code> 并执行我们定义的异步的 <code>main</code> 函数。</p>
<p>比如</p>
<pre><pre class="playpen"><code class="language-rust">#[tokio::main]
async fn main() {
  println!(&quot;hello&quot;);
}
</code></pre></pre>
<p>会被转换为</p>
<pre><pre class="playpen"><code class="language-rust">fn main() {
  let mut rt = tokio::runtime::Runtime::new().unwrap();
  rt.block_on(async {
    println!(&quot;hello&quot;);
  })
}
</code></pre></pre>
<p>Tokio 中具体的 <code>runtime</code> 的细节在后续的章节中会补充。</p>
<a class="header" href="print.html#cargo-特性-cargo-features" id="cargo-特性-cargo-features"><h3>Cargo 特性 Cargo features</h3></a>
<p>我们在定义对 Tokio 的依赖时使用 <code>full</code> 特性。</p>
<pre><code class="language-toml">tokio = { version = &quot;1&quot;, features = [&quot;full&quot;] }
</code></pre>
<p>Tokio 提供了大量的功能 (TCP, UDP, Unix sockets, Timers, sync utilities, multiple scheduler types 等)，但并不是所有的程序都需要用到这么多的功能。在需要缩短编译时间或减小程序大小时，可以只选择所需的特性。</p>
<p>现在的话，还是继续使用 <code>full</code> 特性吧。</p>
<a class="header" href="print.html#spawning" id="spawning"><h1>Spawning</h1></a>
<p>我们要开始换挡加速开始学习 Redis 服务端了。</p>
<p>首先，将我们上一节写的 <code>Set</code>/<code>Get</code> 代码移到示例目录 <code>examples</code> , 这样我们可以让他跟服务端代码一起运行。</p>
<pre><code class="language-shell">mkdir -p examples
mv src/main.rs examples/hello-redis.rs
</code></pre>
<p>接下来创建一个新的 <code>src/main.rs</code> 然后继续。</p>
<a class="header" href="print.html#accepting-sockets" id="accepting-sockets"><h2>Accepting Sockets</h2></a>
<p>我们的 Redis 服务端第一步需要做的是接收一个 TCP 套接字，这个操作通过 <code>tokio::net::TcpListener</code> 完成。</p>
<blockquote>
<p>Tokio 大部分的的类型名称都定义成跟 <code>Rust</code> 标准库中同步类型一样。在有必要的情况下，Tokio 会提供与标准库中该类型遗憾的函数，但是以 <code>async fn</code> 的形式。</p>
</blockquote>
<p><code>TcpListener</code> 绑定到了 <strong>6379</strong> 端口，套接字则会在循环中被接收，每个套接字都会在处理完之后关闭。就目前而言，我们会从中读取命令打印到标准输出，然后返回一个错误。</p>
<pre><pre class="playpen"><code class="language-rust">use mini_redis::{Connection, Frame};
use tokio::net::{TcpListener, TcpStream};

#[tokio::main]
async fn main() {
    let listener = TcpListener::bind(&quot;127.0.0.1:6379&quot;).await.unwrap();

    loop {
        // The second item contains the IP and Port or the new connection
        let (socket, _) = listener.accept().await.unwrap();
        process(socket).await;
    }
}

async fn process(socket: TcpStream) {
    // The `Connection` lets us read/write redis **frame** instead of
    // byte streams. The `Connection` type is defined by mini-redis
    let mut connection = Connection::new(socket);
    if let Some(frame) = connection.read_frame().await.unwrap() {
        println!(&quot;GOT: {:?}&quot;, frame);

        // Response with an error
        let response = Frame::Error(&quot;unimplemented&quot;.to_string());
        connection.write_frame(&amp;response);
    }
}
</code></pre></pre>
<p>接下来，启动这个接收循环</p>
<pre><code class="language-shell">$ cargo run
</code></pre>
<p>接下来在一个独立的终端窗口，启动 <code>hello-redis</code> 示例 (上一节实现的<code>SET</code>/<code>GET</code> )</p>
<pre><code class="language-shell">$ cargo run --example hello-redis
</code></pre>
<p>输出为</p>
<pre><code class="language-shell">Error: &quot;unimplemented&quot;
</code></pre>
<p>在服务端的终端输出如下：</p>
<pre><code class="language-shell">GOT: Array([Bulk(b'set'), Bulk(b'hello'), Bulk(b'world')])
</code></pre>
<a class="header" href="print.html#concurrency" id="concurrency"><h2>Concurrency</h2></a>
<p>我们的服务端还有一个小问题*(除了返回错误)*，他每次只能处理一个请求。当接收了一个连接后，服务端会在当前循环中一直堵塞到完全把返回信息写到套接字中。</p>
<p>我们希望 Redis 服务能够同时处理多个请求，所以我们需要让他并发 <em>(Concurrenty)</em> 起来。</p>
<blockquote>
<p>并发跟并行并不是同一种概念。如果你能够交替着执行两个任务，那你这两个任务可以说是并发但不是并行的。为了能够让他并发起来，你需要两个人，每个人各自处理一个任务。</p>
<p>使用 Tokio 的一个好处就是异步的代码让你能够在不使用多线程的前提下让多个任务并发执行。事实上，Tokio 能够在单线程中并发运行非常多的任务！</p>
</blockquote>
<p>为了能够并发的处理连接，我们需要为每个到达的连接创建一个新的任务，然后让这个任务负责处理该连接。</p>
<p>接收连接的循环现在变成了这样：</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::net::TcpListener;

#[tokio::main]
async fn main() {
  let listener = TcpListener::bind(&quot;127.0.0.1:6379&quot;)
  
  loop {
    let (socket, _) = listerner.accept().await.unwrap();
    // A new task is spqwned for each inbound socket. the socket is
    // moved to the new task and processed there.
    tokio::spawn(async move {
      process(socket).await;
    });
  }
}
</code></pre></pre>
<a class="header" href="print.html#tasks" id="tasks"><h3>Tasks</h3></a>
<p>Tokio 的任务是异步的绿色线程，他通过传递给 <code>tokio::spawn</code> 的 <code>async</code> 语句块创建，这个函数接收 <code>async</code> 语句块后返回一个 <code>JoinHandle</code>，调用者则通过 <code>JoinHandle</code> 与创建的任务交互。有些传递的 <code>async</code> 语句块是具有返回值的，调用者通过 <code>JoinHandle</code> 的 <code>.await</code> 来获取其返回值，</p>
<pre><pre class="playpen"><code class="language-rust">#[tokio::main]
async fn main() {
  let handle = tokio::spawn(async {
    &quot;return value&quot;
  });
  
  // Do some other work
  
  let out = handle.await.unwrap();
  println!(&quot;GOT {}&quot;, out);
}
</code></pre></pre>
<p>在 <code>JoinHandle</code> 上执行 <code>.await</code> 等待会得到一个 <code>Result</code>。当任务在执行时遇到了错误时，<code>JoinHandle</code> 会返回 <code>Err</code> ，这会在任务发生错误，或是因为 <code>Runtime</code> 被强制关闭而导致任务被强制取消时产生。</p>
<p>任务在 Tokio 中是非常轻量的，实际上他只会需要申请一次 64 个字节的内存。所以程序可以轻松的产生成千上万的任务。</p>
<a class="header" href="print.html#static-bound" id="static-bound"><h3><code>'static</code> bound</h3></a>
<p>当你通过 Tokio 的 <code>Runtime</code> 创建一个任务时，这个任务的类型必须是 <code>'static'</code> 的。这意味着被创建的任务不能够包含对任务以外任何数据的引用。</p>
<blockquote>
<p>有一个常见的误解是 <code>'static</code>  始终代表着 &quot;lives forever&quot;, 一直存活。但在这个场景中并不是，标识为 <code>'static</code> 的值只是意味着他不会产生内存泄漏。具体的可以通过 <a href="https://github.com/pretzelhammer/rust-blog/blob/master/posts/common-rust-lifetime-misconceptions.md#2-if-t-static-then-t-must-be-valid-for-the-entire-program">Common Rust Lisetime Misconceptions</a> 进行了解。</p>
</blockquote>
<p>举个例子，下面的代码无法通过编译</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::task;

#[tokio::main]
async fn main() {
  let v = vec![1, 2, 3];
  
  task.spawn(async {
    println!(&quot;Here's a vec: {:?}&quot;, v);
  });
}
</code></pre></pre>
<p>尝试编译的话，会得到下面的错误信息：</p>
<pre><code class="language-shell">error[E0373]: async block may outlive the current function, but
              it borrows `v`, which is owned by the current function
 --&gt; src/main.rs:7:23
  |
7 |       task::spawn(async {
  |  _______________________^
8 | |         println!(&quot;Here's a vec: {:?}&quot;, v);
  | |                                        - `v` is borrowed here
9 | |     });
  | |_____^ may outlive borrowed value `v`
  |
note: function requires argument type to outlive `'static`
 --&gt; src/main.rs:7:17
  |
7 |       task::spawn(async {
  |  _________________^
8 | |         println!(&quot;Here's a vector: {:?}&quot;, v);
9 | |     });
  | |_____^
help: to force the async block to take ownership of `v` (and any other
      referenced variables), use the `move` keyword
  |
7 |     task::spawn(async move {
8 |         println!(&quot;Here's a vec: {:?}&quot;, v);
9 |     });
  |
</code></pre>
<p>这些错误信息源自于在默认条件下，变量并不会 <strong>moved</strong> 到异步的代码块中，<code>v</code> 向量在这个时候仍由 <code>main</code> 函数拥有，但在 <code>println!</code> 这一行产生了对 <code>v</code> 的借用。<code>Rust</code> 的编译器的错误信息还提供了修复这个错误的建议：将第 7 行改为 <code>task::spawn(async move {</code> 能够指示编译器将变量 <code>v</code> 移动到创建的任务中，这样的话该任务就会拥有所有他所依赖的数据，让自己满足 <code>'static'</code>。</p>
<p>如果一个数据需要同时被多个任务并发的访问，那他应该使用同步机制来进行共享，比如使用 <code>Arc</code>。</p>
<p>同时我们也留意到错误信息提到参数类型的存活时间超过了 <code>'static'</code>，这个术语可能会造成困惑，因为 <code>'static</code>  的生命周期已经覆盖了整个程序了，如果参数的存活时间还超过了 <code>'static</code> 是不是意味着存在内存泄漏？对于这个疑惑的具体解释是，这里提到的超过 <code>'static</code> 生命周期的是参数的类型而不是参数的值，当参数的值被不再使用时他就会被销毁了。</p>
<p>当我们提到一个值是 <code>'static</code> 时，说他会永远存活并不意味着不正确。这点非常重要，因为编译器无法确定这个新创建的任务会存活多久，所以他能做的确保这个任务不会存活太久的方式就是让他一直都存在。</p>
<p>上面提供的链接中提到的术语 &quot;bounded by <code>'static</code>&quot; 或 &quot;its type outlives <code>'static</code>&quot; 或 &quot;the value is <code>'static</code> for <code>T: 'static</code>&quot; 都表达了同一个意思，但他们跟以 <code>&amp;'static T</code> 使用的标注是不一样的。</p>
<a class="header" href="print.html#send-bound" id="send-bound"><h3><code>Send</code> bound</h3></a>
<p>通过 <code>tokio::spawn</code> 创建的任务<strong>必须</strong>实现了 <code>Send</code> 语义，这样 Tokio 的 <code>Runtime</code> 才能在他们因为执行 <code>.await</code> 被暂停时将他们切换到不同的线程中。</p>
<p>任务在他调用 <code>.await</code> 时拥有的<strong>所有</strong>数据都是 <code>Send</code> 时满足 <code>Send</code> 的条件，这听起来有点微妙。当 <code>.await</code> 被调用时，任务会让出执行权给调度器，在他下一次被执行时则从上一次让出的位置开始。为了能实现这个机制，所有的状态信息都会在执行的线程间进行转移，这样任务才能在执行线程间转移。相反，如果他的状态不满足 <code>Send</code>，那他就不满足作为一个任务的条件。</p>
<p>举个正常运行的例子</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::task::yield_now;
use std::rc::Rc;

#[tokio::main]
async fn main() {
  tokio::spawn(async {
    {
      let rc = Rc::new(&quot;hello&quot;);
      println!(&quot;{}&quot;, rc);
    }
    
    // `rc` is no longer used. It is **not** persisted when
    // the task yields to the scheduler
    yield_now().await;
  });
}
</code></pre></pre>
<p>下面的例子则无法编译</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::task::yield_now;
use std::rc::Rc;

#[tokio::main]
async fn main() {
  tokio::spawn(async {
    let rc = Rc::new(&quot;hello&quot;);
    
    // `rc` is used after `.await`. It must be persisted to
    // the task's state.
    yield_now().await;
    
    println!(&quot;{}&quot;, rc);
  });
}
</code></pre></pre>
<p>尝试编译时，会产生下面的错误信息</p>
<pre><code class="language-shell">error: future cannot be sent between threads safely
   --&gt; src/main.rs:6:5
    |
6   |     tokio::spawn(async {
    |     ^^^^^^^^^^^^ future created by async block is not `Send`
    | 
   ::: [..]spawn.rs:127:21
    |
127 |         T: Future + Send + 'static,
    |                     ---- required by this bound in
    |                          `tokio::task::spawn::spawn`
    |
    = help: within `impl std::future::Future`, the trait
    |       `std::marker::Send` is not  implemented for
    |       `std::rc::Rc&lt;&amp;str&gt;`
note: future is not `Send` as this value is used across an await
   --&gt; src/main.rs:10:9
    |
7   |         let rc = Rc::new(&quot;hello&quot;);
    |             -- has type `std::rc::Rc&lt;&amp;str&gt;` which is not `Send`
...
10  |         yield_now().await;
    |         ^^^^^^^^^^^^^^^^^ await occurs here, with `rc` maybe
    |                           used later
11  |         println!(&quot;{}&quot;, rc);
12  |     });
    |     - `rc` is later dropped here
</code></pre>
<p>我们会在下一章讨论一个关于这个错误的特殊的例子。</p>
<a class="header" href="print.html#store-values" id="store-values"><h2>Store Values</h2></a>
<p>接下来继续实现 <code>process</code> 函数来处理接收的命令。我们将使用 <code>HashMap</code> 来存储收到的值，<code>SET</code> 操作会插入一条新的记录到 <code>HashMap</code> 中，而 <code>GET</code> 操作则从中读取。并且，我们还会使用一个循环来处理来自同个连接的多个命令。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
async fn process(socket: TcpStream) {
    // The `Connection` lets us read/write redis **frame** instead of
    // byte streams. The `Connection` type is defined by mini-redis

    use mini_redis::Command::{self, Get, Set};
    use std::collections::HashMap;

    // A hashmap is used to store data
    let mut db = HashMap::new();

    // Connection, provided by `mini-redis`, handles parsing frames from
    // the socket
    let mut connection = Connection::new(socket);

    while let Some(frame) = connection.read_frame().await.unwrap() {
        let response = match Command::from_frame(frame).unwrap() {
            Set(cmd) =&gt; {
                db.insert(cmd.key().to_string(), cmd.value().to_vec());
                Frame::Simple(&quot;OK&quot;.to_string())
            }
            Get(cmd) =&gt; {
                if let Some(value) = db.get(cmd.key()) {
                    Frame::Bulk(value.clone().into())
                } else {
                    Frame::Null
                }
            }
            cmd =&gt; panic!(&quot;unimplemented {:?}&quot;, cmd),
        };

        connection.write_frame(&amp;response).await.unwrap();
    }

#}</code></pre></pre>
<p>接下来启动服务端</p>
<pre><code class="language-shell">$ cargo run
</code></pre>
<p>然后打开一个新的终端窗口，运行 <code>hello-redis</code> 示例</p>
<pre><code class="language-shell">$ cargo run --example hello-redis
</code></pre>
<p>然后，我们就能看到下面的输出了</p>
<pre><code class="language-shell">got value from the server; success=Some(b'world')
</code></pre>
<p>现在我们能获取跟设置信息了，但还存在一个问题。设置的信息还没办法在不同的连接中共享，如果其他的套接字连接尝试使用 <code>GET</code> 命令获取 <code>hello</code> 的值，他将找不到任何东西。</p>
<p>完整的代码在 <a href="https://github.com/tokio-rs/website/blob/master/tutorial-code/spawning/src/main.rs">这里</a>。</p>
<p>在下一节，我们会为所有的客户端实现一个共享、持久化的存储。</p>
<a class="header" href="print.html#shared-state" id="shared-state"><h2>Shared State</h2></a>
<p>现在我们有一个可以运行的键值对服务端了，但是还有一个明显的瑕疵：状态不能跨多个连接共享，在这篇文章中我们来解决这个问题。</p>
<a class="header" href="print.html#strategies" id="strategies"><h3>Strategies</h3></a>
<p>在 Tokio 中有几种不同的方式来实现共享状态。</p>
<ol>
<li>通过 <code>Mutex</code> 保护共享的状态</li>
<li>创建一个新的任务管理状态并通过消息传递来处理状态</li>
</ol>
<p>通常你会希望使用第一种方式来处理简单的数据，第二种方式一般用来处理那些需要异步处理的 <strong>I/O</strong> 设施。 在当前章节，共享的状态是一个提供了 <code>insert</code> 跟 <code>get</code> 操作的 <code>HashMap</code>，这两个操作都不是异步的，所以我们选择使用 <code>Mutex</code>。</p>
<a class="header" href="print.html#add-bytes-dependency" id="add-bytes-dependency"><h3>Add <code>bytes</code> dependency</h3></a>
<p>Mini-Redis 包使用了包 <code>bytes</code> 中的 <code>Bytes</code> 类型替代了 <code>Vec&lt;u8&gt;</code> 。<code>Bytes</code> 的目标是为网络编程提供健壮的字节数组结构，他在 <code>Vec&lt;u8&gt;</code> 之上提供的最大的特性就是浅拷贝。换一种说法就是，调用 <code>Bytes</code> 的 <code>clone</code> 函数并不会复制底层的任何数据，取而代之的是，<code>Bytes</code> 使用了引用计数的方式来处理底层的数据，它类似于 <code>Arc&lt;Vec&lt;u8&gt;&gt;</code> 但提供了一些其他的能力。</p>
<p>为了添加对他的依赖，我们需要早 <code>Cargo.toml</code> 中的 <code>[denpendencies]</code> 中添加下面的信息</p>
<pre><code class="language-toml">bytes = &quot;1&quot;
</code></pre>
<a class="header" href="print.html#initialize-the-hashmap" id="initialize-the-hashmap"><h3>Initialize the <code>HashMap</code></h3></a>
<p><code>HashMap</code> 将被用来在不同的线程间进行共享，为了实现这个目的我们使用 <code>Arc&lt;Mutex&lt;_&gt;&gt;</code> 进行包装。</p>
<p>第一步，为了后续方便使用，先使用 <code>use</code> 引入下面的类型；</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use bytes::Bytes;
use std::collections::HashMap;
use std::sync::{Arc, Mutex};

type db = Arc&lt;Mutex&lt;HashMap&lt;String, Bytes&gt;&gt;&gt;;
#}</code></pre></pre>
<p>接着更新 <code>main</code> 函数来初始化 <code>HashMap</code> ，然后将它作为一个 <code>Arc</code> 的句柄*(Handle)*传递给 <code>process</code> 函数。使用 <code>Arc</code> 将允许我们将 <code>HashMap</code> 当成一个引用传递多个任务，而这些任务可能会运行在不同的线程上。在 Tokio 中 <strong>Handle</strong> 一般表示为用来访问一个共享状态的值的引用。</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::net::TcpListener;
use std::collections::HashMap;
use std::sync::{Arc, Mutex};

#[tokio::main]
async fn main() {
    let listener = TcpListener::bind(&quot;127.0.0.1:6379&quot;).await.unwrap();

    println!(&quot;Listening&quot;);

    let db = Arc::new(Mutex::new(HashMap::new()));

    loop {
        let (socket, _) = listener.accept().await.unwrap();
        // Clone the handle to the hash map.
        let db = db.clone();

        println!(&quot;Accepted&quot;);
        tokio::spawn(async move {
            process(socket, db).await;
        });
    }
}
</code></pre></pre>
<a class="header" href="print.html#on-using-stdsyncmutex" id="on-using-stdsyncmutex"><h3>On using <code>std::sync::Mutex</code></h3></a>
<p>注意，使用 <code>std::sync::Mutex</code> 而不是 <code>tokio::sync::Mutex</code> 来保护 <code>HashMap</code>。一个常见的误用就是在异步的代码中使用 <code>tokio::sync::Mutex</code>，异步的 <code>Mutex</code> 是用来保护多个 <code>.await</code> 之间的调用的。</p>
<p>同步的 <code>Mutex</code> 在尝试获取锁时会堵塞当前线程，意味着他同时也会堵塞其他的任务。然而，切换为 <code>tokio::sync::Mutex</code> 通常不会带来什么帮助，因为异步的 <code>Mutext</code> 在内部也是使用同步的 <code>Mutext</code>。</p>
<p>作为一个指导规则，在异步的代码中使用同步的 <code>Mutex</code> 不会有什么问题，只要操作评率保持较低，并且持有锁的操作不跨越多个 <code>.await</code>。 除此之外，使用 <code>parking_log::Mutex</code> 是个更快的替换 <code>std::sync::Mutex</code> 的方案。</p>
<a class="header" href="print.html#update-process" id="update-process"><h3>Update <code>process()</code></h3></a>
<p><code>process</code> 函数不再初始化 <code>HashMap</code>，而是通过参数获取一个 <code>HashMap</code> 的句柄，并且在使用之前要对其进行加锁。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use tokio::net::TcpStream;
use mini_redis::{Connection, Frame};

async fn process(socket: TcpStream, db: Db) {
    // The `Connection` lets us read/write redis **frame** instead of
    // byte streams. The `Connection` type is defined by mini-redis

    use mini_redis::Command::{self, Get, Set};
    use std::collections::HashMap;

    // Connection, provided by `mini-redis`, handles parsing frames from
    // the socket
    let mut connection = Connection::new(socket);

    while let Some(frame) = connection.read_frame().await.unwrap() {
        let response = match Command::from_frame(frame).unwrap() {
            Set(cmd) =&gt; {
                let mut db = db.lock().unwrap();
                db.insert(cmd.key().to_string(), cmd.value().clone());
                Frame::Simple(&quot;OK&quot;.to_string())
            }
            Get(cmd) =&gt; {
                let db = db.lock().unwrap();
                if let Some(value) = db.get(cmd.key()) {
                    Frame::Bulk(value.clone().into())
                } else {
                    Frame::Null
                }
            }
            cmd =&gt; panic!(&quot;unimplemented {:?}&quot;, cmd),
        };

        connection.write_frame(&amp;response).await.unwrap();
    }
}
#}</code></pre></pre>
<a class="header" href="print.html#tasks-threads-and-contention" id="tasks-threads-and-contention"><h3>Tasks, threads, and contention</h3></a>
<p>在竞争比较小的情况中使用堵塞的 <code>Mutex</code> 来保护一个短小的临界区是一个可以接收的策略，当获取锁产生竞争时，执行当前任务的线程会因为等待这个 <code>Mutex</code> 而被堵塞住，而且他并不是只堵塞当前任务，而是堵塞所有被调度到这个线程的任务。</p>
<p>在默认的情况下， Tokio 的 <code>Runtime</code> 使用基于多线程的调度器，任务可能会被调度到 <code>Runtime</code> 所管理的任意一个线程中。如果大量调度中的任务都需要访问同一个 <code>Mutex</code>，那他将会成为一个瓶颈。换种说法，如果使用了 <code>Runtime</code> 的 <code>current_thread</code>模式 ，那这 <code>Mutex</code> 永远都不可能被获取到。</p>
<blockquote>
<p><code>current_thread runtime flavor</code> 是一个轻量的、单线程 <code>Runtime</code>。在只需要创建少量任务并且处理少量套接字的情况下，他是一个不错的选择。比如为客户端的异步函数提供一个同步接口的桥梁时，他就能工作的很好。</p>
</blockquote>
<p>如果同步 <code>Mutex</code> 的竞争成为了程序的瓶颈，最好的修复方式是将它替换为 Tokio 的 <code>Mutext</code>, 或者是下面的几个方式</p>
<ul>
<li>使用单独的任务通过消息传递来管理状态信息</li>
<li>分区 <code>Mutex</code></li>
<li>重构代码避免使用 <code>Mutex</code></li>
</ul>
<p>在我们的例子中，因为每个 <code>Key</code> 都是独立的，使用共享的 <code>Mutex</code> 会是一个较好的方式，为了实现这个目标，我们将单个 <code>Mutex&lt;HashMap&lt;_, _&gt;&gt;</code> 替换为 <code>N</code> 个不同的实例。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
type ShardedDb = Arc&lt;Vec&lt;Mutex&lt;HashMap&lt;String, Vec&lt;u8&gt;&gt;&gt;&gt;&gt;;
#}</code></pre></pre>
<p>所以获取某个 <code>Key</code> 对应的存储则变为两步操作，第一步使用 <code>Key</code> 来确认使用哪个共享的元素，第二步才是获取该元素中所使用的 <code>HashMap</code>。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
let shard = db[hash(key) % db.len()].lock().unwrap();
shard.insert(key, value);
#}</code></pre></pre>
<p>有一个 <a href="https://docs.rs/dashmap">dashmap</a> 包提供已经实现好的分区 <code>HashMap</code>。</p>
<a class="header" href="print.html#holding-a-mutexguard-across-an-await" id="holding-a-mutexguard-across-an-await"><h3>Holding a <code>MutexGuard</code> across an <code>.await</code></h3></a>
<p>你可能会写出类似下面的代码</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use std::sync::Mutex;

async fn increment_and_do_stuff(mutex: &amp;Mutex&lt;i32&gt;) {
  let mut lock = mutex.lock().unwrap();
  *lock += 1;
  
  do_somthing_async().await;
} // lock goes out of scope here
#}</code></pre></pre>
<p>当你尝试用这个代码来创建任务时，会得到如下的错误信息</p>
<pre><code class="language-shell">error: future cannot be sent between threads safely
   --&gt; src/lib.rs:13:5
    |
13  |     tokio::spawn(async move {
    |     ^^^^^^^^^^^^ future created by async block is not `Send`
    |
   ::: /playground/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-0.2.21/src/task/spawn.rs:127:21
    |
127 |         T: Future + Send + 'static,
    |                     ---- required by this bound in `tokio::task::spawn::spawn`
    |
    = help: within `impl std::future::Future`, the trait `std::marker::Send` is not implemented for `std::sync::MutexGuard&lt;'_, i32&gt;`
note: future is not `Send` as this value is used across an await
   --&gt; src/lib.rs:7:5
    |
4   |     let mut lock = mutex.lock().unwrap();
    |         -------- has type `std::sync::MutexGuard&lt;'_, i32&gt;` which is not `Send`
...
7   |     do_something_async().await;
    |     ^^^^^^^^^^^^^^^^^^^^^^^^^^ await occurs here, with `mut lock` maybe used later
8   | }
    | - `mut lock` is later dropped here
</code></pre>
<p>这是因为 <code>std::sync::MutexGuard</code> 这个类型并非 <code>Send</code> 的。这意味着你不能将 <code>Mutex</code> 锁传递给其他的线程，这个错误会出现则是因为 Tokio 会在每次 <code>.await</code> 时在线程间移动这个任务。为了避免这个问题，应该重构代码，让 <code>Mutex</code> 的锁在调用 <code>.await</code> 前销毁。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// This works!
async fn increment_and_do_stuff(mutex: &amp;Mutex&lt;i32&gt;) {
  {
    let mut lock = mutex.lock().unwrap();
    *lock += 1;
  }
  do_something_async().await;
}
#}</code></pre></pre>
<p>要注意的是，下面的方式并不能正常运行</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use std::sync::Mutex;

// This fails too.
async fn increment_and_do_stuff(mutex: &amp;Mutex&lt;i32&gt;) {
    let mut lock = mutex.lock().unwrap();
    *lock += 1;
    drop(lock);

    do_something_async().await;
}
#}</code></pre></pre>
<p>这是因为编译器当前只会使用当前作用域的信息来判断一个 <code>Future</code> 是否满足 <code>Send</code>。在将来的某个时候编译器可能会升级来实现分析 <code>drop</code> 操作，但现在你必须自己明确的指定作用域。</p>
<p>关于这个错误的讨论也可以在 <a href="https://tokio.rs/tokio/tutorial/spawning#send-bound">Send bound section from the spawning chapter</a> 中找到。</p>
<p>你不该使用某种不要求 <code>Send</code> 的方式来创建任务，去尝试避免这个问题。因为 Tokio 在执行 <code>.await</code> 时将持有着锁的任务暂定，然后其他的任务会被调度到当前的线程，如果这个任务也尝试去获取这个锁，就会导致这个任务因为获取不到锁被堵塞，同时前一个持有锁的任务可能会因为没有线程可用而无法重新启用，所以无法释放他持有的锁，从而造成死锁。</p>
<p>我们会在后续继续讨论如果解决这个问题。</p>
<a class="header" href="print.html#restructure-you-code-to-not-hold-the-lock-across-an-await" id="restructure-you-code-to-not-hold-the-lock-across-an-await"><h4>Restructure you code to not hold the lock across an <code>.await</code></h4></a>
<p>我们已经在上面看过一个解决问题的代码示例了，在这里我们提供一种更健壮的方式来实现。比如我们可以将 <code>Mutex</code> 包装到一个结构体里面，并且只会在同步的函数中对其进行加锁。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use std::sync::Mutex;

struct CanIncrement {
  mutex: Mutex&lt;i32&gt;,
}

impl CanIncrement {
  // This function is not marked async
  fn increment(&amp;self) {
    let mut lock = self.mutex.lock().unwrap();
    *lock += 1;
  }
}

async fn increment_and_do_stuff(can_incr: &amp;CanIncrement) {
  ca_incr.increment();
  do_something_async().await;
}
#}</code></pre></pre>
<p>这种方式保证了不会触发 <code>Send</code> 错误，因为 <code>MutexGuard</code>  并没有出现在异步函数中。</p>
<a class="header" href="print.html#spawn-a-task-to-manage-the-state-and-use-message-passing-to-operate-on-it" id="spawn-a-task-to-manage-the-state-and-use-message-passing-to-operate-on-it"><h4>Spawn a task to manage the state and use message passing to operate on it</h4></a>
<p>我们之前提到的第二种方式通常使用在共享的 <strong>IO</strong> 资源的情况，在下一章会详细介绍。</p>
<a class="header" href="print.html#use-tokios-asynchronous-mutex" id="use-tokios-asynchronous-mutex"><h4>Use Tokio's asynchronous mutex</h4></a>
<p>也可以是用 Tokio 提供的 <code>tokio::sync::Mutex</code> 类型，他主要的特点是允许只有锁跨越多个 <code>.await</code> 调用。但同时，异步的 <code>Mutex</code> 也需要花费比普通 <code>Mutex</code> 更多的代价，所以更多是使用另外的两个方式。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use tokio::sync::Mutex; // Note! This uses the Tokio mutex

// This compiles!
// (but restructuring the code would be better in this case)
async fn increment_and_do_stuff(mutex: &amp;Mutex&lt;i32&gt;) {
  let mut lock = mutex.lock().await;
  *lock += 1;
  
  do_something_async().await;
} // lock goes out of scope here
#}</code></pre></pre>
<a class="header" href="print.html#channel" id="channel"><h1>Channel</h1></a>
<p>现在开始来学一些 Tokio 中的并发支持。开始把这些并发的东西应用到我们的客户端中，比如我们想要同时运行两个 Redis 的命令时，可以为每个命令创建一个任务，这样两个命令就能够并行的执行了。</p>
<p>首先我们来简单的尝试一下</p>
<pre><pre class="playpen"><code class="language-rust">use mini_redis::{client, Result};

#[tokio::main]
async fn main() {
    let mut client = client::connect(&quot;127.0.0.1:6379&quot;).await?;
    
    let t1 = tokio::spawn(async {
        let res = client.get(&quot;hello&quot;).await?;
    });
    
    let t2 = tokio::spawn(async {
        client.set(&quot;hello&quot;, &quot;world&quot;.into()).await?;
    }):

    t1.await.unwrap();
    t2.await.unwrap();   
}
</code></pre></pre>
<p>因为 <code>Client</code> 没有实现 <code>Copy</code>，并且两个任务都同时需要在其中使用到 <code>client</code> 变量，所以是编译不过的。并且，因为 <code>Client::set</code> 需要使用 <code>&amp;mut self</code> 也就是可变引用作为参数，因此该对象的使用实际上是排他的。我们可以为每个连接创建一个任务，但那并不是个好主意，我们不能够使用 <code>std::sync::Mutex</code> 因为会有持有锁跨越 <code>.await</code> 的情形；我们不能使用 <code>tokio::sync::Mutex</code>，那会导致在同一时刻只有一个请求在处理。如果 <code>client</code> 实现了 <a href="https://redis.io/topics/pipelining">pipelining</a>，那异步的 <code>Mutex</code> 就会无法充分的利用当前连接了。</p>
<a class="header" href="print.html#message-passing" id="message-passing"><h2>Message Passing</h2></a>
<p>最好的方式是使用消息传递，这种方式需要创建一个单独的任务来管理 <code>client</code> 资源，任何一个想要发送命令的任务都需要发送消息给管理 <code>client</code> 的任务，该任务会处理收到的命令然后将处理结果回复给请求的任务。</p>
<p>使用这个策略，可以只创建一个连接，管理 <code>client</code> 的任务就可以有序的处理 <code>get</code> 跟 <code>set</code> 请求了，而 <code>Channel</code> 则相当于一个缓冲，就算 <code>client</code>处于繁忙状态其他的任务页可以发送命令到 <code>Channel</code>，当他能够处理新请求的时候，他会从 <code>Channel</code> 中获取下一个请求进行处理，这样的方式能够带来很好的吞吐量，还可以再将其扩展为使用连接池的方式。</p>
<a class="header" href="print.html#tokios-channel-primitives" id="tokios-channel-primitives"><h2>Tokio's Channel Primitives</h2></a>
<p>Tokio 提供了数种用于处理不同场景的 <code>Channel</code></p>
<ul>
<li><code>mpsc</code>: 多生产者、单消费者的 <code>Channel</code>，能够发送多个信息</li>
<li><code>oneshot</code> 单生产者、单消费者的 <code>Channel</code>，只能发送一个信息</li>
<li><code>broadcast</code> 多生产者、多消费者，能够发送多个信息，每个消费者都能收到所有信息</li>
<li><code>watch</code> 单生产者、多消费者，能够发送多个信息，但不会保存历史信息，消费者只能收到最新的信息</li>
</ul>
<p>如果需要多生产者、多消费者的 <code>Channel</code>但希望每个信息只被一个消费者收到，可以使用 <a href="https://docs.rs/async-channel/">async-channel</a> 包。还有其他的一些不能用在 Rust 的异步编程中的 <code>Channel</code> 实现，比如 <code>std::sync::mpsc</code> 跟 <a href="https://docs.rs/crossbeam/latest/crossbeam/channel/index.html">crossbeam::channel</a>。这些 <code>Channel</code> 以堵塞线程的方式等待信息的到来，所以不能够在异步的代码中使用。</p>
<p>在这一节中，我们会用到 <code>mpsc</code> 跟 <code>oneshot</code>，其他的 <code>Channel</code> 类型会在后续的章节中用到，然后，本章完整的代码可以在 [这里](https://github.com/tokio-rs/website/blob/master/tutorial-code/channels/src/main .rs) 找到。</p>
<a class="header" href="print.html#define-the-message-type" id="define-the-message-type"><h2>Define The Message Type</h2></a>
<p>在大部分使用消息传递的场景中，负责处理消息的任务都需要响应不止一种命令。在我们的案例中，该任务需要响应 <code>GET</code> 跟 <code>SET</code> 两种命令，因此我们首先会定义一个包含所有命令类型的 <code>Command</code> 枚举。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use bytes::Bytes;

#[derive[Debug]]
enum Command {
  Get {
    key: String
  },
  Set {
    key: String,
    value: Bytes,
  }
}
#}</code></pre></pre>
<a class="header" href="print.html#create-the-channel" id="create-the-channel"><h2>Create The Channel</h2></a>
<p>然后在 <code>main</code> 函数中创建一个 <code>mppsc</code> 类型的 <code>Channel</code></p>
<pre><pre class="playpen"><code class="language-rust">use tokio::sync::mpsc;

#[tokio::main]
async fn main() {
  // Create a new channel with a capacity of at most 32
  let (tx, mut rx) = mpsc::channel(32);
}
</code></pre></pre>
<p><code>mpsc</code> 的 <code>Channel</code> 将用来<strong>发送</strong>命令给管理 Redis 连接的任务，其多生产者的模式允许多个任务通过他来发送消息。创建 <code>Channel</code> 的函数返回了两个值，一个发送者跟一个接收者，这两个句柄通常是分开使用的，他们会被移到到不同的任务中。</p>
<p>创建 <code>Channel</code> 时设置了容量为 32，如果消息发送的速度超过了接收的速度，这个 <code>Channel</code> 只会最多保存 32 个消息，当其中保存的消息超过了 32 时，继续调用 <code>send(...).await</code> 会让发送的任务进入睡眠，直到接收者又从 <code>Channel</code> 中消费了消息。</p>
<p>在使用中会通过 <code>clone</code> 发送者的方式，来让多个任务同时发送消息，如下例</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::sync::mpsc;

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let (tx, mut rx) = mpsc::channel(32);
    let tx2 = tx.clone();

    tokio::spawn(async move {
        tx.send(&quot;sending from first handle&quot;).await;
    });

    tokio::spawn(async move {
        tx2.send(&quot;sending from second handle&quot;).await;
    });

    while let Some(message) = rx.recv().await {
        println!(&quot;GOT = {}&quot;, message);
    }

    Ok(())
}
</code></pre></pre>
<p>每个消息最后都会发送给唯一的接收者，因为通过 <code>mpsc</code> 创建的接收者是不能 <code>clone</code> 的。</p>
<p>当所有发送者出了自身的作用域或被 <code>drop</code> 后就不再允许发送消息了，在这个时候接收者会返回 <code>None</code>，意味着所有的发送者已经被销毁，所以 <code>Channel</code> 也已经被关闭了。</p>
<p>在我们的示例中，Redis 的连接是管理任务所负责的，他知道可以在管理的 <code>Channel</code> 都关闭后，Redis 的连接就不会再有人使用了，因此可以关闭 Redis 的连接了。</p>
<a class="header" href="print.html#spawn-manager-task" id="spawn-manager-task"><h2>Spawn Manager Task</h2></a>
<p>接下来创建负责处理来自 <code>Channel</code> 的任务，首先创建连接到 Redis 的客户端对象，然后依次接收信息并调用 Redis 去处理。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use mini_redis::client;

let manager = tokio::spawn(async move {
  let mut client = client::connect(&quot;127.0.0.1:6379&quot;).await.unwrap();

  while let Some(cmd) = rx.recv().await {
    use Command::*;
    match cmd {
      Get { key } =&gt; {
        client.get(&amp;key).await;
      }
      Set { key, value } =&gt; {
        client.set(&amp;key, value).await;
      }
    }
  }
});
#}</code></pre></pre>
<p>然后更新之前的两个任务，将直接使用 Redis 连接的方式改为通过 <code>Channel</code> 发送命令。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
let tx2 = tx.clone();
let t1 = tokio::spawn(async move {
  let cmd = Command::Get {
    key: &quot;hello&quot;.to_string()
  };
  tx.send(cmd).await.unwrap();
});

let t2 = tokio::spawn(async move {
  let cmd = Command::Set {
    key: &quot;foo&quot;.to_string(),
    value: &quot;bar&quot;.into()
  };
  tx2.send(cmd).await.unwrap();
});
#}</code></pre></pre>
<p>然后在 <code>main</code> 函数的最下面，在程序退出前我们调用前面定义的几个 JoinHandle (t1、t2、manager) 的 <code>.await</code>;</p>
<a class="header" href="print.html#receive-responses" id="receive-responses"><h2>Receive Responses</h2></a>
<p>最后一步是需要接收 <code>manager</code> 任务对我们请求的响应。在操作成功的情形<code>GET</code> 命令需要返回我们之前调用 <code>SET</code> 的结果。</p>
<p>我们将通过传递 <code>oneshot</code> 类型的 <code>Channel</code> 来获取响应，<code>oneshot</code> 是单生产者、单消费者的 <code>Channel</code>，他还为只传递单次消息做了优化，在我们的示例中，这个单次消息就是我们所需的响应。</p>
<p>跟 <code>mpsc</code> 类似， <code>oneshot::channel()</code> 返回发送者跟接收者。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use tokio::sync::oneshot;

let (tx, rx) = oneshot::channel();
#}</code></pre></pre>
<p>而跟 <code>mpsc</code> 不同的是他不需要定义容量，因为他的容量永远都为 1，还有返回的发送者及接收者都不能够进行 <code>clone</code>。</p>
<p>为了接收来自 <code>manager</code> 任务的响应，在发送命令之前我们需要先创建好 <code>oneshot</code> 实例，发送者的部分会被包含到命令之中，以便 <code>manager</code> 用来发送响应，接收者则由任务自己用来接收响应。</p>
<p>首先我们先来更新 <code>Commoand</code> 的定义以让他包含发送者类型 <code>Sender</code>。为了书写方便我们为 <code>Sender</code> 定义了别名。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use tokio::sync::oneshot;
use bytes::Bytes;

type Responder&lt;T&gt; = oneshot::Sender&lt;mini_redis::Result&lt;T&gt;&gt;;

#[derive(Debug)]
enum Command {
  Get {
    key: String,
    resp: Responder&lt;Option&lt;Bytes&gt;&gt;,
  },
  Set {
    key: String,
    value: Vec&lt;u8&gt;,
    resp: Responder&lt;()&gt;,
  }
}
#}</code></pre></pre>
<p>接着，更新发送命令的部分，让他包含 <code>oneshot::Sender</code>。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
let tx2 = tx.clone();
let t1 = tokio::spawn(async move {
  let (resp_tx, resp_rx) = oneshot::channel();
  let cmd = Command::Get {
    key: &quot;hello&quot;.to_string(),
    resp: resp_tx,
  };
  tx.send(cmd).await.unwrap();

  let res = resp_rx.await;
  println!(&quot;GOT = {:?}&quot;, res);
});

let t2 = tokio::spawn(async move {
  let (resp_tx, resp_rx) = oneshot::channel();
  let cmd = Command::Set {
    key: &quot;foo&quot;.to_string(),
    value: &quot;bar&quot;.into(),
    resp: resp_tx,
  };
  tx2.send(cmd).await.unwrap();

  let resp = resp_rx.await.unwrap();
  println!(&quot;GOT = {:?}&quot;, res);
});

#}</code></pre></pre>
<p>最后，更新 <code>manager</code> 任务让他通过 <code>oneshot</code> 的 <code>Channel</code> 返回最终的响应。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
let manager = tokio::spawn(async move {
  let mut client = client::connect(&quot;127.0.0.1:6379&quot;).await.unwrap();

  while let Some(cmd) = rx.recv().await {
    use Command::*;
    match cmd {
      Get { key, mut resp } =&gt; {
        let res = client.get(&amp;key).await;
        let _ = resp.send(res);
      }
      Set { key, value, resp } =&gt; {
        let res = client.set(&amp;key, value.into()).await;
        let _ = resp.send(res);
      }
    }
  }
})
#}</code></pre></pre>
<p>调用 <code>oneshot::Sender</code> 的 <code>send</code> 会立即返回结果因此无需再调用 <code>.await</code>，这是因为 <code>send</code> 函数会在调用的时候立即返回成功或失败的结果。</p>
<p>通过 <code>oneshot</code> 发送的消息只有在接收者已经被销毁时返回错误，这表示已经没有接受者期待我们的响应了，并且接收者不再等待响应是一种可以接受的结果。因此发送者返回的 <code>Err</code> 可以不进行处理。</p>
<p>完整的代码可以在 <a href="https://github.com/tokio-rs/website/blob/master/tutorial-code/channels/src/main.rs">这里</a> 找到。</p>
<a class="header" href="print.html#backpressure-and-bounded-channels" id="backpressure-and-bounded-channels"><h2>Backpressure And Bounded Channels</h2></a>
<p>无论何时介绍并发或者队列，对其容量的限制都是很重要的，因为他能在系统优雅的处理负载，无限制的队列最终会把所有的可用内存都耗尽导致系统以不可预测的方式失效。</p>
<p>Tokio 小心的避免绝对的队列，其中最重要的一部分就是所有的异步操作都是 <code>lazy</code> ，考虑下面的代码</p>
<blockquote>
<p><code>lazy</code> 表示操作不会马上执行，只有在有需要的时候才执行</p>
</blockquote>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
loop {
  async_op();
}
#}</code></pre></pre>
<p>如果异步操作马上就执行，这个循环会不断的将 <code>async_op</code> 放到任务队列中去执行，而不管其之前的操作是否已经完成，这就体现为无限的队列，以回调形式或立即执行的 <code>Future</code> 异步系统很容易就会受这些操作影响。</p>
<p>然而，以 Rust 的异步编程机制实现的 Tokio 并不会真正的去执行上面代码片段中的 <code>async_op</code>。这是因为 没有在他上面调用 <code>.await</code> ，如果将上面的代码改为使用 <code>.await</code> ，这个循环每次都会等待之前的任务完成后才开始一个新的任务。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
loop {
  // Will not repeat until `async_op` completes
  async_op().await;
}
#}</code></pre></pre>
<p>并行跟队列已经郑重的介绍了，当真正想要那么做的时候，可能会用到下面这些</p>
<ul>
<li><code>tokio::spawn</code></li>
<li><code>select!</code></li>
<li><code>join!</code></li>
<li><code>mpsc::channel</code></li>
</ul>
<p>在这么做的时候，要小心的确保这些操作都是有限的，比如在等待接收 TCP 连接的循环中，要确保能打开的套接字的上限。在使用 <code>mpsc::channel</code> 时要选择一个合理的容量，具体的合理值根据程序不同而不同。</p>
<p>小心的选择这些限额将能够大幅度的提高 Tokio 程序的可靠性。</p>
<a class="header" href="print.html#io" id="io"><h1>I/O</h1></a>
<p>Tokio 中的 <code>I/O</code> 操作跟标准库 <code>std</code>的保持一致，只是他是异步的。他提供了用于读取数据的 <code>AsyncRead Trait</code> 及用于写入操作的 <code>AayncWrite Trait</code>，很多类型都实现了他们，比如 <code>TcpStream</code>、<code>File</code>、<code>Stdout</code> 等，还有很多其他的基础类型如 <code>Vec&lt;u8&gt;</code>、 <code>&amp;[u8]</code> ，他们让我们能够像使用 <code>Writer</code> 跟 <code>Reader</code> 一般使用字节数组。</p>
<p>本页面会使用 Tokio 来实现包含基础 <code>I/O</code> 的读取跟写入的一些示例。下一节我们则将会来探索更高级的示例。</p>
<a class="header" href="print.html#asyncread-and-asyncwrite" id="asyncread-and-asyncwrite"><h2><code>AsyncRead</code> And <code>AsyncWrite</code></h2></a>
<p>这两个 <code>Trait</code> 基础设施为字节流提供了异步的读取跟写入能力，事实上这两个 <code>Trait</code> 的函数并不会被直接调用，就像你不会手动的去调用 <code>Future</code> 的 <code>poll</code> 函数，他们更多的是会被其他的使用 <code>Trait</code> 来调用，如 <code>AsyncReadExt</code> 跟 <code>AsyncWriteExt</code>。</p>
<p>接下来让我们来稍微看其中几个函数，所有的这些函数都是 <code>async</code> 并且必须使用 <code>.await</code> 来调用。</p>
<a class="header" href="print.html#async-fn-read" id="async-fn-read"><h3><code>async fn read()</code></h3></a>
<p><code>AsyncReadExt::read()</code> 提供了用来读取数据到缓存然后返回读取字节数的异步函数。</p>
<p><strong>Note</strong>： 当 <code>read()</code> 返回 <code>Ok(0)</code> 时，意味着读取的数据流已经关闭，后续再进行的 <code>read()</code> 调用都会马上返回 <code>Ok(0)</code>。对于 <code>TcpStream</code> 来说则意味着套接字的读取部分已经关闭。</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::fs::File;
use tokio::{self, AsyncReadExt};

#[tokio::main]
async fn main() -&gt; io::Result&lt;()&gt; {
  let mut f = File::open(&quot;foo.txt&quot;).await?;
  let mut buffer = [0; 10];
  
  // read up to 10 bytes
  let n = f.read(&amp;mut buffer[..]).await?;
  
  println!(&quot;The bytes: {:?}&quot;, &amp;buffer[..n]);
  Ok(())
}
</code></pre></pre>
<a class="header" href="print.html#async-fn-read_to_end" id="async-fn-read_to_end"><h3><code>async fn read_to_end()</code></h3></a>
<p><code>AsyncReadExt::read_to_end()</code> 从数据流中读取所有的数据直到读完。</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::io::{self, AsyncReadExt};
use tokio::fs::File;

#[tokil::main]
async fn main() -&gt; io::Result&lt;()&gt; {
  let mut f = File::open(&quot;foo.txt&quot;).await?;
  let mut buffer = Vec::new();
  
  // read to whole file
  f.read_to_end(&amp;mut buffer).await?;
  Ok(())
}
</code></pre></pre>
<p><code>async fn write()</code></p>
<p><code>AsyncWriteExt::write</code> 将缓存中的数据写入到写入者中，然后返回写入的数据字节数。</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::io::{self, AsyncWriteExt};
use tokio::fs::File;

#[tokio::main]
async fn main() -&gt; io::Result&lt;()&gt; {
  let mut file = File::create(&quot;foo.txt&quot;).await?;
  
  // Writes some prefix of the byte string, but not necessarily all of it.
  let n = file.write(b&quot;some bytes&quot;).await?;
  println!(&quot;Wrote the first {} bytes of 'some bytes'&quot;, n);
  Ok(())
}
</code></pre></pre>
<a class="header" href="print.html#async-fn-write_all" id="async-fn-write_all"><h3><code>async fn write_all()</code></h3></a>
<p><code>AsyncWriteExt::write_all</code> 写入缓存中所有的数据到写入者中。</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::io::{self, AsyncWriteExt};
use tokio::fs::File;

#[tokio::main]
async fn main() -&gt; io::Result&lt;()&gt; {
  let mut buffer = File::create(&quot;foo.txt&quot;).await?;
  
  buffer.write_all(b&quot;some bytes&quot;).await?;
  Ok(())
}
</code></pre></pre>
<p>每个 <code>Trait</code> 都包含了一些其他的函数，可以查看文档来全面的了解可用的函数。</p>
<a class="header" href="print.html#helper-functions" id="helper-functions"><h2>Helper Functions</h2></a>
<p>跟标准库 <code>std</code> 一样，<code>tokio::io</code> 模块也包含了许多适用于标准输入、标准输出及标准错误的实用函数，比如 <code>tokio::io::copy</code> 异步复制函数可用于从读取对象中读取所有数据到写入对象中。</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::fs::File;
use tokio::io;

#[tokio::main]
async fn main() -&gt; io::Result&lt;()&gt; {
  let mut reader: &amp;[u8] = b&quot;hello&quot;;
  let mut file = File::create(&quot;foo.txt&quot;).await?;
  
  io::copy(&amp;mut reader, &amp;mut file).await?;
  Ok(())
}
</code></pre></pre>
<p>我们知道这里能够复制是因为字节数组同样实现了 <code>AsyncRead</code>。</p>
<a class="header" href="print.html#echo-server" id="echo-server"><h2>Echo Server</h2></a>
<p>接下来会通过一个 Echo 服务来熟悉异步 IO。</p>
<p>该 Echo 服务会绑定一个 <code>TcpListener</code> 并用他在循环中接收进来的连接，都会该套接字中读取所有的数据，然后马上将数据写回该套接字。对于客户端来说，则是发送数据给服务端然后会接收到一模一样的回复。</p>
<p>我们会以两种不同的策略来实现 Echo 服务两次。</p>
<a class="header" href="print.html#using-iocopy" id="using-iocopy"><h3>Using <code>io::copy()</code></h3></a>
<p>第一步我们先使用一个实用的 <code>io::copy</code> 函数来实现。</p>
<p>这是个 TCP 服务所以我们首先需要一个接收连接的循环，在每次接收到一个套接字时则会创建一个任务对其进行处理。</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::io;
use tokio::net::TcpListener;

#[tokio::main]
async fn main() -&gt; io::Result&lt;()&gt; {
  let mut listener = TcpListener::bind(&quot;127.0.0.1:6142&quot;).await.unwrap();
  
  loop {
    let (mut socket, _) = listener.accept().await?;
    
    tokio::spawn(async move {
      // Copy data here
    })
  }
}
</code></pre></pre>
<p>正如我们之前介绍的，这个实用函数需要 <code>Reader</code> 跟 <code>Writer</code> 两个参数，并从 <code>Reader</code> 中拷贝数据到 <code>Writer</code>。我们现在只有一个同时实现了 <code>AsyncRead</code> 跟 <code>AsyncWrite</code> 的 <code>TcpStream</code>，并且因为 <code>io::copy</code> 的两个参数都需要 <code>&amp;mut</code> 类型，所以我们现在手上的套接字不能同时作为两个参数传递给它。</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// This fails to cpmpile
io::copy(&amp;mut socket, &amp;,ut socket).await
#}</code></pre></pre>
<a class="header" href="print.html#spliting-a-reader--writer" id="spliting-a-reader--writer"><h3>Spliting A Reader + Writer</h3></a>
<p>为了解决这个问题，我们需要将套接字拆分为 <code>Reader</code> 跟 <code>Writer</code>，而拆分的最佳方法取决于我们所需要拆分的具体类型。</p>
<p>任何一个 <code>Reader</code> + <code>Writer</code> 都可以通过 <code>io::split</code> 来拆分，这个函数获取一个参数然后将其拆分为 <code>Reader</code> 跟 <code>Writer</code> 返回，返回的这两个句柄现在可以独立的使用，比如将他们用到不同的任务中。</p>
<p>举个例子，Echo 服务可以像这样来使用 <code>Writer</code> 跟 <code>Reader</code></p>
<pre><pre class="playpen"><code class="language-rust">use tokio::io::{self, AsyncReaderExt, AsyncWriterExt};
use tokio::net::TcpStream;

#[tokio::main]
async fn main() -&gt; io::Result&lt;()&gt; {
  let socket = TcpStream::connect(&quot;127.0.0.1:6142&quot;).await?;
  let (mut rf, mut wr) = io::split(socket);
  
  // Write data in the background
  let write_data = tokio::spawn(async move {
    wr.write_all(b&quot;hello\r\n&quot;).await?;
    wr.write_all(b&quot;world\r\n&quot;).await?;
    
    Ok::&lt;_,io::Error&gt;(())
  });
  
  let mut buf = vec![0; 128];
  
  lop {
    let n = rd.read(&amp;mut buf).await?;
    
    if n == 0 {
      break;
    }
    
    println!(&quot;GOT {:?}&quot;, &amp;buf[..n]);
  }
  
  Ok(())
}
</code></pre></pre>
<p><code>io::split</code> 支持将任意的实现了 <code>AsyncRead</code> + <code>AsyncWrite</code> 的值拆分为独立的句柄，在函数的内部，他使用 <code>Arc</code> 跟 <code>Mutex</code> 来实现这个功能。为了避免这个额外的负荷，我们可以使用 <code>TcpStream</code> 提供的两个特殊的拆分函数。</p>
<p><code>TcpStream::split</code>  获取一个自身的引用并返回 <code>Reader</code> 跟 <code>Writer</code>。因为使用了自身的引用，所以两个返回的句柄需要保留在调用了 <code>split()</code> 进行拆分在同一个任务中，这个特殊的 <code>split</code> 调用是无需任何代价的，他不需要使用到任何类似 <code>Arc</code> 或 <code>Mutex</code> 之类的东西。<code>TcpStream</code> 同时也提供使用了 <code>Arc</code> 作为实现的 <code>into_split</code> 用来支持将拆分的 <code>Reader</code> 跟 <code>Writer</code> 应用到不同的任务中。</p>
<p>因为 <code>io::copy</code> 是在持有 <code>TcpStream</code> 的同一个任务中调用的，所以我们使用 <code>TcpSteram::split</code>，处理请求的任务现在变为了</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
tokio::spawn(async move {
  let (mut rd, mut wr) = soket.split();
  
  if io::copy(&amp;mut rd, &amp;mut wr).await.is_err() {
    eprintln!(&quot;failed to copy&quot;);
  }
});
#}</code></pre></pre>
<p>完整的代码可以在 <a href="https://github.com/tokio-rs/website/blob/master/tutorial-code/io/src/echo-server.rs">这里</a> 找到。</p>
<a class="header" href="print.html#manual-copying" id="manual-copying"><h3>Manual Copying</h3></a>
<p>接下来我们来实现一个手动复制数据的 Echo 服务，这里面我们使用了 <code>AsyncReadExt::read</code> 跟 <code>AsyncWriteExt::write_all</code>。</p>
<p>完整的代码如下:</p>
<pre><pre class="playpen"><code class="language-rust">use tokio::io::{self, AsyncReadExt, AsyncWriteExt};
use tokio::net::TcpListener;

#[tokio::main]
async fn main() -&gt; io::Result&lt;()&gt; {
  let mut listener = TcpListener::bind(&quot;127.0.0.1:6142&quot;).await.unwrap();
  
  loop {
    let (mut socket, _) = listener.accept().await?;
    
    tokio::spawn(async move {
      let mut buf = vec![0; 1024];
      
      loop {
        match socket.read(&amp;mut buf).await {
          // Return value of `Ok(0)` signfies that eh remote has closed
          Ok(0) =&gt; return,
          Ok(n) =&gt; {
            // Copy the data back to socket
            if socket.write_all(&amp;buf[..n]).await.is_err() {
              // Unexpected socket error. There isn't much we can
              // do here so just stop processing.
              return;
            }
          }
          Err(_) =&gt; {
            // Unexpected socket error. There isn't must we can do
            // here so just stop processing.
            return;
          }
        }
      }
    });
  }
}
</code></pre></pre>
<p>接下来逐步拆分说明，因为使用了 <code>AsyncRead</code> 跟 <code>AsyncWrite</code> 相关的东西，所以需要在当前代码中引入下面的两个 Trait</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use tokio::io::{self, AsyncReadExt, AsyncWriteExt};
#}</code></pre></pre>
<a class="header" href="print.html#allocating-a-buffer" id="allocating-a-buffer"><h4>Allocating A Buffer</h4></a>
<p>当前的实现方案需要将数据从套接字中读取到缓存，然后再将其从缓存中写回套接字</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
let mut buf = vec![0; 1024];
#}</code></pre></pre>
<p>一个创建在栈中的缓存是需要明确避免的，正如早前提到的，所有的任务数据都需要保存到任务中，用来在 <code>.await</code> 调用之间使用。在我们的例子中， <code>buf</code> 会跨越 <code>.await</code> 调用使用，所有的任务数据都需要保存到单次申请中，你可以假定将使用 <code>enum</code> 来保存那些需要跨越 <code>.await</code> 调用的数据。</p>
<p>如果将缓存表现为栈中的数组，在每次创建任务时产生的内部数据结构看起来会是这样：</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
struct Task {
  // internal task fields here
  task: enum {
    AwaitingRead {
      socket: TcpStream,
      buf: [BufferType],
    },
    AwaitingWriteAll {
      socket: TcpStream,
      buf: [BufferType],
    }
  }
}
#}</code></pre></pre>
<p>如果使用一个栈中的数组作为上面的缓存类型，他会被内联的保存到任务的数据结构中，这会让任务的数据结构变得非常庞大。而且，缓存的大小通常跟页的大小设为相同的，这会让任务的数据大小变成一个不是很合适的尺寸: <code>$page-size + a-few_bytes</code>。</p>
<p>编译器会将异步代码块优化的比我们看到的基础的 <code>enum</code> 更好，在实际中，变量并不像枚举那样在不同的状态中移动，但是任务的数据结构大小仍然会像我们看到的那么大。</p>
<p>因此，使用专用独立的内存分配来使用缓存会是更好的方式。</p>
<a class="header" href="print.html#handling-eof" id="handling-eof"><h4>Handling EOF</h4></a>
<p>当我们所读取的 TCP 流被关闭了，对其调用的 <code>read()</code> 会返回 <code>Ok(0)</code>，这意味着我们需要中断这个读取循环了，忘记中断这个读取的循环是很多常见 Bug 的来源</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
loop {
  match socket.read(&amp;mut buf).await {
    Ok(0) =&gt; return
  }
}
#}</code></pre></pre>
<p>没有中断这个读取的循环会导致 CPU 被占满 100%，当套接字关闭后，<code>socket.read()</code> 会马上返回，所以这个循环会无限循环下去。</p>
<p>完整的代码可以在 <a href="https://github.com/tokio-rs/website/blob/master/tutorial-code/io/src/echo-server.rs">这里</a> 找到。</p>
<a class="header" href="print.html#framing" id="framing"><h1>Framing</h1></a>
<a class="header" href="print.html#datastruct" id="datastruct"><h1>DataStruct</h1></a>
<a class="header" href="print.html#simple-dynamic-string" id="simple-dynamic-string"><h1>Simple Dynamic String</h1></a>
<p>[TOC]</p>
<a class="header" href="print.html#a简介" id="a简介"><h2>简介</h2></a>
<p>鉴于 c 没有提供一般的字符串处理方式，导致对字符串的各种处理都很麻烦，所以 redis 自身实现了一个 <code>简单动态字符串</code> 对象，用于操作所有的字符串，同时为了跟 c 兼容，所以 sds 在实现的时候保证了它能直接适用于 <em>标准库</em> 提供的各种 <code>strxxx</code> 函数。</p>
<p>因为这个实现本身不是很复杂，所以这里不会每个细节都介绍，而是选择主要的实现进行简单的说明。</p>
<pre><code class="language-c">typedef char *sds;
struct sdshdr {
    unsigned int len; // len 字段指的是当前 sds 的长度
    unsigned int free; // free 指的是 buff 中剩余的长度
    char buf[];
};
</code></pre>
<p>把 <code>char*</code> 定义成 sds 好处是，sds 其实就是 c 语言的字符串，能够兼容 c 标准库的 strXXX 函数。而上面定义的 sdshdr 事实上并不会直接返回给调用者，那它是如何做到兼容标准库的呢，答案就在于 sdshdr 的最后一个字段 buf。</p>
<a class="header" href="print.html#a实现" id="a实现"><h2>实现</h2></a>
<a class="header" href="print.html#a新建-sds-与如何兼容标准-c-字符串" id="a新建-sds-与如何兼容标准-c-字符串"><h3>新建 sds 与如何兼容标准 C 字符串</h3></a>
<p>我们先来说明每个字段的作用，然后在通过新建一个 sds 实例来说明实际的实现。</p>
<pre><code class="language-c">sds sdsnew(const char *init) {
    size_t initlen = (init == NULL) ? 0 : strlen(init);
    return sdsnewlen(init, initlen);
}

sds sdsnewlen(const char *init, size_t initlen) {
    struct sdshdr *sh;
    
    if (init) {
        sh = zmalloc(sizeof(struct sdshdr) + initlen + 1);
    }
    else {
        sh = zcalloc(sizeof(struct sdshdr) + initlen + 1);
    }
    
    if (sh == NULL) return NULL;
    
    sh-&gt;len = initlen;
    sh-&gt;free = 0;
    if (initlen &amp;&amp; init)
        memcpy(sh-&gt;buf, init, initlen);
    sh-&gt;buf[initlen] = '\0';
    return (char*)sh-&gt;buf;
}
</code></pre>
<p>接着开始分析以上的实现，首先要创建一个新的 sds 实例，就要调用 <code>sdsnew</code>, 并传递一个初始内容 <code>init</code>, 并且算出初始内容 init 的长度 initlen, 接着调用 <code>sdsnewlen</code>，这里就是真正初始化 sds 实例的地方了。</p>
<p>首先会判断 init 是否为空，如果不为空，则调用 <code>zmalloc</code>，否则调用 <code>zcalloc</code>，这里是一个比较低层次的优化，如果 init 不为空的话，那稍后会以 init 的内容来填充 <code>sdshdr</code> 的 buf，所以就没必要再使用 <code>zcalloc</code> 来为他填充 0
了。
另一点需要注意的是我们看到，在申请内存时，申请的是实际大小再 +1, 这是因为，sds 的字符串都会以 <code>'\0'</code> 空字符为结束符，所以需要申请多一个字节大小的空间。
接下来的就都是常规的初始化，包括初始化长度、剩余空间(为 0, 因为申请的空间刚好可用于存放 init)、跟 buf。最后的 <strong><em>重点</em></strong> 就是，<em>返回的是 sh-&gt;buf 而不是 sh。所以实际上返回的是一个指向 sh-&gt;buf 的指针，这也是 sds 能兼容 c 标准库的重点</em>，我们这里提前说明，由于返回的是 sh-&gt;buf，所以之后所有的 sds 接口，在拿到参数 <code>sds xxx</code> 后，都会使用</p>
<pre><code class="language-c">struct sdshdr *sh = (void *)(s - (sizeof(struct sdshdr)));    
</code></pre>
<p>来得到指向 <code>sdshdr</code> 的正确指针。</p>
<a class="header" href="print.html#a获取-sds-长度-sdslen" id="a获取-sds-长度-sdslen"><h3>获取 sds 长度 (sdslen)</h3></a>
<p>因为 sdshdr 本身已经保存了当前字符串的长度，所以可以在 O(1) 的时间内获取到 sds 的长度，而不是类似 strlen，需要使用 O(n) 的时间来遍历字符串获取长度。唯一需要注意的是 长度是不包括 <code>'\0'</code> 字符的。</p>
<pre><code class="language-c">size_t sdslen(sds s) {
    struct sdshdr *sh = (void *)(s - (sizeof(struct sdshdr)));
    return sh-&gt;len;
}
</code></pre>
<a class="header" href="print.html#a连接字符串-sdscatsdscatsds" id="a连接字符串-sdscatsdscatsds"><h3>连接字符串 (sdscat、sdscatsds)</h3></a>
<p>sds 提供的连接字符串函数，同时支持 C 字符串跟 sds 字符串，并且能在 O(n) 的时间内完成字符串的连接，下面我们来看看他的实现，连接 C 字符串跟连接 sds 虽然接口不同，但最终调用的实现都是一样的。</p>
<pre><code class="language-c">sds sdscat(sds s, const char *t) {
    return sdscatlen(s, t, strlen(t));
}

sds sdscatsds(sds s, const sds t) {
    return sdscatlen(s, t, sdslen(t));
}
</code></pre>
<p>从上面我们可以看出，两个接口的区别就只在于获取长度的方式不同，最终调用的实现都是 <code>sdscatlen</code>，接下来我们看看它的实现</p>
<pre><code class="language-c">sds sdscatlen(sds s, const void *t, size_t len) {
    struct sdshdr *h;
    size_t curlen = sdslen(s);
    
    s = sdsMakeRoomFor(s, len);
    if (s == NULL) return NULL;
    sh = (void *)(s - (sizeof(struct sdshdr)));
    memcpy(s + curlen, t, len);
    sh-&gt;len = curlen + len;
    sh-&gt;free = sh-&gt;free - len;
    s[curlen + len] = '\0';
    return s;
}
</code></pre>
<p>整个实现除了 <code>sdsMakeRoomFor</code> 其他的都是常规操作，获取原始字符串的长度，用 <code>sdsMakeRoomFor</code> 申请内存，然后复制新的字符串到原始字符串后面，更新长度信息，更新剩余内存信息，在拼接后的字符串末尾添加 <code>'\0'</code>。</p>
<a class="header" href="print.html#a扩展-sds-存储空间" id="a扩展-sds-存储空间"><h4>扩展 sds 存储空间</h4></a>
<p>所以我们来看看 <code>sdsMakeRoomFor</code> 到底做了啥</p>
<pre><code class="language-c">// 扩展 s 的内存空间，以存放额外的 addlen 个长度的字符
sds sdsMakeRoomFor(sds s, size_t addlen) {
    struct adshdr *sh, *newsh;
    size_t free = sdsavail(s); // 获取 s 的剩余空间
    size_t len, newlen;
    
    if (free &gt;= addlen) return s; // 如果剩余空间足够放下 addlen，则直接使用原来的 s
    
    // 到这里说明原有的空间不足以存放 addlen, 所以获取原有 s 的长度，
    // 然后计算新的所需内存大小
    len = sdslen(s);
    newlen = (len + addlen); 
    
    // 这里涉及到一个 sds 的内存申请策略，下面我们会详解
    if (newlen &lt; SDS_MAX_PREALLOC)
        nrewlen *= 2;
    else
        newlen += SDS_MAX_PREALLOC;
        
    // 计算出新的大小后，重新分配内存
    newsh = zrealloc(sh, sizeof(struct sdshdr) + newlen + 1);
    if (newsh == NULL) return NULL;
    
    // 更新剩余大小，然后返回新的 sds
    newsh-&gt;free = newlen - len;
    return newsh-&gt;buf;
}
</code></pre>
<p>因为这里的说明篇幅较长，所以我把它加到代码的注释里了，然后这边需要再重点解释一下的就是 sds 的内存申请策略，在上面我们看到了一个 <strong>阀值</strong>，由 <code>SDS_MAX_PREALLOC</code> 定义了一个最大值</p>
<ul>
<li>小于这个阀值的情况下，会申请所需内存的两倍大小，以备后续的 sds 操作，简而言之就是以空间换取时间，因为内存申请的代价是较高的。</li>
<li>另外一种情况是，如果所需内存已经超过阀值了，则返回申请的大小再加上阀值大小的内存块，这里是为了避免申请过于庞大的内存，造成内存使用压力激增。</li>
<li>这个大小一般定义为 2MB.</li>
</ul>
<a class="header" href="print.html#a连接字符串-sdscat-sdscatlen" id="a连接字符串-sdscat-sdscatlen"><h3>连接字符串 (sdscat, sdscatlen)</h3></a>
<p>对于大部分的函数，都支持对 C 字符串跟 sds 的互操作，各个函数大致都命名为 sdsxxx 跟 sdsxxxsds，对于 cat 也是，所以之后的各个函数都不会每个都解释。
sdscat 是将 sds 跟其他的字符串连接起来，如果是传递的 C 字符串，那字符串必须以空字符结尾，另一点要注意的是，连接之后原本的 sds 指针会失效，需要使用的是由该函数返回的新的 sds 指针，以下是代码</p>
<pre><code class="language-c">sds sdscat(sds s, const char *t) {
    return sdscatlen(s, t, strlen(t));
}

sds sdscatlen(sds s, const char *t, size_t len) {
    struct sdshdr *sh;
    size_t curlen = sdslen(s);
    
    s = sdsMakeRoomFor(s, len);
    if (s == NULL) return NULL;
    sh = (void *)(s - (sizeof(struct sdshdr)));
    memcpy(s + curlen, t, len);
    sh-&gt;len = curlen + len;
    sh-&gt;free = sh-&gt;free - len;
    s[curlen + len] = '\0';
    return s;
}
</code></pre>
<a class="header" href="print.html#a复制新的字符串覆盖现有-sds-sdscpy-sdscpylen" id="a复制新的字符串覆盖现有-sds-sdscpy-sdscpylen"><h3>复制新的字符串覆盖现有 sds (sdscpy, sdscpylen)</h3></a>
<p>将新的 C 字符串或者 sds 复制到指定的 sds 中，并会覆盖源 sds。</p>
<pre><code class="language-c">sds sdscpy(sds s, const char *t) {
    return sdscpylen(s, t, strlen(t));
}

sds sdscpylen(sds s, const char *t, size_t len) {
    struct sdshdr *sh = (void *)(s - (sizeof(struct sdshdr)));
    size_t totlen = sh-&gt;free + sh-&gt;len; // 原有的 buf 总空间
    
    if (totlen &lt; len) { // 如果原有不足以存放新的字符串，则重新申请内存
        // 在原有 buf 的基础上，在申请一块内存，
        // 所以是 len - sh-&gt;len
        s = sdsMakeRoomFor(s, len - sh-&gt;len); 
        if (s == NULL) return NULL;
        sh = (void *)(s - (sizeof(struct sdshdr)));
        totlen = sh-&gt;free + sh-&gt;len;
    }
    
    memcpy(s, t, len); // 复制字符串到 sds 中
    s[len] = '\0';
    sh-&gt;len = len;
    sh-&gt;free = totlen - len;
    return s;
}
</code></pre>
<a class="header" href="print.html#a格式化输出字符串-拼接到-sds--sdscattprint-" id="a格式化输出字符串-拼接到-sds--sdscattprint-"><h3>格式化输出字符串 拼接到 sds ( sdscattprint )</h3></a>
<p>这个便捷的接口支持以 C 字符串相同的格式化方式来拼接新的字符串到现有的 sds, 当然，也可以直接提供一个空的 sds，用格式化字符串的方式来新建一个 sds。
使用这个接口的好处在于， sds 会帮使用者管理内存，而无需自己再尝试分配内存来格式化字符串，降低了使用的复杂度。</p>
<pre><code class="language-c">sds sdscatprintf(sds s, const char *fmt, ...) {
    va_list ap;
    char *t;
    va_start(ap, fmt);
    t = sdscatvprintf(s, fmt, ap);
    va_end(ap);
    return t;
}
</code></pre>
<p>上面这个接口所做的全部就只是，将变长参处理的开始跟结束提取出来，让 <code>sdscatvprintf</code> 可以专注于格式化的操作。接着看具体的实现</p>
<pre><code class="language-c">sds sdscatvprintf(sds s, const char *fmt, va_list ap) {
    va_list cpy;
    char staticbuf[1024], *buf = staticbuf, *t;
    size_t buflen = strlen(fmt) * 2;    
    
    // 如果缓存数组的长度小于预估的字符串长度，则以预估的为准
    if (buflen &gt; sizeof(staticbuf)) {
        buf = zmalloc(buflen);
        if (buf == NULL) return NULL;
    }
    else {
        buflen = sizeof(staticbuf);
    }   
    
    while (1) {
        // 放置哨兵
        buf[buflen - 1] = '\0';
        va_copy(cpy, ap);
        vsnprintf(buf, buflen, fmt, cpy);
        va_end(cpy);
        
        // 判断哨兵是否存在
        if (buf[buflen - 2] != '\0') {

            if (buf != staticbuf) zfree(buf);
            buflen *= 2;
            buf = zmalloc(buflen);
            if (buf == NULL) return NULL;
            continue;
        }
        break;
    }
    
    t = sdscat(s, buf);
    if (buf != staticbuf) zfree(buf);
    return t;
</code></pre>
<ul>
<li>上面先尝试分配了 1024 个字节的缓存，然后计算出格式化字符串 fmt 的长度 buflen，如果预分配的 1024 个字节小于 buflen 的话，则以 buflen 为准。这么做的原因是因为想尽量不从堆中分配内存来格式化，尽可能的提高效率。</li>
<li>然后进入循环，不断的使用标准库的 <code>vsnprintf</code> 格式化字符串
<ul>
<li>每次尝试都是使用 ap 的一份 copy，并且在格式化之前会在 buf 的倒数第二个位置先放置一个哨兵 <code>'\0'</code>, 如果格式化后哨兵元素没有被覆盖，则说明 buf 的长度大于最终格式化后的长度，格式化成功。</li>
<li>否则每次以两倍 buf 的大小重新扩展 buf，用于尝试格式化字符串。
在尝试期间会判断 buf 是否预先准备的 staticbuf，如果不是的话则尝试释放后重新分配。</li>
</ul>
</li>
<li>最终调用 sdscat 把格式化好的字符串连接到 s 中并返回。</li>
</ul>
<a class="header" href="print.html#a效率更高的格式化输出--sdscatfmt-" id="a效率更高的格式化输出--sdscatfmt-"><h3>效率更高的格式化输出 ( sdscatfmt )</h3></a>
<p>前面的 sdscatprint 主要是使用了标准库内置的格式化输出再加上了自身的一些内存管理，而 sdscatfmt 则是连格式化输出都是自己来实现，主要的区别就是处理各个内置的符号，如 <code>%s、%d</code> 等，同时也添加了对于 sds 字符串的支持 (<code>%S</code>)，并且如果字符串的处理是使用 %S 时，可以得到相较于标准库来说非常高的效率。下面我们来看看他的具体实现。</p>
<pre><code class="language-c">sds sdscatfmt(sds s, const char *fmt, ...) {
    struct sdshdr *sh = (void *)(s - (sizeof(struct sdshdr)));
    size_t initlen = sdslen(s);
    const char *f = fmt;
    int i;
    va_list ap;
    
    va_start(ap, fmt);
    i = initlen;
    // 遍历整个字符串，查找以 % 开头的字符
    while(*f) {
        // 
        case '%':
            next = *(f+1);
            f++;
            switch(next) {
            case 's':
            case 'S':
                // 处理字符串
                str = va_arg(ap, char *);
                l = (next == 's')? strlen(str) : sdslen(str);
                if (sh-&gt;free &lt; l) {
                    s = sdsMakeRoomFor(s, l);
                    sh = (void *)(s - (sizeof(struct sdshdr)));
                }
                memcpy(s+i, str, l);
                sh-&gt;len += l;
                sh-&gt;free -= l;
                i += l;
                break;
            case 'i':
            case 'I':
                // 处理整数
                if (next == 'i')
                    num = var_arg(ap, int)
                else
                    num = va_arg(ap, long long);
                {
                    char buf[SDS_LLSTR_SIZE];
                    l = sdsll2str(buf, num);
                    if (sh-&gt;free &lt; l) {
                        s = sdsMakeRoomFor(s, l);
                        sh = (void *)(s - sizeof(struct sdshdr)));
                    }
                    memcpy(s+i, buf, l);
                    sh-&gt;len += l;
                    sh-&gt;free -= l;
                    i += l;                
                }
                break;
            case 'u':
            case 'U':
                // 处理无符号整数
                if (next == 'u')
                    unum = va_arg(ap, unsigned int);
                else
                    unum = va_arg(ap, unsigned long long);
                {
                    char buf[SDS_LLSTR_SIZE];
                    l = sdsull2str(buf, unum);
                    if (sh-&gt;free &lt; l) {
                        s = sdsMakeRoomFor(s, l);
                        sh = (void *)(s-(sizeof(struct sdshdr)));
                    }
                    memcpy(s+i, buf, l);
                    sh-&gt;len += l;
                    sh-&gt;free -= l;
                    i += l;
                }
                break;
            default:
                s[i++] = next;
                sh-&gt;len += 1;
                sh-&gt;free -= 1;
                break;
            }
        default:
            s[i++] = *f;
            sh-&gt;len += 1;
            sh-&gt;free -= 1;
            breka;
        }    
        f++;
    }
    va_end(ap);
    s[i] = '\0';
    return s;
}
</code></pre>
<p>这其中还调用了我们未曾分析过的 sdsll2str 以及 sdsull2str，这两个函数分别类似于 atoi 的反向操作，也就是将数字转换成字符串的表示形式，我们大致解释一下实现方式</p>
<pre><code class="language-c">int sdsll2str(char *s, long long value) {
    char *p, aux;
    unsigned long long v;
    size_t l;
    
    // 取每个数字上 10 的余数，然后除以 10，最后生成逆向的，
    // 也就是从个位开始到高位的数字
    v = (value &lt; 0) ? -value : value;
    p = s;
    do {
        *p++ = '0' + (v%10);
        v /= 10;
    } while(v);
    if (value &lt; 0) *p++ = '-';
    
    /* compute length and add null term */
    l = p - s;
    *p = '\0';
    
    /* 反转字符串得到正向的字符串 */
    p--;
    while (s&lt;p) {
        aux = *s;
        *s = *p;
        *p = aux;
        s++;
        p--;
    }
    return l;
}
</code></pre>
<a class="header" href="print.html#a删除-sds-前后端的指定字符--sdstrim-" id="a删除-sds-前后端的指定字符--sdstrim-"><h3>删除 sds 前后端的指定字符 ( sdstrim )</h3></a>
<p>类似于其他语言里字符串的 string.trim 等处理，删除前后端的空格，而这个函数则是删除前后指定的字符串。
比如可以指定</p>
<pre><code class="language-c">sds s = sdsnew(&quot;AA..a.aaHelloWorld   :::&quot;);
s = sdstrim(s, &quot;Aa. :&quot;);
printf(&quot;%s\n&quot;, s); // 输出 HelloWorld
</code></pre>
<p>下面是具体的实现</p>
<pre><code class="language-c">sds sdstrim(sds s, const char *cset) {
    struct sdshdr *sh = (void *)(s - sizeof(struct sdshdr)));
    char *start, *end, *sp, *ep;
    size_t len;
    
    sp = start = s;
    ep = end = s + sdslen(s) - 1;
    while (sp &lt;= end &amp;&amp; strchr(cset, *sp)) sp++;
    while (ep &gt; start &amp;&amp; strchr(cset, *ep)) ep--;
    len = (sp &gt; ep) ? 0 : ((ep - sp) + 1);
    if (sh-&gt;buf != sp) memmove(sh-&gt;buf, sp, len);
    sh-&gt;buf[len] = '\0';
    sh-&gt;free = sh-&gt;free + (sh-&gt;len - len);
    sh-&gt;len = len;
    return s;    
}
</code></pre>
<a class="header" href="print.html#a复制-sds-字符串--sdsdup-" id="a复制-sds-字符串--sdsdup-"><h3>复制 sds 字符串 ( sdsdup )</h3></a>
<p>有时候复制一个字符串也是很有用的，比如在测试的会使用一个测试字符串，不断的使用它的 copy 去测试。具体的实现可以为简单的调用 <code>sdsnewlen</code>,</p>
<pre><code class="language-c">sds sdsdup(sds s) {
    return sdsnewlen(s, sdslen(s));
}
</code></pre>
<a class="header" href="print.html#a截断-sds-字符串--sdsrange-" id="a截断-sds-字符串--sdsrange-"><h3>截断 sds 字符串 ( sdsrange )</h3></a>
<p>跟其他语言内置的 String.SubString 类似，sds 也提供了截断字符串的函数，而对比于其他的实现，sds 还提供了更便捷的使用方式：使用 -1 来表示字符串的最后一个字符, 然后用 -2 表示导数第二个字符，以此类推。如</p>
<pre><code class="language-c">sds s = sdsnew(&quot;Hello World&quot;);
sdsrange(s, 1, -2);
printf(&quot;%s\n&quot;, s); // 输出 ello Worl
</code></pre>
<p>下面看具体的实现</p>
<pre><code class="language-c">void sdsrange(sds s, int start, int end) {
    struct sdshdr *sh = (void *)(s - (sizeof(struct sdshdr)));
    size_t newlen, len = sdslen(s);
    
    if (len == 0) return;
    if (start &lt; 0) {
        start = len + start;
        if (start &lt; 0) start = 0;
    }
    if (end &lt; 0) {
        end = len + end;
        if (end &lt; 0) end = 0;
    }
    
    newlen = (start &gt; end) ? 0 : (end-start)+1;
    if (newlen != 0) {
        // 如果开始长度大于长度，则截断为0.
        if (start &gt;= (signed)len) {
            newlen = 0;
        }
        // 如果结束长度大于长度，则截断到原有的结尾
        else if (end &gt;= (signed)len) {
            end = len-1;
            newlen = (start &gt; end) ? 0 : (end-start)+1;
        }
    }
    else {
        // 如果新长度为 0 则从头开始
        start = 0;
    }
    if (start &amp;&amp; newlen) memmove(sh-&gt;buf, sh-&gt;buf+start, newlen);
    sh-&gt;buf[newlen] = '\0';
    sh-&gt;free = sh-&gt;free+(sh-&gt;len-newlen);
    sh-&gt;len = newlen;
}
</code></pre>
<a class="header" href="print.html#a其他函数" id="a其他函数"><h3>其他函数</h3></a>
<p>以下是其他的便捷函数列表</p>
<pre><code class="language-c">// 从数字初始化 sds
sdsfromlonglong(long long value);
// 转换大小写
void sdstolower(sds s);
void sdstoupper(sds s);
// 比较 sds 字符串
// positive if s1 &gt; s2; negative if s1 &lt; s2
// 0 if s2 and s2 are exactly the same binary string
int sdscmp(const sds s1, const sds s2);
// 初始化空的 sds
sds sdsempty(void);
// 扩展 sds 字符串的长度
sds sdsgrowzero(sds s, size_t len);
</code></pre>
<a class="header" href="print.html#a字典" id="a字典"><h1>字典</h1></a>
<p>[TOC]</p>
<a class="header" href="print.html#a简介-1" id="a简介-1"><h2>简介</h2></a>
<p>字典是用来存储键值对的结构，亦即是他允许使用一个 <code>键</code> 来查找对应的 <code>值</code>，为什么需要把查找的依据跟需要的数据分开呢，因为 <code>值</code> 有可能会是一个非常复杂的结构，而 <code>键</code> 一般是从 <code>值</code> 推导出来的一个较为简单的表示方式。</p>
<p>字典也称哈希表，因为他主要是以各种 <code>哈希函数</code> 对 <code>键</code> 进行计算，并得到 一个唯一的 <strong>索引</strong>，来指向对应的 <code>值</code>。</p>
<blockquote>
<a class="header" href="print.html#a哈希-hash" id="a哈希-hash"><h3>哈希 (Hash)</h3></a>
<p><strong><em>哈希函数</em></strong>
哈希函数是指一类函数，他会将输入的参数进行一系列运算，并得出一个唯一的结果，一个设计良好的哈希函数能够从不同的参数得出不同的结果，并且这个结果将平均分布在样本空间中，如果将计算结果当成一个点，所有可能的结果集当成平面上的一个圆，那这些点将会平均的分布在这个圆上的各个地方。</p>
<p><strong><em>时间复杂度</em></strong>
因为字典的查找在正常情况下，都只消耗跟 哈希 函数对应的时间，所以我们确定它是可以在常数时间，也就是 O(1) 得到结果的。</p>
<p><strong><em>碰撞</em></strong>
有时候 哈希 函数根据 <code>键</code> 计算出来的索引不一定是唯一的，这个时候就会产生碰撞，也就是两个不同的 <code>键</code> 会指向同一个索引，这时有很多种方式来解决这个碰撞问题，如开地址法，链表法等，我们这里的实现只会介绍链表法。</p>
</blockquote>
<p>这里进行分析的是 Redis 源码中的 Dict 实现，他较之正常定义的字典来说，主要是增加了自动扩展机制，使用了二重字典，在字典的负载超过阀值之后，就会以渐进的方式逐步将字典的内容扩展到一个新的字典中。</p>
<a class="header" href="print.html#a实现-1" id="a实现-1"><h2>实现</h2></a>
<a class="header" href="print.html#a字典的定义" id="a字典的定义"><h3>字典的定义</h3></a>
<div id="dictDefine" ></div>
<pre><code class="language-c">
// 定义了两个宏，用来表示字典的状态
 #define DICT_OK  0
 #define DICT_ERR 1

// hash 后的 字典项
// key 保存了用以查找该项的依据
// v 是保存的值
// next 则是前面说的链表法的实现，
// 当有碰撞时则将 hash 结果相同的项连接成一个链表
typedef struct dictEntry {
    void *key;
    union {
        void *val;
        uint64_t u64;
        int64_t s64;
        double d;
    } v;
    struct dictEntry *next;
} dictEntry;

// dictType 保存了当前字典的信息，包括了 Hash 函数的具体实现
// 以及 `键` 跟 `值` 的比较、复制跟销毁方式
typedef struct dictType {
    unsigned int (*hashFunction)(const void *key);
    void *(*keyDup)(void *privdata, const void *key);
    void *(*valDup)(void *privdata, const void *key);
    int (*keyCompare)(void *privdata, const void *key1, const void *key2);
    void (*keyDestructor)(void *privdata, const void *key);
    void (*valDestructor)(void *privdata, const void *key);
} dictType;

// 这个是确切的 哈希表，也就是字典的实现，
// 保存了字典的尺寸、使用量等
typedef struct dictht {
    dictEntry **table;
    unsigned long size;
    unsigned long sizemask;
    unsigend long used;
} dictht;

// 字典对外公开的结构，其中即包含了用于重新扩展字典的标志信息:
// rehashidx
typedef struct dict {
    dictType *type;
    void *privdata;
    dictht ht[2];
    long rehashidx;
    int iterators;
} dict;

</code></pre>
<p>上面是跟字典相关的所有结构的定义，我们也大致说明了各个字段的意义。
基本上就是 dictht 是每个确切的 hash 表，而 dict 管理了两个 dictht，对他们进行了一个更上层的管理，如 Rehashing 机制。
接下来看看具体的实现，首先看看如何创建一个字典。</p>
<a class="header" href="print.html#a乱入的字典说明" id="a乱入的字典说明"><h3>乱入的字典说明</h3></a>
<p>由上面的结构定义我们能看到，Redis 的实现使用了 dict 来管理两个 dictht。
一般的字典实现是：</p>
<pre><code>* 创建 Hash 表
* 增加元素
    * 对 Key Hash, 得到索引
    * 在指定的索引插入新元素
    * 如果有冲突，则使用各种冲突解决方案
* 查找元素
    * 对要查找的 Key 进行 Hash，得到索引
    * 在对应的索引位置查找需要的元素
</code></pre>
<p>Reids 的 dict 则将这些操作再次进行了包装，以满足再字典接近满负荷时，可以平滑的进行扩展，这个扩展的机制即是上面说过的 <strong>Rehashing</strong></p>
<ul>
<li>它使用了两个 <code>dictdt</code>, 并使用了 <code>reashidx</code> 来管理当前使用的 <code>dictdt</code></li>
<li>当使用中的 <code>dictdt</code> 达到负荷条件时，则启动 <code>Rehashing</code> 机制
<ul>
<li>将新的数据插入到另一个 <code>dictdt</code></li>
<li>并分批将旧的数据从原有的 <code>dictdt</code> 中移到新的 <code>dictht</code>。</li>
</ul>
</li>
</ul>
<p>所以在下文中，我们会经常看到 isRehashing 这个函数，它是用来判断当前 <code>dict</code> 是否正在进行 <code>Rehashing</code>，如果是的话则启动分批处理函数处理旧数据，并提示其他操作都要在新的 <code>dictht</code> 中进行。</p>
<a class="header" href="print.html#a创建字典-dictcreate" id="a创建字典-dictcreate"><h3>创建字典 (dictCreate)</h3></a>
<p>创建一个字典的时候需要提供两个参数。</p>
<ul>
<li>一个字典类型的定义，其中包括了如何对 <code>键</code> 进行哈希，<code>建</code> 跟 <code>值</code> 的复制、比较以及销毁，具体定义可见 <a href="print.html#dictDefine">dictType</a> 的定义.</li>
<li>一个私有数据：<code>privDataPtr</code>, 这个私有数据字典本身并不会用到，提供它的原因是为了字典类型的各个操作，当他们之间需要维护状态或者通信等操作时可以使用</li>
</ul>
<pre><code class="language-c">dict *dictCreate(dictType *type, void privDataPrt) {
    dict *d = zmalloc(sizeof(*d));

    _dictInit(d, type, privDataPtr);
    return d;
}

int _dictInit(dict *d, dictType *type, void *privDataPtr) {
    // 首先重置当前字典的两个内置哈希表
    _dictReset(&amp;d-&gt;ht[0]);
    _dictReset(&amp;d-&gt;ht[1]);
    d-&gt;type = type;
    d-&gt;privdata = privDataPtr;
    d-&gt;rehashidx = -1;
    d-&gt;iterators = 0;
    return DICT_OK;
}
</code></pre>
<p>以上字典的初始化，主要是在 _dictInit 中实现，其中包括了重置字典的状态，初始化各个字段</p>
<ul>
<li>type 保存了字典的类型</li>
<li>privdata 保存了私有数据</li>
<li>rehashidx 保存了字典当前的扩展状态，如果值为 -1 说明当前没有在进行扩展</li>
<li>iterators 则保存了迭代器信息</li>
</ul>
<p>而 _dictReset 则设置各个字段的初始值。</p>
<pre><code class="language-c">static void _dictReset(dictht *ht) {
    ht-&gt;table = NULL;
    ht-&gt;size = 0;
    ht-&gt;sizemask = 0;
    ht-&gt;used = 0;
}
</code></pre>
<a class="header" href="print.html#a添加一个键值-dictadd" id="a添加一个键值-dictadd"><h3>添加一个键值 (dictAdd)</h3></a>
<p>接下来是往字典中添加一个项，也就是添加一个 <code>键</code> 跟其对应的 <code>值</code>。
测试代码如下：</p>
<pre><code class="language-c">// 这里我们使用了之前介绍过的 sds 模块，用他来保存我们的键值
// dictType = ...
dict *c = dictCreate(dtype, NULL);
sds *key = sdsnew(&quot;hello&quot;);
sds *val = sdsnew(&quot;world&quot;);

if (DICT_OK == dictAdd(d, key, val)) {
    // add kv ok
}
</code></pre>
<p>以上的测试代码中，首先新建了一个字典，然后分别使用 sds 来表示对应的 key 跟 value，然后将其加入字典中。
然后我们看看 dictAdd 的具体实现</p>
<pre><code class="language-c">int dictAdd(dict *d, void *key, void *val) {
    dictEntry *entry = dictAddRaw(d, key);
    if (!entry) return DICT_ERR;
    dictSetVal(d, entry, val);
    return DICT_OK;
}
</code></pre>
<p>所有的功能基本都是调用其他的内部接口，我们继续看内部的实现。
首先是 <code>dictAddRaw</code>, 它是根据 key 计算出其对应的索引值，并判断 key 是否已经存在于字典中，如果已经存在，则不做任何事情，如果不存在则返回对应的索引，然后才往字典中进行添加。</p>
<pre><code class="language-c">dictEntry *dictAddRaw(dict *d, void *key) {
    int index;
    dictEntry *entry;
    dictht *ht;

    // 这一步是用来扩展字典的，我们留到后面的章节来进行说明
    if (dictIsRehashing(d)) _dictRehashStep(d);

    // 如果 key 已经在字典中存在，则直接返回 NULL
    if ((index = _dictKeyIndex(d, key)) == -1)
        return NULL;

    // 接着添加一个新的节点用来存放新的字典项
    // 首先是获取当前可以插入数据的 dictht
    ht = dictIsRehashing(d) ? &amp;d-&gt;ht[1] : &amp;d-&gt;ht[0];
    entry = zmalloc(sizeof(*entry));
    // 将新的节点设为头节点，旧有的链表则作为新节点的后续节点
    entry-&gt;next = ht-&gt;table[index];
    ht-&gt;table[index] = entry;
    // 添加已有节点数
    ht-&gt;used++;

    // 设置新节点的 key 字段，这里的 dictSetKey 是个宏，作用是
    // 如果 字典类型指定了 key 的复制方式，则使用指定的复制方式，
    // 否则直接复制 key 的指针
    dictSetKey(d, entry, key);
    return entry;
}

</code></pre>
<p>上面很多步骤都包含了 dictIsRehashing 的调用，它用来判断当前字典是否正在扩展，具体如何扩展，我们留到后续的章节说明。
接着再继续看看 _dictKeyIndex 是如何获取 key 对应的索引值的</p>
<pre><code class="language-c">static int _dictKeyIndex(dict *d, const void *key) {
    unsigned int h, idx, table;
    dictEntry *he;

    // 判断字典是否需要扩展
    if (_dictExpandIfNeeded(d) == DICT_ERR)
        return -1;

    h = dictHashKey(d, key); // 计算 key 对应的索引值
    for (table = 0; table &lt;= 1; table++) {
        idx = h &amp; d-&gt;ht[table].sizemask;
        he = d-&gt;ht[table].table[idx];
        while (he) {
            if (dictCompareKeys(d, key, he-&gt;key))
                return -1;
            he = he-&gt;next;
        }
        if (!dictIsRehashing(d)) break;
    }
    return idx;
}
</code></pre>
<p>重点只有两处</p>
<ul>
<li>第一处是 dictHashKey 的实现，不过因为 <code>字典</code> 已经把 hash 的实现方式丢给了调用方，所以它这里只是单纯的转调 <code>dictType</code> 中的 <code>hashFunction</code>，具体的 hash 是怎么做的这里就不详细讨论了，最后会附上 Redis 使用的 HashFunction</li>
<li>第二处则是 for 循环，而 for 循环的重点则是查找两个 dictht，在指定的索引中寻找空位供新的元素插入。</li>
</ul>
<p>然后附上 <code>dictSetKey</code> 的宏定义</p>
<pre><code class="language-c">#define dictSetKey(d, entry, _key_) do { \
    if ((d)-&gt;type-&gt;keyDup) \
        entry-&gt;key = (d)-&gt;type-&gt;keyDup((d)-&gt;privdata, _key_); \
    else \
        entry-&gt;key = (_key_); \
} while(0)
</code></pre>
<a class="header" href="print.html#a查找一个键值" id="a查找一个键值"><h3>查找一个键值</h3></a>
<p>在上面的示例代码中，我们往其中插入了元素之后，想根据 Key 来获取对应的 Value，则可以如下般调用</p>
<pre><code class="language-c">sds key = sdsnew(&quot;hello&quot;);

dictEntry *val = dictFind(c, key);
// 得到 dictEntry 后，即可从 v 字段得到所需的 value
printf(&quot;%s\n&quot;, (sds)val-&gt;v);

</code></pre>
<p>具体的实现很简单，其实就是一个插入新元素的过程，并且比插入元素还要简单得多，
首先计算 key 的 hash 值，得到其在 table 中对应的索引，然后遍历对应索引上的链表，逐一进行比较。</p>
<pre><code class="language-c">dictEntry *dictFind(dict *d, const void *key) {
    dictEntry *he;
    unsigned int h, idx, table;

    // 如果 dict 为空，则直接返回 NULL
    if (d-&gt;ht[0].size == 0) return NULL;

    // Rehashing 判断，下面章节会有详细说明
    if (dictIsRehashing(d)) _dictRehashStep(d);

    // 计算 key 对应的 hash 值
    h = dictHashKey(d, key);
    // 在有必要的情况下遍历两个 table, 这里的必要指的就是在 Rehashing 的情况下
    for (table = 0; table &lt;= 1; table++) {
        // 根据当前 table 的尺寸进行取余
        idx = h &amp; d-&gt;ht[table].sizemask;
        // 得到对应的元素链表
        he = d-&gt;ht[table].table[idx];
        // 逐一进行比较，如果找到则返回对应的 dictEntry
        while (he) {
            if (dictCompareKeys(d, key, he-&gt;key))
                return he;
            he = he-&gt;next;
        }
        // 只有正在进行 Rehashing 的情况下，才有必要再检查 ht[1]
        if (!dictIsRehashing(d)) return NULL;
    }
}
</code></pre>
<a class="header" href="print.html#a删除一个键值" id="a删除一个键值"><h3>删除一个键值</h3></a>
<p>删除的步骤也与查找跟插入类似，这里不再详说。</p>
<pre><code class="language-c">int dictDelete(dict *ht, const void *key) {
    return dictGenericDelete(ht, key, 0);
}

static int dictGenericDelete(dict *d, const void *key, int nofree) {
    unsigned int h, idx;
    dictEntry *he, *prevHe;
    int table;

    if (d-&gt;ht[0].size == 0) return DICT_ERR;
    if (dictIsRehashing(d)) _dictRehashStep(d);

    h = dictHashKey(d, key);

    for (table = 0; table &lt;= 1; table++) {
        idx = h &amp; d-&gt;ht[table].sizemask;
        he = d-&gt;ht[table].table[idx];
        prevHe = NULL;

        while (he) {
            if (dictCompareKeys(d, key, he-&gt;key)) {
                if (prevHe)
                    prevHe-&gt;next = he-&gt;next;
                else
                    d-&gt;ht[table].table[idx] = he-&gt;next;

                if (!nofree) {
                    dictFreeKey(d, he);
                    dictFreeVal(d, he);
                }
                zfree(he);
                d-&gt;ht[table].used--;
                return DICT_OK;
            }
            prevHe = he;
            he = he-&gt;next;
        }
        if (!dictIsRehashing(d)) break;
    }
    return DICT_ERR;
}
</code></pre>
<a class="header" href="print.html#rehashing-的实现" id="rehashing-的实现"><h3>Rehashing 的实现</h3></a>
<p>在上面已经大致讲过了 Rehashing 的机制，也看到了在添加元素时，会调用对应的函数对 <code>dictht</code> 进行 Rehashing, 所以马上我们就来分析下具体的实现。</p>
<a class="header" href="print.html#a判断状态" id="a判断状态"><h4>判断状态</h4></a>
<p>首先是判断是否正在进行 <code>Rehashing</code></p>
<pre><code class="language-c">#define dictIsRehashing(d) ((d)-&gt;rehashidx != -1)
</code></pre>
<p>直接判断 rehashidx 这个字段是否为 -1，如果正在 Rehashing，则 该字段则<em>不等于</em> -1</p>
<a class="header" href="print.html#a何时进行-rehashing" id="a何时进行-rehashing"><h4>何时进行 Rehashing</h4></a>
<p>那什么时候会导致这个字段改变呢？也就是说，什么时候会导致 dict 需要进行 <code>Rehashing</code>，回忆上面的 <code>_dictKeyIndex</code>, 里面包含了：</p>
<pre><code class="language-c">static int _dictKeyIndex(dict *d, const void *key) {
    // ...
    if (_dictExpandIfNeeded(d) == DICT_ERR)
    // ...
}
</code></pre>
<p><code>_dictExpandIfNeeded</code> 即根据 dict 的当前状态进行了判断，从而设置了该 rehashidx 字段。
接下来看看判断的具体逻辑</p>
<pre><code class="language-c">static int _dictExpandIfNeeded(dict *d) {
    // 如果正在进行，则直接返回 OK
    if (dictIsRehashing(d)) return DICT_OK;

    // 如果当前 dictht 为空，则已默认的大小 (4) 对其进行初始化
    if (d-&gt;ht[0].size == 0) return dictExpand(d, DICT_HT_INITIAL_SIZE);

    if (d-&gt;ht[0].used &gt;= d-&gt;ht[0].size &amp;&amp;
        (dict_can_resize ||
         d-&gt;ht[0].used / d-&gt;ht[0].size &gt; dict_force_resize_ratio) {
        return dictExpand(d, d-&gt;ht[0].used * 2);
     }
     return DICT_OK;
}
</code></pre>
<p>最后一个判断条件比较复杂，我们在这边进行解释:</p>
<blockquote>
<p>首先要注意的是 dict_can_resize 这个标志，如果禁止了该标志，则只有当 <code>used</code> 是     <code>size</code> 的 <code>dict_force_resize_ratio</code> 倍时才会进行 <code>Rehashing</code>.
否则只要 <code>used</code> 大于 <code>size</code>, 也就是使用率大于 1,即会开始进行 <code>Rehashing</code></p>
</blockquote>
<p>然后我们可以看到，满足条件后会直接进入 <code>dictExpand</code> 这个调用。
该函数主要是根据当前 <code>dict</code> 的尺寸选择扩张的大小，并为将要使用的 <code>dictht</code> 进行初始化、分配内存等操作，最后则将 <code>rehashidx</code> 这个标志置为 0.
这个 0 表示的意思是，从需要 Rehashing 的 <code>dictht</code> 的第 0 个槽开始操作，所以只要不是 -1,它就表示当前正在处理的位置。</p>
<pre><code class="language-c">int dictExpand(dict *d, unsigned long size) {
    dictht n;
    // 选择将要扩张的尺寸，策略是扩张到下一个大于 size 的基于 2 的倍数的尺寸
    unsigned long realsize = _dictNextPower(size);

    // 如果已经在 Reashing 或者 dictht 的尺寸不合法，则返回错误
    if (dictIsRehashing(d) || d-&gt;ht[0].used &gt; size)
        return DICT_ERR;

    // 如果要扩张的尺寸等于当前尺寸（如已经满负荷，达到了 LONG_MAX) 则返回错误
    if (realsize == d-&gt;ht[0].size) return DICT_ERR;

    n.size = realsize;
    n.sizemask = realsize - 1;
    n.table = zcalloc(realsize * sizeof(dictEntry*));
    n.used = 0;

    // 如果是第一次扩张，也就是 dict 还没有使用过，则直接设置使用新的 dictht
    if (d-&gt;ht[0].table == NULL) {
        d-&gt;ht[0] = n;
        return DICT_OK;
    }

    // 否则设置为需要 Rehashing, 并设置新的 dictht
    d-&gt;ht[1] = n;
    d-&gt;rehashidx = 0;
    return DICT_OK;

}

static unsigned long _dictNextPower(unsigned long size) {
    unsigned long i = DICT_HT_INITIAL_SIZE;

    if (size &gt;= LONG_MAX) return LONG_MAX;
    while (1) {
        if (i &gt;= size)
            return i;
        i *= 2;
    }
}

</code></pre>
<a class="header" href="print.html#a如何-rehashing" id="a如何-rehashing"><h4>如何 Rehashing</h4></a>
<p>当当前 <code>dict</code> 进入 <code>Rehashing</code> 状态后，每次对 <code>dict</code> 进行操作时都会进入 <code>_dictRehashStep</code>，这里会先判断当前是否有操作在迭代 <code>dict</code>，如果没有的话，则进入 <code>Rehashing</code> 的操作。</p>
<p>Rehashing 的具体操作为：</p>
<ul>
<li>默认执行 10 次</li>
<li>每次遍历一个 hash 槽</li>
<li>将旧的数据导到新的 <code>dictht</code></li>
<li>更新 rehashidx 指向下一个需要更新的 <code>dictht.table</code></li>
</ul>
<pre><code class="language-c">static void _dictRehashStep(dict *d) {
    if (d-&gt;iterators == 0) dictRehash(d, 1);
}

// n 是指要 rehash 的次数，每次都会尝试从原有的 dictht 中导入 10 条数据到新的 dictht
int dictRehash(dict *d, int n) {
    int empty_visits = n * 10;
    if (!dictIsRehashing(d)) return 0;

    while (n-- &amp;&amp; d-&gt;ht[0].used != 0) {
        dictEntry *de, *nextde;

        assert(d-&gt;ht[0].size &gt; (unsigned long)d-&gt;rehashidx);

        while (d-&gt;ht[0].table[d-&gt;rehashidx] == NULL) {
            d-&gt;rehashidx++;
            if (--empty_visits == 0) return1;
        }
        // 找到需要转移的槽
        de = d-&gt;ht[0].table[d-&gt;rehashidx];

        while (de) {
            unsigned int h;
            nextde = de-&gt;next;
            // 计算就有元素在新的 dictht 中的位置
            h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[1].sizemask;
            // 重新连接冲突链表
            de-&gt;next = d-&gt;ht[1].table[h];
            d-&gt;ht[1].table[h] = de;
            d-&gt;ht[0].used--;
            d-&gt;ht[1].used++;
            de = nextde;
        }
        d-&gt;ht[0].table[d-&gt;rehashidx] = NULL;
        d-&gt;rehashidx++;
    }

    // 如果旧有 ht 已经清空，将新的 ht 移到 dict-&gt;ht[0]
    // 并标志 Rehashing 已完成
    if (d-&gt;ht[0].used == 0) {
        zfree(d-&gt;ht[0].table);
        d-&gt;ht[0] = d-&gt;ht[1];
        _dictReset(&amp;d-&gt;ht[1]);
        d-&gt;rehashidx = -1;
        return 0;
    }

    // 说明 Rehashing 还未完成
    return 1;
}
</code></pre>
<a class="header" href="print.html#a遍历字典" id="a遍历字典"><h3>遍历字典</h3></a>
<a class="header" href="print.html#a迭代器定义" id="a迭代器定义"><h4>迭代器定义</h4></a>
<p>首先看看迭代器的定义</p>
<ul>
<li><code>d</code> 是一个指向需要迭代的 <code>dict</code> 的指针，</li>
<li><code>index</code> 就是迭代器的当前位置</li>
<li><code>table</code> 是当前遍历使用的 <code>dictht</code></li>
<li><code>safe</code> 用来说明当前迭代器使用期间，不允许进行 Rehashing</li>
<li><code>entry</code> 是当前正在迭代的元素</li>
<li><code>nextEntry</code> 是下一个要迭代的元素，为了能够安全的迭代到下一个元素，因为使用者可能会删除当前元素</li>
</ul>
<pre><code class="language-c">typedef struct dictIterator {
    dict *d;
    long index;
    int table, safe;
    dictEntry *entry; *nextEntry;
    // unsafe iterator fingerprint for misuse detection;
    // 用以判断 迭代器 的安全性
    long long fingerprint;
} dictIterator;
</code></pre>
<a class="header" href="print.html#a获取迭代器" id="a获取迭代器"><h4>获取迭代器</h4></a>
<a class="header" href="print.html#a获取普通迭代器" id="a获取普通迭代器"><h5>获取普通迭代器</h5></a>
<p>普通的迭代器说明当前迭代器使用期间，<code>dict</code> 有可能被纂改。</p>
<pre><code class="language-c">dictIterator *dictGetIterator(dict *d) {
    dictIterator *iter = zmalloc(sizeof(*iter));

    iter-&gt;d = d;
    iter-&gt;table = 0;
    iter-&gt;index = -1;
    iter-&gt;safe = 0;
    iter-&gt;entry = NULL;
    iter-&gt;nextEntry = NULL;
    return iter;
}
</code></pre>
<a class="header" href="print.html#a获取安全迭代器" id="a获取安全迭代器"><h5>获取安全迭代器</h5></a>
<p>安全迭代器跟普通迭代器的区别只有 <code>safe</code> 字段，并保证使用期间不会进行 <code>Rehashing</code></p>
<pre><code class="language-c">dictIterator *dictGetSafeIterator(dict *d) {
    dictIterator *i = dictGetIterator(d);
    i-&gt;safe = 1;
    return i;
}
</code></pre>
<a class="header" href="print.html#a遍历迭代器" id="a遍历迭代器"><h4>遍历迭代器</h4></a>
<p><code>dict</code> 提供了迭代器机制，以便对其中的元素进行遍历的。具体的使用只需如一个循环般，一直调用 <code>dictNext</code> 直到返回 NULL 即可。</p>
<pre><code class="language-c">dictEntry *dictNext(dictIterator *iter) {
    while (1) {

        if (iter-&gt;entry == NULL) {
            // 如果第一次调用 dictNext, 则初始化对应的标志
            dictht = *ht = &amp;iter-&gt;d-&gt;ht[iter-&gt;table];
            if (iter-&gt;index == -1 &amp;&amp; iter-&gt;table == 0) {
                if (iter-&gt;safe)
                    iter-&gt;d-&gt;iterators++;
                else
                    iter-&gt;fingerprint = dictFingerprint(iter-&gt;d);
            }
            // 更新指向下一个元素的索引
            iter-&gt;index++;

            // 如果超过了当前 ht 的尺寸，则判断是否正在 Rehashing，是的话，切换到新的
            // ht，因为新元素不会在插入到原有的 ht[0]
            if (iter-&gt;index &gt;= (long)ht-&gt;size) {
                if (dictIsRehashing(iter-&gt;d)) &amp;&amp; iter-&gt;table == 0) {
                    iter-&gt;table++;
                    iter-&gt;index = 0;
                    ht = &amp;iter-&gt;d-&gt;ht[1];
                }
                else { // 到这里说明已经迭代到最后了
                    break;
                }
            }
            iter-&gt;entry - ht-&gt;table[iter-&gt;index];
        }
        else {
            // 如果当前槽仍有元素，则指向下一个
            iter-&gt;entry = iter-&gt;nextEntry;
        }

        // 如果当前元素不为空，则返回给用户，并准备好下一个
        if (iter-&gt;entry) {
            iter-&gt;nextEntry = iter-&gt;entry-&gt;next;
            return iter-&gt;entry;
        }
    }
    return NULL;
}
</code></pre>
<a class="header" href="print.html#a释放迭代器" id="a释放迭代器"><h5>释放迭代器</h5></a>
<p>迭代器用完了就得释放，特别是对于安全迭代器，因为他会导致 Rehashing 无法执行（除非是 Dict 严重超负荷：5倍）</p>
<pre><code class="language-c">void dictReleaseIterator(dictIterator *iter) {
    if (!(iter-&gt;index == -1 &amp;&amp; iter-&gt;table ==0)) {
        if (iter-&gt;safe)
            iter-&gt;d-&gt;iterators--;
        else
            assert(iter-&gt;fingerprint == dictFingetprint(iter-&gt;d));
    }
    zfree(iter);
}
</code></pre>
<a class="header" href="print.html#a指纹验证" id="a指纹验证"><h5>指纹验证</h5></a>
<p>最后一提的是，在上面我们看到了 <code>dictFingerprint</code> 这个调用，能够为 <code>dict</code> 的当前状态生成一个指纹，并且在上面的普通迭代器中也保存了遍历开始时的指纹，这个指纹是用来保证，在遍历时用户没有对 <code>dict</code> 进行了非法的操作，比如删除元素或者增加元素。
具体的实现就不说明了。</p>
<pre><code class="language-c">long long dictFingerprint(dict *d) {
    long long integers[6], hash = 0;
    int j;

    iteragers[0] = (long)d-&gt;ht[0].table;
    iteragers[1] = d-&gt;ht[0].size;
    iteragers[2] = d-&gt;ht[0].used;
    iteragers[3] = (long)d-&gt;ht[1].table;
    iteragers[4] = d-&gt;ht[1].size;
    iteragers[5] = d-&gt;ht[1].used;

    for (j = 0; j &lt; 6; j++) {
        hash += iteraters[j];

        hash = (~hash) + (hash &lt;&lt; 21);
        hash = hash ^ (hash &gt;&gt; 24);
        hash = (hash + (hash &lt;&lt; 3)) + (hash &lt;&lt; 8); // hash * 265
        hash = hash ^ (hash &gt;&gt; 14);
        hash = (hash + (hash &lt;&lt; 2)) + (hash &lt;&lt; 4); // hash * 21
        hash = hash ^ (hash &gt;&gt; 28);
        hash = hash + (hash &lt;&lt; 31);
    }
    return hash;
}
</code></pre>
<a class="header" href="print.html#a跳跃表" id="a跳跃表"><h1>跳跃表</h1></a>
<p>[TOC]</p>
<a class="header" href="print.html#a介绍" id="a介绍"><h2>介绍</h2></a>
<p>跳跃表是一种有序的列表，可以提供平均 O(logN)、最差 O(N) 复杂度的查找性能，而且相对于 AVL 跟 RB Tree 之类的结构来说有两大优势：</p>
<ul>
<li>实现简单很多</li>
<li>平均性能差不多</li>
</ul>
<p>所以有不少的实现在实现有序的 Set 时，更倾向于使用跳跃表，而且跳跃表在搜索引擎的实现中也占很重要的一部分。</p>
<p>在这里我们选择使用 <strong>redis 的跳跃表</strong>实现来对齐进行分析。
首先我们介绍一下他的大致结构</p>
<p><img src="./image/skiplist_linklist_complete.png" alt="" /></p>
<p>如图所示，所谓的跳跃表，即是在有序的列表中，加入了跳跃使用的指针，以允许从当前节点直接访问后续的其他节点，而不是只能通过遍历的形式来访问其他节点。
接着我们再看看他的基本定义：</p>
<pre><code class="language-c">// 跳跃表的节点定义
typedef struct zskiplistNode {
    void *obj;  // 当前节点的值
    double score; // 当前节点的分值
    struct zskiplistNode *backward; // 指向上一个节点
    struct zskiplistLevel {
        struct zskiplistNode *forward;  // 下一层节点
        unsigned int span;  // 跃度，也就是跳跃的距离
    } level []; 
} zskiplistNode;

// 跳跃表的定义
typedef struct zskiplist {
    struct zskiplistNode *header, *tail;
    unsigned long length;
    int level;
} zskiplist;
</code></pre>
<p>整个跳跃表由 zskiplistNode 跟 zskiplist 组成；
zskiplist 负责管理整个链表的情况，如使用 header 跟 tail 来提供正反两个方向的遍历。
使用 length 来保存列表中 item 的数目，并使用 level 来提示算法，当前跳跃表的最高层数
<strong><em>需要注意的是，header 永远是有 MAX 层的，所以 header 的层数不计入 level 中。</em></strong></p>
<a class="header" href="print.html#a实现-2" id="a实现-2"><h2>实现</h2></a>
<a class="header" href="print.html#a创建-skiplist" id="a创建-skiplist"><h3>创建 skiplist</h3></a>
<p>接着是 zskiplistNode 的介绍</p>
<ul>
<li>obj 是保存对象的指针</li>
<li>score 是当前对象的分值，也就是用于排序的依据，这个一般会由内部算法生成，一般是为了提供区间搜索，比如得到某个分值区间的数据。</li>
<li><strong>综合以上两点，排序有两种方式，一种是依据 obj 本身的比较函数，另一种是依据 score</strong></li>
</ul>
<p><strong>所以在下面的例子中，避免复杂度，所有的测试都以 score 为准</strong></p>
<p>接下来我们从代码层面开始分析，首先是 skiplist 的初始化</p>
<pre><code class="language-c">zskiplistNode *zslCreateNode(int level, double score, void *obj) {
    zskiplistNode *zn = zmalloc(
        sizeof(*zn) * level * sizeof(struct zskiplistLevel));
    zn-&gt;score = score;
    zn-&gt;obj = obj;
    return zn;
}
zskiplist *zslCreate(void) {
    int j;
    zskiplist *zsl;
    
    // 对于 zmalloc 可以理解为就是 malloc 的简单封装，以便于随时更改内存分配器
    zsl = zmalloc(sizeof(*zsl));
    zsl-&gt;level = 1;
    zsl-&gt;length = 0;
    
    // 这里即是分配出一个有 ZSKIPLIST_MAXLEVEL 层的节点作为 header
    // 并把新建节点的 score 设为 0，obj 设为 NULL
    // 正如上面所说的，header 本身是不列入层数计算，并且不存放任何 obj 的
    zsl-&gt;header = zslCreateNode(ZSKIPLIST_MAXLEVEL, 0, NULL);
    for (j = 0; j &lt; ZSKIPLIST_MAXLEVEL; j++) {
        zsl-&gt;header-&gt;level[j].forward = NULL;
        zsl-&gt;header-&gt;level[j].span = 0;
    }
    zsl-&gt;header-&gt;backward = NULL;
    zsl-&gt;tail = NULL;
    return zsl;
}
</code></pre>
<p>通过以上函数，调用 zslCreate 之后，即可得到一个初始化完成的 skiplist，结构大致如下</p>
<div id="init_state"></div>
<pre><code class="language-sh">                                           _____
[ level  ] = 1                            | MAX | --&gt; NULL
[ length ] = 0                            |  .  | --&gt; NULL
[ header ] -------&gt;  [ score ] = 0        |  .  | --&gt; NULL
[  tail  ] = NULL    [ obj   ] = NULL     |  .  | --&gt; NULL
                     [ level ] ---------&gt; |  1  | --&gt; NULL
                                          |  0  | --&gt; NULL
                                          |_____|
</code></pre>
<a class="header" href="print.html#a插入数据" id="a插入数据"><h3>插入数据</h3></a>
<p>接下来我们通过测试代码来逐步分析 skiplist 在进行操作时会有什么动作</p>
<div id="testcode"></div>
<pre><code class="language-c">
// 初始化要插入的对象
int array[10];
for (int i = 0; i &lt; (sizeof(array) / sizeof(int)); i++) {
    array[i] = i + 1;
}

zskiplist *sl = zslCreate();
zskiplistNode *node = zslInsert(sl, array[0], array);
zskiplistNode *node2 = zslInsert(sl, array[1], array + 1);

</code></pre>
<p>上面的代码我们初始化了一个包含 10 个数字的数字，作为 obj 来插入列表
然后测试插入了两个元素，包括第一个 score 为 1 obj 为 1 的对象，以及第二个 score 为 2 obj 为 2 的对象。
接下来我们先分析下， <code>zslInsert</code> 到底做了什么。</p>
<pre><code class="language-c">zskiplistNode *zslInsert(zskiplist *zsl, double score, void *obj) {
    // x 是当前处理的节点
    // update 数组保存的是:
    // 小于 新节点的节点将指向新节点，大于新节点的节点将更新 span
    zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x;
    unsigned int rank[ZSKIPLIST_MAXLEVEL];
    int i, level;
    
    x = zsl-&gt;header; // 首先获取 header
    // 从当前 skiplist 的最高层开始查找合适的位置，因为越高层指向的目标就可能越远
    for (i = zsl-&gt;level-1; i &gt;= 0; i--) {
        // storea rank that is crossed to reach the insert position
        // 保存 rank ???
        rank[i] = i == (zsl-&gt;level-1) ? 0 : rank[i+1];
        
        // 如果新增对象的 score 小于下一个节点的 score
        // 或 score 相等但 compare 的结果小于下一节点的 obj
        // 这里使用下一节点是因为，当前节点是从 header 开始的，而 header 是存实际 obj 的
        while (x-&gt;level[i].forward &amp;&amp;
            (x-&gt;level[i].forward-&gt;score &lt; score ||
                (x-&gt;level[i].forward-&gt;score == score &amp;&amp;
                compareStringObjects(x-&gt;level[i].forward-&gt;obj, obj) &lt; 0)
            )) {
             
             // rank 加上当前节点当前层的跨度？
             rank[i] += x-&gt;level[i].span;
             x = x-&gt;level[i].forward;
         }
         // 保存所有节点到 update 中
         update[i] = x;
    }
    
    // 新建一个节点，给予一个随机的层级
    level = zslRandomLevel();
    // 如果新节点的层数大于现有的最大层，则更新现有的所有旧有层次
    if (level &gt; zsl-&gt;level) {
        for (i = zsl-&gt;level; i &lt; level; i++) {
            // 更新所有旧有层次，让其指向 header,
            // 并让所有第 i 层的 span 跨度设为 zsl 的节点数，也就是直接跨越到最后
            rank[i] = 0;
            update[i] = zsl-&gt;header;
            update[i]-&gt;level[i].span = zsl-&gt;length;
        }
        zsl-&gt;level = level; // 更新 skiplist 的最高层为
    }
    
    // 终于到创建新节点的这步了，创建一个 level 层的节点，并设置好 sroce 跟 obj
    x = zslCreateNode(level, score, obj); 
    
    // 更新新节点的低于旧有最高层的层次
    for (i = 0; i &lt; level; i++) {
        // 更新 x 的第 i 层节点的指向
        x-&gt;level[i].forward = update[i]-&gt;level[i].forward;
        update[i]-&gt;level[i].forward = x;
        
        x-&gt;level[i].span = update[i]-&gt;level[i].span - (rank[0] - rank[i]);
        update[i]-&gt;level[i].span = (rank[0] - rank[i]) + 1;
    }
    
    // 将所有高于新节点的层的跨度增加 1
    for (i = level; i &lt; zsl-&gt;level; i++) {
        update[i]-&gt;level[i].span++;
    }
    
    // 更新新节点的后退指针，如果是第一层，则设置为 NULL(因为没有上一层了）
    // 否则设置为 update[0] ??
    x-&gt;backward = (update[0] == zsl-&gt;header) ? NULL : update[0];
    // 如果有下一个节点，则将下一个节点的后退指针设为新节点
    if (x-&gt;level[0].forward)
        x-&gt;level[0].forward-&gt;backward = x;
    else
        // 如果没有下一个节点，说明是最后一个节点
        zsl-&gt;tail = x; 
        
    zsl-&gt;length++;
    return x;
}
</code></pre>
<p>redis 的 skiplist 的插入代码较长，所以我们分段进行分析，并且在分析的时候已我们的测试代码为准，如我们现在即将调用的</p>
<pre><code class="language-c">// array[0] = 1
// array    = 1 
zskiplistNode *node = zslInsert(sl, array[0], array);
</code></pre>
<p>首先从 <a href="print.html#init_state">初始化图</a> 可以得知 skiplist 现在的状态，接下来逐步分析插入的代码，
我们向 sl 插入了 score 为 1，obj 指向 1 的信息，接下来进入函数的第一步骤</p>
<pre><code class="language-c">x = zsl-&gt;header;
// 当前的 level 是 1, 所以只会循环一次，并且 i = 0
for (i = zsl-&gt;level-1; i &gt;= 0; i--) {
    // 所以这里的 rank[i] = 0;
    rank[i] = i == (zsl-&gt;level-1) ? 0 : rank[i+1];    
    
    // 而这里的 forward 一开始是为 NULL 的，所以不会进入循环
    while (x-&gt;level[i].forward &amp;&amp;
        (x-&gt;level[i].forward-&gt;score &lt; score ||
            (x-&gt;level[i].forward-&gt;score == score &amp;&amp;
            compareStringObjects(x-&gt;level[i].forward-&gt;obj, obj) &lt; 0)
        )) {
         
         rank[i] += x-&gt;level[i].span;
         x = x-&gt;level[i].forward;
     }
     
     update[i] = x;
}
</code></pre>
<p>所以执行完之后，各变量的状态转为</p>
<pre><code class="language-c">x      = header;
update = [ header, NULL, ... ];
rank   = [ 0, 0, 0, ... ];
</code></pre>
<p>并假设新节点的层级由随机数得到 3，则下面的第二步骤的具体细节为</p>
<pre><code class="language-c">level = zslRandomLevel(); // 假设为 3
// 当前 zsl-&gt;level 为 1， 所以进入循环
if (level &gt; zsl-&gt;level) {
    // 这边的循环则是更新指定的 update 跟 rank
    for (i = zsl-&gt;level; i &lt; level; i++) {
        // 更新所有旧有层次，让其指向 header,
        // 并让所有第 i 层的 span 跨度设为 zsl 的节点数，也就是直接跨越到最后
        rank[i] = 0;
        update[i] = zsl-&gt;header;
        update[i]-&gt;level[i].span = zsl-&gt;length;
    }
    zsl-&gt;level = level; // 更新 skiplist 的最高层为
}
</code></pre>
<p>执行完后，各变量的状态转为</p>
<pre><code class="language-c">zsl-&gt;level = 3;
update = [ header, header, header, NULL, ... ];
rank   = [0, 0, 0, ... ];
header-&gt;level[1].span = 1;
header-&gt;level[2].span = 1;
</code></pre>
<p>接下来是插入的最后一个步骤了，这里会依据 update 的内容来更新 skiplist，并且会往其中加入新节点</p>
<pre><code class="language-c">// 创建新节点
x = zslCreateNode(level, score, obj); 
    
for (i = 0; i &lt; level; i++) {
    // 将新节点的各层的 forward 设置为对应 update 的 forward
    // 并将原有 update 节点的 forward 指向新节点
    x-&gt;level[i].forward = update[i]-&gt;level[i].forward;
    update[i]-&gt;level[i].forward = x;
    
    // 将新节点各层的 span 设置为原有节点对应层的 span 并减去 rank[0] - rank[i];
    x-&gt;level[i].span = update[i]-&gt;level[i].span - (rank[0] - rank[i]);
    // 然后更新原有 update 对应层的 span 为 rank[0] - rank[i] + 1，也就是对应的 span 加上1 
    update[i]-&gt;level[i].span = (rank[0] - rank[i]) + 1;
}
    
// 将所有高于新节点的层的跨度增加 1
for (i = level; i &lt; zsl-&gt;level; i++) {
    update[i]-&gt;level[i].span++;
}
    
// 更新新节点的后退指针，如果是第一层，则设置为 NULL(因为没有上一层了）
// 否则设置为 update[0] ??
x-&gt;backward = (update[0] == zsl-&gt;header) ? NULL : update[0];
// 如果有下一个节点，则将下一个节点的后退指针设为新节点
if (x-&gt;level[0].forward)
    x-&gt;level[0].forward-&gt;backward = x;
else
    // 如果没有下一个节点，说明是最后一个节点
    zsl-&gt;tail = x; 
    
zsl-&gt;length++;
return x;
</code></pre>
<p>这次调用马上结束了，最后来看看这次的调用结果，将 zsl 这个 skiplist 变成什么样了，</p>
<pre><code class="language-json">x = {
    backward: NULL,
    score   : 3,
    obj     : 3,
    level   : [ NULL, ... ]
}

zsl = {
    leve  : 3
    length: 1
    header: -----&gt;  [ score ] = 0
    tail  : x       [ obj   ] = NULL
                    [ level ] = [ ... ]       
}                                                                   
                           
zsl-&gt;header.level = [                        
    {                            ____            
        span: 1, forward ----&gt;  |    |
    },                          |  x |
    {                           |  3 |
        span: 1, forward ----&gt;  |  3 |
    },                          |    |
    {                           |    |
        span: 1, forward ----&gt;  |____|
    }
]
</code></pre>
<p>接下来分析第二次插入时的情况，这次我们就不逐步分析，而是直接查看插入后的结果了。
首先是对第一步骤的分析，我们现在要插入的节点是 score = 3, obj = 3，在执行完第一步骤后继续执行第二步骤，根据新节点的随机 level 填充 update 跟更新 skiplist 的 level, 我们假设新节点的随机层数为 2 ，则执行代码</p>
<pre><code class="language-c">zslInsert(zsl, array[2], array + 2); // 3, 3
</code></pre>
<!--
x = header;
zsl->level = 3;
rank = [ 0, 0, 0, ... ]
<p>第一次循环 i == 2</p>
<p>rank[2] = 0;
rank[2] = x-&gt;level[2].span = 1
x = x-&gt;level[2].forward = { score: 1, obj: 1, level: [ NULL, ... ] }
rank = [0, 0, 1, ...]
update = [ NULL, NULL, { 1, 1 }, ...]</p>
<p>第二次循环 i == 1
rank[1] = rank[2] = 1
update[1] = x = { 1, 1 }
rank = [0, 1, 1]
update = [ NULL, { 1, 1 }, { 1, 1}, ... ]</p>
<p>第三次循环 i == 0
rank[0] = rank[1] = 1
update[i] = { 1, 1 }
rank = [ 1, 1, 1 ]
update = [ { 1, 1 }, { 1, 1 }, { 1, 1 } ]</p>
<p>level = 2, 因为 level &lt; zsl-&gt;level 所以没进入循环
--&gt;</p>
<pre><code class="language-json">x = {                       zsl = {
    backward: NULL,             level: 3,
    score: 3,                   length: 1,
    obj  : 3,                   header: ------&gt; header,
    level: [ empty, ... ]    };
};                           

level = 2
rank = [ 1, 1, 1, ... ]
// update 列表中对象是指 { level, score, obj }
update = [ { 3, 3, 3 }, { 3, 3, 3 }, { 3, 3, 3 }, ... ]
</code></pre>
<p>第三步骤，则负责更新整个 update 对应的对象，以及新对象的指针</p>
<!--
for 0 -> 2
第一次循环 i = 0
x->level[0].forward = update[0]->level[0].forward; = NULL
update[0]->level[0].forward = x
x->level[0].span = update[0].level[0].span = 0 - (rank[0] - rank[0]) = 0
update[0].level[i].span = (rank[0] - rank[0] + 1 = 1
<p>总共三步循环都是把旧有节点指向 x，并更新对应的指针跟 rank</p>
<p>x-&gt;backward = (update[0]) = { 1, ,1 1 }
zsl-&gt;tail = x
zsl-&gt;length++;
--&gt;
<img src="./image/skiplist_status1.jpg" alt="状态1" /></p>
<p>下面我们继续插入新的数据节点，这次插入的是另一个节点</p>
<pre><code class="language-c">zslInsert(zsl, array[1], array + 1); // 2, 2
</code></pre>
<p>并且我们假设其随机生成的层数 level 为 4 层，则插入之后 skiplist 的状态为</p>
<!--
x = header;
循环3次
i == 2
rank[2] = 0;
rank[2] = 1
x = { 1, 1, 1 }
update[2] = { 1, 1, 1 }
<p>i == 1
rank[1] = 1
x = { 1, 1, 1 }
update[1] = { 1, 1, 1 }</p>
<p>i == 0
rank[i] = 1
x = { 1, 1, 1 }
update[0] = { 1, 1, 1 }</p>
<p>level = 4
for ( i = 3 ; i &lt; 4; i++) {
update...
}
rank[3] = 0;
update[3] = header
update[3]-&gt;level[3].span = 2</p>
<p>rank = [1, 1, 1, 0, ...]
update = [ {1,1,1}, {1,1,1}, {1,1,1}, {0,0,0} ]
zsl-&gt;level = 4</p>
<p>x = { 4, 2, 2 } // level 4, score 2, obj 2
for(i = 0; i &lt; 4; i++) {
}
x-&gt;level[0].forward = update[0]-&gt;level[0].forward;
x-&gt;level[0].forward = {1,1,1}[0].forward; = {3,3,3}</p>
<p>x-&gt;level[0].span = update[0]-&gt;level[0].span - (rank[0] - rank[0])
= {1,1,1}[0].span - 0 = 0</p>
<p>x-&gt;level[1].forward = update[1]-&gt;level[1].forward
x-&gt;level[1].forward = {1,1,1}[1].forward;
x-&gt;level[1].span = update[1]-&gt;level[1].span - (rank[0] - rank[1])
= {1,1,1}[1].span - (1-1) = 0</p>
<p>x-&gt;level[2].forward = update[2]-&gt;level[2].forward;
x-&gt;level[2].forward = {1,1,1}[2].forward;
x-&gt;level[2].span = update[2]-&gt;level[2].span - (rank[0] - rank[2])
= {1,1,1}[2].span - (1-1) = 0</p>
<p>x-&gt;level[3].forward = update[3]-&gt;level[3].forward
= header[3].forward = NULL<br />
x-&gt;level[3].span = header[3].span - (0 - rank[3]) = 1</p>
<p>--&gt;</p>
<p><img src="./image/skiplist_status2.jpg" alt="" /></p>
<a class="header" href="print.html#a查找数据" id="a查找数据"><h3>查找数据</h3></a>
<p>插入一定量的数据之后，整个 skiplist 树已经趋于稳定状态，现在我们开始来介绍下查找数据的流程，同样的，我们还是以测试代码为驱动，来分析具体的查找流程，一下是测试代码</p>
<pre><code class="language-c">// 函数原型
zskiplistNode* zslGetElementByRank(zskiplist *zsl, unsigned long rank);

// 测试代码
zskiplistNode *node;
node = zslGetElementByRank(zsl, 2); // 获取排名第二的元素
</code></pre>
<p>因为整个 skiplist 都是有序的，所以最简单的查找方式就是从头开始找（<em>当然也可以从后面开始找，这样就可以换一种顺序来得到数据了，但我们为了简单只讨论第一种</em>），但因为 skiplist 为我们提供了指向多级节点之后的指针，我们才能提高查找的效率。
从上面的结构图我们可以看到，从层数来分析，层级越高的元素，能够跨越的距离就越远，所以在进行搜索的时候我们会倾向于从最高点开始往下找，这样就能充分利用 skiplist 为我们提供的效率。
接下来继续看看 skiplist 的查找实现</p>
<pre><code class="language-c">zskiplistNode* zslGetElementByRank(zskiplist *zsl, unsigned long rank) {
    zskiplistNode *x;
    unsigned long traversed = 0;
    int i;
    
    x = zsl-&gt;header;
    // 从当前最高层开始，遍历所有的层
    for (i = zsl-&gt;level-1; i &gt;= 0; i--) {
        // 如果当前节点的当前层跟下一节点的距离，小于我们想查找的位置，
        // 则将当前节点指向下一节点，并将已经跨越的距离加上当前节点跟下一节点的距离
        while (x-&gt;level[i].forward &amp;&amp; (traversed + x-&gt;level[i].span &lt;= rank)) {
            traversed += x-&gt;level[i].span;
            x = x-&gt;level[i].forward;
        }
        
        // 如果找到了对应的层级
        if (traversed == rank) {
            return x;
        }
    }
    return NULL;
}
</code></pre>
<p>具体的实现也是跟设定的逻辑一致，从最高层开始最小化查找的次数。</p>
<a class="header" href="print.html#a查找的另一种方式" id="a查找的另一种方式"><h4>查找的另一种方式</h4></a>
<p>接下来看另外一个实现，查找某个元素，因为保存的是 <code>void*</code> 指针，所以就导致了，必须提供自定义的比较函数，否则就会使用直接比较指针地址的方式。</p>
<pre><code class="language-c">// 函数原型
typedef int (cmpfunc)(void *x, void *y);
zskiplistNode* zslGetNode(zskiplist *zsl, void *obj, cmpfunc cmp);

int cmp(void *xp, void *yp) {
    int x = *(int *)xp;
    int y = *(int *)yp;
    
    if (x == y) { 
        return 0;
    }
    else if (x &lt; y) {
        return -1;
    }
    else {
        return 1;
    }
}

// 开始查找
int i = 3;
zskiplistNode *node = zslGetNode(zsl, &amp;i, cmp);

</code></pre>
<p>为了便于理解，我们首先把查找的过程以图形的方式画出。</p>
<p><img src="./image/skiplist_status3.jpg" alt="" /></p>
<p>查找方式跟之前的还是相同的，区别就只是不再按 score 查找，而是根据 obj 之间的 cmp 函数，来决定是要使用当前层往前找，还是使用第一层的指针往前找而已。下面是具体的查找代码</p>
<pre><code class="language-c">zskiplistNode* zslGetNode(zskiplist *zsl, void *obj, cmpfunc cmp) {
    zskiplistNode *x;
    int c;
    int i;
    
    x = zsl-&gt;header;
    for (i = zsl-&gt;level; i &gt;= 0; i--) {
    
        while ( (c = cmp(obj, x-&gt;level[i].forward-&gt;obj)) &lt; 0) {
            x = x-&gt;level[i].forward;
        }
        
        if (c == 0) { // found it!
            return x-&gt;level[i].forward;	
        }
    }
    
    return NULL;
}
</code></pre>
<a class="header" href="print.html#a删除" id="a删除"><h3>删除</h3></a>
<p>看完上面的所有介绍后，其实可以很容易的就联想到，关于 skiplist 的操作，基本都是基于其中的 update 指针，也就是那个指向指定节点 x 的前置节点集合。只要得到这个集合，要删除某个操作时，只需要将 update 指针指向 x 的节点，改成指向 x 对应层级的下一层就可以了，而高度高于 x 节点的，则只需要将 span 减少。
<em>redis 的源码中给出的是针对某个 score 的 robj 的比较</em></p>
<pre><code class="language-c">int zslDelete(zskiplist *zsl, double score, robj *obj) {
    zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x;
    int i;
    
    // 首先更新 update 为指向对应 score 跟 obj 节点的前置节点
    x = zsl-&gt;header;
    for (i = zsl-&gt;level-1; i &gt;= 0; i--) {
        while (x-&gt;level[i].forward &amp;&amp;
               (x-&gt;level[i].forward-&gt;score &lt; sroce ||
                (x-&gt;level[i].forward-&gt;score == score &amp;&amp;
                 compareStringObjects(x-&gt;level[i].forward-&gt;obj, obj) &lt; 0))) {
            x = x-&gt;level[i].forward;       
         }
         update[i] = x;
    }
    
    // 将 x 置为要删除的目标节点
    x = x-&gt;level[0].forward;
    // 最终判断最终找到的节点是否为目标对象
    if (x &amp;&amp; score == x-&gt;score &amp;&amp; equalStringObjects(x-&gt;obj, obj)) {
        // 调用 deleteNode 删除对应节点，然后释放内存
        zslDeleteNode(zsl, x, update);
        zslFreeNode(x);
        return 1;
    }
    return 0; // not found
}

void zslDeleteNode(zskiplist *zsl, zskiplistNode *x, zskiplistNode **update) {
    int i;
    for (i = 0; i &lt; zsl-&gt;level; i++) {
        // 如果层次低于或等于即将删除的节点，则更新指针以及跨度
        if (update[i]-&gt;level[i].forward == x) {
            update[i]-&gt;level[i].span += x-&gt;level[i].span - 1;
            update[i]-&gt;level[i].forward = x-&gt;level[i].forward;
        }
        else {
            // 如果层次高于即将删除的节点，则直接减少一个跨度
            update[i]-&gt;level[i].span -= 1;
        }                
    }
    
    // 更新即将删除节点的指针
    if (x-&gt;level[0].forward) {
        x-&gt;level[0].forward-&gt;backward = x-&gt;backward;
    }
    else {
        zsl-&gt;tail = x-&gt;backward;
    }
    
    // 如果删除节点的高度是 skiplist 的最高高度，则尝试调整 skiplist 的高度
    while (zsl-&gt;level &gt; 1 &amp;&amp; 
           zsl-&gt;header-&gt;level[zsl-&gt;level - 1].forward == NULL)  {
        zsl-&gt;level--;
    }    
}
</code></pre>
<a class="header" href="print.html#a整数集合" id="a整数集合"><h1>整数集合</h1></a>
<p>[TOC]</p>
<a class="header" href="print.html#a简介-2" id="a简介-2"><h2>简介</h2></a>
<p>整数集合，从名称即可得知是用来存放整数的集合，它相较于其他集合的区别在于：</p>
<ul>
<li>自动判断数据类型</li>
<li>压缩内存占用</li>
<li>保存的整数是有序的</li>
</ul>
<p>因为存放的始终只有整数，所以在存储数据时，会判断该数据的大小，并使用最小的尺寸来存放对应的数据，以达到节省内存的目的。
如集合 <code>Array = [ 1, 2, 3 ]</code> 可以只使用 8 位的 <code>char</code> 来保存。</p>
<p>简而言之即是，集合中的元素大小，以集合中最大元素的大小为准。</p>
<p>PS: 其实不大想写这个，觉得很无意义。</p>
<a class="header" href="print.html#a实现-3" id="a实现-3"><h2>实现</h2></a>
<a class="header" href="print.html#intset-的定义" id="intset-的定义"><h3>intset 的定义</h3></a>
<p><code>intset</code> 的定义很简单，只有三个字段</p>
<pre><code class="language-c">
// encoding 的类型定义
#define INTSET_ENC_INT16 (sizeof(int16_t))
#define INTSET_ENC_INT32 (sizeof(int32_t))
#define INTSET_ENC_INT64 (sizeof(int64_t))

typedef struct intset {
    uint32_t encoding;
    uint32_t length;
    int8_t contents[];
} intset;
</code></pre>
<p>第一个 <code>encoding</code> 用来表示该结构的编码，也就是上文说的元素大小，编码决定了 <code>intset</code> 中每个元素的大小，最后一个 <code>contents</code> 是一个变长的数组，根据保存数据的总数进行扩充，而第二个 <code>length</code> 则表示了 contents 的长度。</p>
<a class="header" href="print.html#a创建一个-intset" id="a创建一个-intset"><h3>创建一个 intset</h3></a>
<p>创建一个空的 <code>intset</code> 用于保存将要加入的整数</p>
<pre><code class="language-c">intset *intsetNew(void) {
    intset *is = zmalloc(sizeof(intset));
    is-&gt;encoding = intrev32ifbe(INTSET_ENC_INT16);
    is-&gt;length = 0;
    return is;
}
</code></pre>
<p>上面那一段唯一令人迷惑的，就只有 <code>intrev32ifbe(INTSET_ENC_INT16)</code>, 从源码可以了解到，这里其实只是为了将所有数字转换为 <code>little endian</code>。一系列的函数如 <code>memrev32ifbe</code>、<code>intrev32ifbe</code> 等都是 revert 32 bit if big endian 的意思。</p>
<pre><code class="language-c">#if (BYTE_ORDER == LITTLE_ENDIAN)
#define memrev16ifbe(p)
...
#define intrev16ifbe(v) (v)
...
#else
#define memrev16ifbe(p) memrev16(p)
...
#endif
</code></pre>
<a class="header" href="print.html#a添加一个整数" id="a添加一个整数"><h3>添加一个整数</h3></a>
<p>整数集合内部是有序的，所以每次添加都要为其保持集合的顺序，所以在添加的时候遵循以下步骤：</p>
<ul>
<li>判断新的数字是否超过了当前编码，是的话则先扩充</li>
<li>判断新的数字是否已存在当前集合，是的话直接返回，
<ul>
<li>具体的查找方式是使用二分法</li>
</ul>
</li>
<li>否则查找新的数字应该存在集合中的那个位置</li>
<li>然后插入对应的位置，并移动集合中受影响的元素的位置</li>
</ul>
<pre><code class="language-c">intset *intsetAdd(intset *is, int64_t value, uint8_t *success) {
    // 得到新数字的编码
    uint8_t valenc = _intsetValueEncoding(value);
    uint32_t pos;
    if (success) *success = 1;

    // 如果新整数的编码大于当前编码，则进行扩容
    if (valenc &gt; intrev32ifbe(is-&gt;encoding)) {
        return intsetUpdradeAndAdd(is, value);
    }
    else {
        // 判断新的整数是否已存在集合中，是的话则直接返回
        if (intsetSearch(is, value, &amp;pos)) {
            if (success) *success = 0;
            return is;
        }

        // 新整数，先扩容
        is = intsetResize(is, intrev32ifbe(is-&gt;length)+1);
        // 如果需要保存的位置，不在末端，则将对应位置之后的元素往后挪
        if (pos &lt; intrev32ifbe(is-&gt;length))
            intsetMoveTail(is, pos, pos+1);
    }

    // 在指定的 pos 填充新整数
    _intsetSet(is, pos, value);
    is-&gt;length = intrev32ifbe(intrev32ifbe(is-&gt;length)+1);
    return is;
}
</code></pre>
<a class="header" href="print.html#a确定添加整数的编码" id="a确定添加整数的编码"><h4>确定添加整数的编码</h4></a>
<p>大致的步骤跟上面总结的一致，接下来应该往细节看了。
获取编码的方式，直接使用对应类型的最大最小值来确定编码。</p>
<pre><code class="language-c">static uint8_t _intsetValueEncoding(int64_t v) {
    if (v &lt; INT32_MIN || v &gt; INT32_MAX)
        return INTSET_ENC_INT64;
    else if (v &lt; INT16_MIN || v &gt; INT16_MAX)
        return INTSET_ENC_INT32;
    else
        return INTSET_ENC_INT16;
}
</code></pre>
<a class="header" href="print.html#a判断新整数是否已存在" id="a判断新整数是否已存在"><h4>判断新整数是否已存在</h4></a>
<p>然后是查找对应的整数是否存在集合中，判断的逻辑为：</p>
<ul>
<li>先检查集合是否为空（废话。。。）</li>
<li>然后检查是否在最大值跟最小值的区间外，满足任一个条件都说明新整数不在集合中。</li>
<li>否则则使用二分查找法在集合中查找对应的元素。</li>
</ul>
<pre><code class="language-c">static uint8_t intsetSearch(intset *is, int64_t value, uint32_t *pos) {
    int min = 0, max = intrev32ifbe(is-&gt;length)-1, mid = -1;
    int64_t cur = -1;

    // 如果集合为空，则新整数可以作为集合的第一个元素存在
    if (intrev32ifbe(is-&gt;length) == 0) {
        if (pos) *pos = 0;
        return 0;
    }
    else {
        // 如果新整数大于最大值，则新整数可作为新元素直接加在末端
        if (value &gt; _intsetGet(is, intrev32ifbe(is-&gt;length)-1)) {
            if (pos) *pos = intrev32ifbe(is-&gt;length);
            return 0;
        }
        // 如果新整数小于最小值，则可作为第一个元素插入集合
        else if (value &lt; _intsetGet(is, 0)) {
            if (pos) *pos = 0;
            return 0;
        }
    }

    // 如果前面的检查都不符合，则使用二分查找法查找新元素是否存在，并更新如果不存在时，它应该存在的位置
    while (max &gt;= min) {
        mid = ((unsigned int)min + (unsigned int)max) &gt;&gt; 1;
        cur = _intsetGet(is, mid);
        if (value &gt; cur) {
            min = mid + 1;
        }
        else if (value &lt; cur) {
            max = mid - 1;
        }
        else {
            break;
        }
    }

    if (value == cur) {
        if (pos) *pos = mid;
        return 1;
    }
    else {
        if (pos) *pos = min;
        return 0;
    }
}
</code></pre>
<a class="header" href="print.html#a获取指定索引的值" id="a获取指定索引的值"><h4>获取指定索引的值</h4></a>
<p>上面在做检查的时候需要获取指定索引的值来进行比较，以判断新的整数是否已存在集合中</p>
<pre><code class="language-c">static int64_t _intsetGet(intset *is, int pos) {
    return _intsetGetEncoded(is, pos, intrev32ifbe(is-&gt;encoding));
}

static int64_t _intsetGetEncoded(intset *is, int pos, uint8_t enc) {
    int64_t v64;
    int32_t v32;
    int16_t v16;

    if (enc == INTSET_ENC_INT64) {
        memcpy(&amp;v64, ((int64_t *)is-&gt;contents) + pos, sizeof(v64));
        memrev64ifbe(&amp;v64);
        return v64;
    }
    else if (enc == INTSET_ENC_INT32) {
        memcpy(&amp;v32, ((int32_t *)is-&gt;contents) + pos, sizeof(v32));
        memrev32ifbe(&amp;v32);
        return v32;
    }
    ele {
        memcpy(&amp;v16, ((int16_t *)is-&gt;contents) + pos, sizeof(v16));
        memrev16ifbe(&amp;v16);
        return v16;
    }
}
</code></pre>
<p>上面还有一些 <code>memrev32ifbe</code> 之类的调用，就不一一细说了，具体做的就是根据系统平台的定义，把所有 little endian 的转换为 big endian 的表示，具体的做法就是交换内存位置，如 <code>abc -&gt; cba</code></p>
<a class="header" href="print.html#a调整-intset-的大小" id="a调整-intset-的大小"><h4>调整 intset 的大小</h4></a>
<p>在确定需要把新的整数加入集合后， <code>intset</code> 即调整现有的集合大小，以便容纳新的元素，具体的操作都在 <code>intsetResize</code> 中。</p>
<pre><code class="language-c">static intset *inisetResize(intset *s, uint32_t len) {
    uint32_t size = len * intrev32ifbde(is-&gt;encoding);
    is = zrealloc(is, sizeof(intset) + size);
    return is;
}
</code></pre>
<a class="header" href="print.html#a重排位置" id="a重排位置"><h4>重排位置</h4></a>
<p>调整完大小后，则根据新元素应该所处的位置，挪动其他元素对应的位置，如要在一个长度为 10 的集合中的 2 这个位置插入新元素，则需要把原本的 2-10 的元素移到 3-11, 然后才能往 2 中插入新元素。</p>
<pre><code class="language-c">static void intsetMoveTail(intset *is, uint32_t from, uint32_t to) {
    void *src, *dst;
    uint32_t bytes = intrev32ifbe(is-&gt;length) - from;
    uint32_t encoding = intrev32ifbe(is-&gt;encoding);

    if (encoding == INTSET_ENC_INT64){
        src = (int64_t *)is-&gt;contents + from;
        dst = (int64_t *)is-&gt;contents + to;
        bytes * sizeof(int64_t);
    }
    else if (encoding == INTSET_ENC_INT32) {
        src = (int32_t *)is-&gt;contents + from;
        dst = (int32_t *)is-&gt;contents + to;
        bytes * sizeof(int32_t);
    }
    else {
        src = (int16_t *)is-&gt;contents + from;
        dst = (int16_t *)is-&gt;contents + to;
        bytes * sizeof(int16_t);
    }

    memmove(dst, src, bytes);
}
</code></pre>
<a class="header" href="print.html#a插入新元素" id="a插入新元素"><h4>插入新元素</h4></a>
<p>终于到了最后一步了，经过上面那么多的准备之后，现在有了空间来让我们放新元素了，也确定了新的整数应该放到哪个位置，同时也把相关的内存空间都挪好了，最后一步就是把元素放到对应的位置。</p>
<pre><code class="language-c">static void _intsetSet(intset *is, int pos, int63_t value) {
    uint32_t encoding intrev32ifbe(is-&gt;encoding);

    if (encoding == INTSET_ENC_INT64) {
        ((int64_t *)is-&gt;contents)[pos] = value;
        memrev64ifbe((int64_t *)is-&gt;contents) + pos);
    }
    else if (encoding == INTSET_ENC_INT32) {
        ((int32_t *)is-&gt;contents)[pos] = value;
        memrev32ifbe((int32_t *)is-&gt;contents) + pos);
    }
    else {
        ((int16_t *)is-&gt;contents)[pos] = value;
        memrev16ifbe((int16_t *)is-&gt;contents) + pos);
    }
}
</code></pre>
<a class="header" href="print.html#a升级-intset" id="a升级-intset"><h3>升级 intset</h3></a>
<p>在插入新数据的时候我们已经见到了这行代码 <code>intsetUpgradeAndAdd</code>，在新的整数大小超过了现有编码时，自动对当前的数据进行扩容，调整存放在集合中的所有元素，将其全部转换为新元素对应的编码。</p>
<pre><code class="language-c">static intset *intsetUpgradeAndAdd(intset *is, int64_t value) {
    // 获取新的跟旧的元素编码
    uint8_t curenc = intrev32ifbe(s-&gt;encoding);
    uint8_t newenc = _intsetValueEncoding(value);
    int length = intrev32ifbe(is-&gt;length);

    //
    // 进入扩充的情况只有：
    // 1. value 只能比当前集合中编码的最大值大
    // 2. value 只能比当前集合中编码的最小值小
    // value &lt; 0 说明他是因为第 2 种情况需要扩充，所以添加到最前
    // value &gt; 9 说明他是因为第 1 种情况需要扩充，所以添加到最末
    int prepend = value &lt; 0 ? 1 : 0;

    // 重新分配空间
    is-&gt;encoding = intrev32ifbe(newenc);
    is = intsetResize(is, intrev32ifbe(is-&gt;length) + 1);

    // 使用旧编码从末尾开始获取旧的元素，然后使用新的编码填充
    // 这样就不用担心覆盖了其他元素了，prepend 则用来保证有一个空位来存放新元素
    while (length--)
        _intsetSet(is, length + prepend, _intsetGetEncoded(is, length, curenc));

    if (prepend)
        _intsetSet(is, 0, value);
    else
        _intsetSet(is, intrev32ifbe(is-&gt;length), value);

    if-&gt;length = intrev32ifbe(intrev32ifbe(is-&gt;length) + 1);
    return is;
}
</code></pre>
<a class="header" href="print.html#a查找元素" id="a查找元素"><h3>查找元素</h3></a>
<p>很遗憾，已经在添加元素的地方说过了。</p>
<a class="header" href="print.html#a删除元素" id="a删除元素"><h3>删除元素</h3></a>
<p>找到对应的元素后，挪动相关联的所有元素。</p>
<pre><code class="language-c">intset *intsetRemove(intset *is, int64_t value, int *success) {
    uint8_t valenc = _intsetValueEncoding(value);
    uint32_t pos;
    if (success) *success = 0;

    if (valenc &lt;= intrev32ifbe(is-&gt;encoding) &amp;&amp; intsetSearch(is, value, &amp;pos)) {
        uint32_t len = intrev32ifbe(is-&gt;length);
        if(success) *success = 1;

        if (pos &lt; (len-1))
            intsetMoveTail(is, pos + 1, pos);
        is = intsetResize(is, len-1);
        is-&gt;length = intrev32ifbe(len-1);
    }
    return is;
}
</code></pre>
<a class="header" href="print.html#a其他" id="a其他"><h3>其他</h3></a>
<a class="header" href="print.html#a获取集合长度" id="a获取集合长度"><h4>获取集合长度</h4></a>
<pre><code class="language-c">uint32_t intsetLen(intset *is) {
    return intrev32ifbe(is-&gt;length);
}
</code></pre>
<a class="header" href="print.html#a压缩列表-ziplist" id="a压缩列表-ziplist"><h1>压缩列表 <code>ziplist</code></h1></a>
<p>[TOC]</p>
<a class="header" href="print.html#a简介-3" id="a简介-3"><h2>简介</h2></a>
<p>压缩列表是一个通用的线性列表，主要的用途是压缩内存的使用，不像其他的各种通用列表，其中的所有元素都只能用指针来表示，压缩列表为各种类型及各种尺寸的元素提供了不同的保存形式，以尽量节约各种元素占用的内存。
最便捷的使用方式应该是，针对自己的不同类型，将 <code>ziplist</code> 再进行一层简单的封装，以提高使用上的便捷跟内存的节约。
而作为一个列表，他的操作基本都是以 <code>O(n)</code> 的方式存在，所以对于有效率要求的，应该使用 <code>skiplist</code> 或者 <code>vector</code> 等类型。</p>
<a class="header" href="print.html#a实现-4" id="a实现-4"><h2>实现</h2></a>
<a class="header" href="print.html#ziplist-的定义" id="ziplist-的定义"><h3><code>ziplist</code> 的定义</h3></a>
<p>压缩列表在表现形式上只是一个字节数组，所以无需多余的类型定义，所有的操作都基于分配的内存块以及操作接口即可。</p>
<a class="header" href="print.html#a创建一个-ziplist" id="a创建一个-ziplist"><h3>创建一个 <code>ziplist</code></h3></a>
<p>因为没有确切的结构定义，所以这次我们只能从 <code>ziplist</code> 的初始化函数中来推敲其内存结构，先从代码开始</p>
<pre><code class="language-c">#define ZIPLIST_BYTES(zl)       (*((uint32_t *)(zl)))
#define ZIPLIST_TAIL_OFFSET(zl) (*((uint32_t *)((zl) + sizeof(uint32_t))))
#define ZIPLIST_LENGTH(zl)      (*((uint32_t *)((zl) + sizeof(uint32_t) * 2)))
#define ZIPLIST_HEADER_SIZE     (sizeof(uint32_t) * 2 + sizeof(uint16_t))
#define ZIPLIST_ENTRY_HEAD      ((zl) + ZIPLIST_HEADER_SIZE)
#define ZIPLIST_ENTRY_TAIL(zl)  ((zl) + intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl)))
#define ZIPLIST_ENTRY_END(zl)   ((zl) + intrev32ifbe(ZIPLIST_BYTES(zl)) - 1)

unsigned char *ziplistNew(void) {
    // 结构的 header 大小，确定是 4 * 2 + 2 + 1 = 10 byte
    unsigned int bytes = ZIPLIST_HEADER_SIZE + 1;
    unsigned char *zl = zmalloc(bytes);
    
    // 设置前四个字节为 header 的长度
    ZIPLIST_BYTES(zl) = intrev32ifbe(bytes);
    // 设置 tailoffset 为 header 的长度
    ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(ZIPLIST_HEADER_SIZE);
    // 设置第 9-10 个字节为 0，可以看出 9-10 表示长度
    ZIPLIST_LENGTH(zl) = 0;
    // 最后一个位用特殊的位表示结束
    zl[bytes-1] = ZIP_END;
    return zl;
}
</code></pre>
<p>经过这一步，可以大致确定空的 ziplist 结构为：</p>
<p><img src="./image/ziplist_header.jpg" alt="" /></p>
<a class="header" href="print.html#a往-ziplist-中插入新数据" id="a往-ziplist-中插入新数据"><h3>往 ziplist 中插入新数据</h3></a>
<p>接下来尝试往 <code>ziplist</code> 中加入新的元素，加入元素前先看看具体的添加元素的接口定义。</p>
<pre><code class="language-c">#define ZIPLIST_HEAD 0
#define ZIPLIST_TAIL 1

unsigned char *ziplistPush(unsigned char *zl, unsigned char *s, unsigned int slen, int where) {
    unsigned char *p;
    p = (where == ZIPLIST_HEAD) ? ZIPLIST_ENTRY_HEAD(zl) : ZIPLIST_ENTRY_END(zl);
    return __ziplistInsert(zl, p, s, slen);
}

</code></pre>
<p>从以上定义可以大致得到接口的解释：</p>
<ul>
<li>zl 是 <code>ziplist</code> 的指针</li>
<li>s 是新元素的指针</li>
<li>slen 是新元素的大小</li>
<li>where 是新元素要添加的位置，添加元素只能添加到 head 或者 tail, 所以他只能是以下两种取值之一：
<ul>
<li>ZIPLIST_HEAD</li>
<li>ZIPLIST_TAIL</li>
</ul>
</li>
</ul>
<p>接着做的是根据 where 找到对应的 head 或者 tail 元素， 并将其保存在指针 p 之中，然后转调  <code>__ziplistInsert</code> 函数，从名称即可看出它是负责将新元素插入到 <code>ziplist</code> 的元素旁*（前或后？）*</p>
<p>接着我们先看看怎么获取 head 或者 tail 元素。</p>
<pre><code class="language-c">// 获取 head 元素
#define ZIPLIST_ENTRY_HEAD(zl) ((zl) + ZIPLIST_HEADER_SIZE)
// 获取 tail 元素
#define ZIPLIST_ENTRY_TAIL(zl) ((zl) + intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl)))
</code></pre>
<p>获取 head 的操作很简单直接将指针加上 header 的尺寸，也就是 10。那也就是说，头元素会放在 length 跟 end 之间</p>
<p><img src="./image/ziplist_header_head.jpg" alt="" /></p>
<p>而获取 tail 的操作则是将指针加上 tail_offset 这个字段的值，说明了 <code>tail_offset</code> 是用来保存整个 <code>ziplist</code> 所有元素的大小。</p>
<p>接着进入插入元素的逻辑</p>
<pre><code class="language-c">#define ZIP_DECODE_PREVLENSIZE(ptr, prevlensize) do {       \
    if ((ptr)[0] &lt; ZIP_BIGLEN) {                            \
        (prevlensize) = 1;                                  \
    }                                                       \
    else {                                                  \
        (prevlensize) = 5;                                  \
    }                                                       \
while (0);                                                  \
    

#define ZIP_DECODE_PREV(ptr, prevlensize, prevlen) do {    \
    ZIP_DECODE_PREVLENSIZE(ptr, prevlensize);              \
    if ((prevlensize) == 1) {                              \
        (prevlen) = (ptr)[0];                              \
    }                                                      \
    else if ((prevlensize) == 5) {                         \
        assert (sizeof((prevlensize)) == 4);               \
        memcpy(&amp;(prevlen), ((char *)(ptr) + 1, 4);         \
        memrev32ifbe(&amp;prevlen);                            \
    }                                                      \
} while (0);                                               \

static unsigned char *__ziplistInsert(unsigned char *zl, unsigned char *p, unsigned char *s, unsigned int slen) {
    size_t curlen = intrev32ifbe(ZIPLIST_BYTES(zl)), reqlen;
    unsigned int prevlensize, prevlen = 0;
    size_t offset;
    int nextdiff = 0;
    unsigned char encoding = 0;
    long long value = 123456789;
    
    zlentry tail;
    
    // 如果指定的位置不是结束标示符，则说明列表不为空
    if (p[0] != ZIP_END) {
        // 获取 p 前一个节点长度的类型以及前一个节点的长度
        // 类型有两种，一种是 1 个字节的，一种是 5 个字节的，取决于 p[0] 是否大于 ZIP_BIGLEN (254)
        ZIP_DECODE_PREV(p, prevlensize, prevlen);
    }
    // 如果要插入的位置是最后，则确认 ziplist 是否为空
    // 如果不为空，则获取当前列表的最后一个节点的长度，并保存
    // 到 prevlen 中
    else {
        unsigned char *ptail = ZIPLIST_ENTRY_TAIL(zl);
        if (ptail[0] != ZIP_END) {
            prevlen = zipRawEntryLength(ptail);
        }
    }
    
    // ...   
}
</code></pre>
<p>上文中的省略号是即将处理的插入逻辑，我们先停下来，看看 <code>zipRawEntryLength</code> 的具体实现，他是为了获取指定节点所占用的内存大小，所以在这里，我们可以大概的探索一下节点的具体结构。</p>
<a class="header" href="print.html#a探索-ziplist-中存储元素的结构" id="a探索-ziplist-中存储元素的结构"><h4>探索 ziplist 中存储元素的结构</h4></a>
<pre><code class="language-c">statuc unsigned int zipRawEntryLength(unsigned char *p) {
    unsigned int prevlensize, encoding, lensize, len;
    // 解码上一个节点的长度表示，1 或 5 个字节
    ZIP_DECODE_PREVLENSIZE(p, prevlensize);
    ZIP_DECODE_LENGTH(p + prevlensize, encoding, encoding, lensize, len);
    return prevlensize + lensize + len;
}
</code></pre>
<p>从上面的实现大致可以推算出，一个 entry 的内容由三个部分组成，一个是前一个节点的长度尺寸（<em>这里之所以用尺寸，是因为他不是指上一个节点的大小，而是指上一个节点使用了多少个字节</em>）+ 当前节点的长度尺寸 + 当前节点的内容尺寸。</p>
<p>首先是如何看上一个节点的尺寸, 从第一个字节的大小来判断上一个节点的大小，这里只分两种情况，一种是 1 个字节，另一种是 5 个字节</p>
<pre><code class="language-c">#define ZIP_DECODE_PREVLENSIZE(ptr, prevlensize) do { \
    if ((ptr)[0] &lt; ZIP_BIGLEN) {                      \
        (prevlensize) = 1;                            \
    } else {                                          \
        (prevlensize) = 5;                            \
    }                                                 \
} while (0);                                          \        
</code></pre>
<p>接下来我们看到他使用 <code>p + prevlensize</code> 作为 <code>ZIP_DECODE_LENGTH</code> 的参数，说明 <code>[p, p + prevlensize]</code> 这个区间存放的就是上一个节点的长度，而这个长度分两种类型，小于 <code>ZIP_BIGLEN</code> 的使用一个字节，第一个字节存放一个特殊值，剩下的 4 个字节就是大于 <code>ZIP_BIGLEN</code> 的长度表示方式。</p>
<pre><code class="language-c">
// 0xC0 = 1100 0000 与下面的 0x3F = 0011 1111 相呼应
#define ZIP_STR_MASK 0xC0            // 192 
#define ZIP_INT_MASK 0x30            // 48
#define ZIP_STR_06B  (0 &lt;&lt; 6)        // 0
#define ZIP_STR_14B  (1 &lt;&lt; 6)        // 64
#define ZIP_STR_32B  (2 &lt;&lt; 6)        // 128
#define ZIP_INT_16B (0xC0 | 0 &lt;&lt; 4)  // 192
#define ZIP_INT_32B (0xC0 | 1 &lt;&lt; 4)  // 192 | 16 = 208
#define ZIP_INT_64B (0xC0 | 2 &lt;&lt; 4)  // 192 | 32 = 224
#define ZIP_INT_24B (0xC0 | 3 &lt;&lt; 4)  // 192 | 48 = 240
#define ZIP_INT_8B  0xFE             // 254

#define ZIP_INT_IMM_MASK 0x0F        // 15
#define ZIP_INT_IMM_MIN  0xF1        // 241   
#define ZIP_INT_IMM_MAX  0xFD        // 253
#define ZIP_INT_IMM_VAL(v) (v &amp; ZIP_INT_IMM_MASK)

// 获取 entry 的编码
#define ZIP_ENTRY_ENCODING(ptr, encoding) do {  \
    (encoding) = (ptr)[0];                      \
    if ((encoding) &lt; ZIP_STR_MASK)              \
        (encoding) &amp;= ZIP_STR_MASK;             \
} while (0);

#define ZIP_DECODE_LENGTH(ptr, encoding, lensize, len) do { \
    ZIP_ENTRY_ENCODING((ptr), (encoding));               \
    if ((encoding) &lt; ZIP_STR_MASK) {                     \
        // ZIP_STR_06B 编码，以一个字节保存长度，并且总长度    \
        // 小于等于 63 -&gt; MAX = 63
        // &amp; 0x3f 的意思就是，放弃用于判断编码的高 2 位
        // 因为最高的两位用来放对应的 MASK 标示了
        if ((encoding) == ZIP_STR_06B) {                 \
            (lensize) = 1;                               \
            (len) = (ptr)[0] &amp; 0x3F;                     \
        // ZIP_STR_14B 编码，以两个字节保存长度，长度计算方式为 \
        // (byte1 &amp; 63) * 256 + byte2 -&gt; MAX = 16383
        } else if ((encoding) == ZIP_STR_14B)) {         \
            (lensize) = 2;                               \
            (len) = (((ptr)[0] &amp; 0x3F) &lt;&lt; 8) | (ptr)[1]; \
        // ZIP_STR_32B 编码，以四个字节保存长度，长度计算方式为 \
        // byte1 * 16777216 + byte2 * 65536 +            \
        // byte3 * 256 + byte4                           \
        } else if ((encoding) == ZIP_STR_32B) {          \
            (lensize) = 5;
            (len) = ((ptr)[1] &lt;&lt; 24 |                    \
                    ((ptr)[2] &lt;&lt; 16 |                    \            
                    ((ptr)[3] &lt;&lt; 8  |                    \
                    ((ptr)[4]);                          \                        
        }                                                \
        } else {                                         \
            assert(NULL);                                \
        }                                                \
    } else {                                             \
        (lensize) = 1;                                   \
        (len) = zipIntSize(encoding);                    \
    }                                                    \
} while (0);                    
</code></pre>
<p>从获取编码的实现 <code>ZIP_ENTRY_ENCODING</code> 来看，第一个字节保存的就是编码的类型，而对编码类型的管理则主要分为两种类型，这两种类型以 192 为边界（即 <code>ZIP_STR_MASK</code>），小于他的即是 <code>STR</code> 编码，否则是 <code>INT</code> 编码，最后则是 <code>INT_IMM</code> 类型的编码。
所以在 <code>ZIP_DECODE_LENGTH</code> 的实现中，也是以 <code>ZIP_STR_MASK</code> 为边界，处理各种不同的编码。</p>
<p><strong>以下是 <code>STR</code> 编码的计算方式</strong></p>
<blockquote>
<p>ZIP_STR_06B 编码，以一个字节保存长度，并且总长度小于等于 63</p>
<blockquote>
<p><code>MAX = 63</code></p>
</blockquote>
</blockquote>
<blockquote>
<p>ZIP_STR_14B 编码，以两个字节保存长度，长度计算方式为</p>
<blockquote>
<p><code>(byte1 &amp; 63) * 256 + byte2 -&gt; MAX = 16383</code></p>
</blockquote>
</blockquote>
<blockquote>
<p>ZIP_STR_32B 编码，以四个字节保存长度，长度计算方式为</p>
<blockquote>
<p><code>byte1 * 16777216 + byte2 * 65536 + byte3 * 256 + byte4</code></p>
</blockquote>
</blockquote>
<p>最后还有 <code>Int</code> 类型的处理方式，他被实现在 <code>zipIntSize</code> 中，我们来看看具体的实现</p>
<pre><code class="language-c">static unsigned int zipIntSize(unsigned char encoding) {
    switch(encoding) {
    case ZIP_INT_8B:  return 1;
    case ZIP_INT_16B:  return 2;    
    case ZIP_INT_24B:  return 3;
    case ZIP_INT_32B:  return 4;
    case ZIP_INT_64B:  return 8;        
    default: return 0;
    }
    assert(NULL);
    return 0;
}
</code></pre>
<p>都是简单的通过类型来返回对应的字节数。
通过以上的定义，我们可以得出 entry 的大致定义：
<img src="./image/zip_entry_encoding.jpg" alt="" /></p>
<p>整个 entry 的大小取决于使用什么 <code>encoding</code>，当使用 <code>STR</code> 编码时，按照不同的类型来设置 <code>lensize</code> 跟 <code>len</code>，当使用 <code>INT</code> 编码时，则 lensize 始终为 1，len 则取决于使用整型的长度.</p>
<a class="header" href="print.html#a继续插入逻辑" id="a继续插入逻辑"><h3>继续插入逻辑</h3></a>
<p>经过上面一大坨的分析 <code>entry</code> 结构跟获取前一个节点的长度之后，我们继续上文插入新元素的逻辑。</p>
<pre><code class="language-c">static unsigned char *__ziplistInsert(unsigned char *zl, unsigned char *p, unsigned char *s, unsigned int slen) {
    // ....
    
    // 尝试对要插入的元素进行编码, 具体的做法是尝试把 s 转换为数字，
    // 然后根据数值大小判断使用什么编码
    if (zipTryEncoding(s, slen, &amp;value, &amp;encoding)) {
        // 编码成功，则说明 s 可以转换为数字，以 INT 类型的编码确定 s 压缩后的尺寸
        reqlen = zipIntSize(encoding);
    }
    else {
        // 如果编码成 INT 失败，则直接使用字符串 s 的长度 slen
        reqlen = slen;
    }
    
    // 接着根据 prevlen 尝试对 prevlen 进行编码，并返回编码的长度
    // 该函数的实现是：
    //      如果第一个参数不为 null，则将具体的长度写入第一个参数
    //      否则直接返回 prevlen 使用的长度    
    reqlen += zipPrevEncodeLength(NULL, prevlen);
    
    // 根据 encoding 跟 slen 编码长度信息，分析得出要使用什么编码来保存长度信息。
    // 具体的实现为：
    //      如果第一个参数不为空，会把编码后的长度信息写入第一个参数，
    //      否则直接返回编码长度这里是直接返回编码后的长度
    reqlen += zipEncodeLength(NULL, encoding, slen);
    
    // 如果插入点不是最后一个元素，则要先将下一个节点的 prevlensize 减去插入点的长度
    // 比如旧的 prev 节点的要用 5 个字节来保存，而新节点的长度只需要 1 个字节，
    // 那 p 的 上关于前一个节点长度的信息就要调整为 1 个字节，也就是要缩减 4 个字节
    // 所以得到 1 - 5 == -4
    nextdiff = (p[0] != ZIP_END) ? zipPrevLenByteDiff(p, reqlen) : 0;
    
    // 保存 p 在现有 zl 的偏移量，因为接下来的 realloc 中，可能会改变 zl 的内存地址j
    offset = p - zl;
    
    //  重新调整内存大小，调整为：
    //      当前 zl 的长度 + 新节点的长度 + p 新旧前置节点的长度差（可为负数）
    zl = ziplistResize(zl, curlen + reqlen + nextdiff);
    
    // 重新得到要插入新节点的位置信息
    p = zl + offset;
    
    // ...
}
</code></pre>
<p>上面代码的逻辑已经在注释中解释完了，基本就是：</p>
<ul>
<li>计算新节点的前置长度所需的字节数</li>
<li>计算新节点的长度及内容所需的字节数</li>
<li>计算新旧前置节点长度的差值</li>
<li>重新调整内存</li>
</ul>
<p>经过以上处理后，所有东西都已经就绪，可以开始将新节点插入 <code>ziplist</code> 了。</p>
<pre><code class="language-c">static unsigned char *__ziplistInsert(unsigned char *zl, unsigned char *p, unsigned char *s, unsigned int slen) {
    // ....
    
    // 如果插入点不为 zl 的结尾
    if (p[0] != ZIP_END) {
        // 将旧有的 p 节点移动到新的位置中，如果新节点与 p 的旧有前置节点长度有差值，
        // 则把差值部分也复制过去
        memmove(p + reqlen, p - nextdiff, curlen - offset - 1 + nextdiff);
        
        // 然后设置 p 的前置节点信息为新插入的节点, 完成这一步后， 
        // 查新信息已被弥补，p 重新成为一个完整的节点
        zipPrevEncodeLength(p + reqlen, reqlen);
        
        // 然后更新 zl 的最后节点的偏移量信息，也就是用来获取最后一个节点的偏移量
        // 其实就是将原有的长度，加上新节点的长度
        ZIPLIST_TAIL_OFFSET(zl) = 
            intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl) + reqlen);
            
        // 如果新节点之后的节点
        tail = zipEntry(p + reqlen);
        // 如果新节点的后续节点不为最末的节点，则还要再加上之前新旧两个前置节点的长度差
        if (p[reqlen + tail.headersize + tail.len] != ZIP_END) {
            ZIPLIST_TAIL_OFFSET(zl) =
                intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl)) + nextdiff);
        }
    }
    else {
        // 这里表示 新的节点即是最终节点
        ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(p - zl);
    }
    
    // 如果有差值， p 后续的节点都可能需要重新调节前一个节点的尺寸
    // __ziplistCasecadeUpdate 即是调整指定节点及其之后的所有节点对应的
    // prevlensize 这个字段，直到遇见一个无需调整的。
    if (nextdiff != 0) {
        先获取 p 的 offset，jjj
        offset = p - zl;
        zl = __ziplistCasecadeUpdate(zl, p + reqlen);
        p = zl + offset;
    }
    
    // 最终插入新节点
    // 首先在 p 上面插入前一个节点的长度信息
    // 然后插入 encoding 跟 p 的内容长度
    // 接着根据两种编码的不同来插入最终的值
    //      如果使用的是 STR 类型的编码，则直接复制对应的字符串进去
    //      如果使用的是整数，则根据编码写入对应的字节数
    // 最终更新 ziplist 的长度
    p += zipPrevEncodeLength(p, prevlen);
    p += zipEncodeLength(p, encoding, slen);
    if (ZIP_IS_STR(encoding)) {
        memcpy(p, s, slen);
    }
    else {
        zipSaveInteger(p, value, encoding);
    }
    ZIPLIST_INCR_LENGTH(z, 1);
    return zl;
}

</code></pre>
<p>接下来就只剩下两个调用还未解析了，分别是：扩展后续节点的前置节点长度信息，保存整数内容。
首先看看怎么来保存一个整数到 entry 中。</p>
<pre><code class="language-c">static void zipSaveInteget(unsigned char *p, int64_t value, unsigned char encoding) {
    int16_t i16;
    int32_t i32;
    int64_t i64;
    
    if (encoding == ZIP_INT_8B) {
        // 八位整数，直接赋值
        ((int8_t *)p)[0] = (int8_t)value;
    }
    else if (encoding == ZIP_INT_16B) {
        // 十六位整数，直接赋值，然后转换大小端
        i16 = value;
        memcpy(p, &amp;i16, sizeof(i16));
        memrev16ifbe(p);
    }
    else if (encoding == ZIP_INT_24B) {
        // 24 位整数，移除高八位，因为高八位肯定为 0
        // 然后转换为小端表示法，所以下面可以用 (uint8_t *) + 1 来移动数据
        i32 = value &lt;&lt; 8;
        memrev32ifbe(&amp;i32);
        memcpy(p, ((uint8_t *)&amp;i32) + 1, sizeof(i32) - sizeof(uint8_t));
    }
    else if (encoding = ZIP_INT_32B) {
        // 32 位整数，直接保存
        i32 = value;
        memcpy(p, &amp;i32, sizeof(i32));
        memrev32ifbe(&amp;i32);
    }
    else if (encoding = ZIP_INT_64B) {
        // 32 位整数，直接保存
        i64 = value;
        memcpy(p, &amp;i64, sizeof(i64));
        memrev64ifbe(&amp;i32);
    }
    else if (encoding &gt;= ZIP_INT_IMM_MIN &amp;&amp; encoding &lt;= ZIP_INT_IMM_MAX) {
        // 数字直接存储在 encoding 的剩余空间内
    }
    else {
        assert(NULL);
    }
}
</code></pre>
<p>OK，最后剩下的就是如何扩充插入点之后的那些元素的前置节点的长度信息了。</p>
<pre><code class="language-c">static unsigned char * __ziplistCascadeUpdate(unsigned char *zl, unsigned char *p) {
    size_t curlen = intrev32ifbe(ZIPLIST_BYTES(zl)), rawlen, rawlensize;
    size_t offset, noffset, extra;
    unsigned char *np;
    zentry cur, next;
    
    while (p[0] != ZIP_END) {
        // 获取当前节点
        cur = zipEntry(p);
        // rawlen 表示当前节点的大小
        rawlen = cur.headersize + cur.len;
        // 然后获取当前节点的长度应该用多少个字节来表示
        rawlensize = zipPrevEncodeLength(NULL, rawlen);
        
        // 如果当前节点已经是最后一个节点，则退出调整
        // 因为已经没有后续的节点需要记录他的大小了
        if (p[rawlen] == ZIP_END) break;
        
        // 获取下一个节点
        next = zipEntry(p + rawlen);
        // 如果下一个节点的前置节点所用的大小跟当前节点的一样，说明不需要进行调整
        if (next.prevrawlen == rawlen) break;
        
        // 如果后续节点用来保存前置节点的长度小于当前节点所需的长度，
        // 则需要申请新的空间，然后把整个内存块往后移，然后调整长度信息
        if (next.prevrawlensize &lt; rawlensize) {
            // 保存 p 的偏移量，因为接下来会 ReAlloc
            offset = p - zl;
            // extra 保存还需要多少内存才足够保存长度信息
            extra = rawlensize - next.prerawlensize;
            zl = ziplistResize(zl, curlen + extra);
            p = zl + offset;
            
            // np 指向 next 节点，
            // noffset 指向 next 的偏移量
            np = p + rawlen;
            noffset = np - zl;
            
            // 保证如果 下一个节点不是最后一个节点，则更新 ziplist 的长度为更新后的长度
            // 也就是加上长度的差值
            if ((zl + intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl)) != np) {
                ZIPLIST_TAIL_OFFSET(zl) =
                    intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl)) + extra);
            }
            
            // 往后挪动下一个节点及之后的所有元素
            // np + rawlensize 是目标区域
            // np + next.prevrawlensize, 是 next 节点原本的内存区域（除了 prevlenrawlensize
            // 下面则是需要移动的字节数，通过
            // curlen - noffset - next.prevrawlensize - 1
            // 计算出除了 next 节点及其之后所有元素的大小，也就是所需移动的元素，-1 表示不包括 ZIP_END        
            memmove(np + rawlensize, 
                np + next.prevrawlensize,
                curlen - noffset - next.prevrawlensize - 1);
                
            // 将 p 指向下一个节点，以便更新后续节点
            p += rawlen;
            // curlen 加上差值，用于更新整体的长度
            curlen += extra;
        }
        // 否则说明后续节点用来保存前置节点的长度大于或等于当前节点所需的长度，
        else {
            // 当大于时，为了避免频繁移动内存带来的开销，直接使用较大的内存来放置
            // 只需要小块内存的长度信息
            if (next.prevrawlensize &gt; rawlensize) {
                zipPrevEncodeLengthForceLarge(p + rawlen, rawlen);
            }        
            else {
                // 进到这里说明使用的长度是相同的，所以直接设置长度信息
                zipPrevEncodeLength(p + rawlen, rawlen);
            }
        }
    }
}

// 然后看看 zipPrevEncodeLengthForceLarge 的实现
// 可以看到，他是直接使用 ZIP_BIGLEN 编码来保存长度的
static void zipPrevEncodeLengthForceLarge(unsigned char *p, unsigned int len) {
    if (p == NULL) return;
    p[0] = ZIP_BIGLEN;
    memcpy(p + 1, &amp;len, sizeof(len));
    memrev32ifbe(p + 1);
}
</code></pre>
<p>插入新数据到这里已经结束了，接着我们看看那个为了方便而使用的 <code>zipEntry</code> 函数, 从这个函数里我们可以验证之前推测出来的节点结构的正确性。</p>
<pre><code class="language-c">static zlentry zipEntry(unsigned char *p) {
    zlentry e;
    
    ZIP_DECODE_PREVLEN(p, e.prevrawlensize, e.prevrawlen);
    ZIP_DECODE_LENGTH(p + e.prevrawlensize, e.encoding, e.lensize, e.len);
    e.headersize = e.prevrawlensize + e.lensize;
    e.p = p;
    return e;
}
</code></pre>
<p>这里面的几个宏我们都已经分析过了，大致就是：</p>
<ul>
<li>通过 <code>ZIP_DECODE_PREVLEN</code> 得到
<ul>
<li>前置节点的长度所使用的字节数 <code>e.prevrawlensize</code></li>
<li>前置节点的长度 <code>e.prevrawlen</code></li>
<li>当前节点的编码 <code>e.encoding</code></li>
<li>当前节点的长度所使用的字节数 <code>e.lensize</code></li>
<li>当前节点的长度 <code>e.len</code></li>
</ul>
</li>
<li>然后通过上面的信息，得到 header 的长度 <code>e.headersize</code></li>
</ul>
<p>Ok,到这里我们已经可以确定我们前面所做的所有分析都是正确的了。</p>
<blockquote>
<p>早知道先看这个函数了 <strong>~T.T~</strong></p>
</blockquote>
<a class="header" href="print.html#a查找" id="a查找"><h2>查找</h2></a>
<p><code>ziplist</code> 提供了许多的查找接口，按索引位置查找，按内容查找等，我们下面来分析几个主要的接口。</p>
<a class="header" href="print.html#a查找元素-1" id="a查找元素-1"><h3>查找元素</h3></a>
<p>首先查看函数原型</p>
<pre><code class="language-c">unsigned char *ziplistFind(unsigned char *p, unsigned char *vstr, unsigned int vlen, unsigned int skip);
</code></pre>
<p>从 p 开始查找，vstr 是要查找元素的指针，vlen 是 vstr 的长度，这里的 p 跟 vstr 都是当前 ziplist 的元素。所以所有对节点的的操作都可以直接使用。</p>
<pre><code class="language-c">unsigned char *ziplistFind(unsigned char *p, unsigned char *vstr, unsigned int vlen, unsigned int skip) {
    int skipcnt = 0;
    unsigned char vencoding = 0;
    long long vll = 0;
    
    // 循环直到最后一个节点
    while (p[0] != ZIP_END) {
        unsigned int prevlensize, encoding, lensize, len;
        unsigned char *q;
        
        // 获取当前节点的前一个节点使用了多少个字节来保存长度信息
        ZIP_DECODE_PREVLENSIZE(p, prevlensize);
        // 获取当前节点的编码，长度使用了多少个字节，以及实际内容的长度
        ZIP_DECODE_LENGTH(p + prevlensize, encoding, lensize, len);
        
        // 将 q 指向当前节点的内容区域
        q = p + prevlensize + lensize;
        
        // 检查是否已经跳过了强制跳过的元素数
        if (skipcnt == 0) {
            // 开始分析 p 是否目标节点
            
            // 如果当前节点是字符串，则世界比较内容是否相同，相同的话直接返回当前节点
            if (ZIP_IS_STR(encoding)) {
                if (len == vlen &amp;&amp; memcmp (q, vstr, vlen) == 0) {
                    return p;
                }
            }
            else {
                // 否则就是数字
                if (vencoding == 0) {
                    // 尝试对目标 vstr 进行编码，以得到其编码方式，如果编码失败，则将
                    // vencoding 设为 UCHAR_MAX, 避免下次再次尝试编码
                    if (!zipTryEncoding(vstr, vlen, &amp;vll, &amp;vencoding)) {
                        vencoding = UCHAR_MAX;
                    }
                    
                    assert(vencoding);
                }
                
                // 如果之前的编码成功，则提取对应的数值保存到 ll 中
                if (vencoding != UCHAR_MAX) {
                    long long ll = zipLoadInteger(q, encoding);
                    if (ll == vll) {
                        return p;
                    }
                }
            }
            
            skipcnt = skip;
        
        }
        else {
            skipcnt--;
        }
        
        // 如果比较不成功，则将 p 指向下一个节点
        p = q + len;
    }
}
</code></pre>
<p>整体的查找流程非常简单</p>
<ul>
<li>获取当前节点的编码信息</li>
<li>检查是否已经跳过指定的元素
<ul>
<li>如果不是，直接进入下一节点</li>
</ul>
</li>
<li>判断当前节点是否字符串
<ul>
<li>如果是，则直接比较长度跟内存块是否相同</li>
<li>如果不是，则说明是数字
<ul>
<li>如果还没尝试编码 vstr, 则尝试编码
<ul>
<li>编码成功，得到 vstr 的编码</li>
<li>编码失败，设置 vencoding 为 UCHAR_MAX，下次就无需再尝试编码</li>
</ul>
</li>
<li>如果编码过，且编码成功，则提取数值进行比较傲</li>
</ul>
</li>
</ul>
</li>
<li>如果之前的比较都不成功，则指向下一个节点，重新开始上面的流程</li>
</ul>
<a class="header" href="print.html#a获取指定索引的元素-ziplistindex" id="a获取指定索引的元素-ziplistindex"><h3>获取指定索引的元素 <code>ziplistIndex</code></h3></a>
<p>同样的 <code>ziplist</code> 也支持按索引查找元素，只是这个操作的复杂度是线性的。
因为 <code>ziplist</code> 保存总体的长度，所以可以直接获取最后一个节点，并且每个节点保存了上一个节点的长度，所以支持逆序的遍历，在这两个特性的支持下， <code>ziplist</code> 的索引支持负数，-1 表示最后一个节点，以此类推。</p>
<pre><code class="language-c">unsigned char *ziplistIndex(unsigned char *zl, int index) {
    unsigned char *p;
    unsigned int prevlensize, prevlen = 0;
    
    if (index &lt; 0) {
        index = (-index) - 1;
        // 获取最后一个节点
        p = ZIPLIST_ENTRY_TAIL(zl);
        // 如果列表不为空，则进入遍历流程
        if (p[0] != ZIP_END) {
            // 获取前一个节点的长度，用于指向上一个节点
            ZIP_DECODE_PREVLEN(p, prevlensize, prevlen);
            // 如果还没移动到目标索引并且列表还没到头
            while (prevlen &gt; 0 &amp;&amp; index--) {
                // 指向上一个节点，并获取上一个节点的长度信息
                p -= prevlen;
                ZIP_DECODE_PREVLEN(p, prevlensize, prevlen);
            }
        }
    }
    else {
        // 从头开始遍历 `ziplist`，直到遇到结尾或者到达指定的索引位置
        p = ZIPLIST_ENTRY_HEAD(zl);
        while (p[0] != ZIP_END &amp;&amp; index--) {
            p += zipRawEntryLength(p);
        }
    }
    
    // 返回找到的节点，如果遇到结尾或者 `ziplist` 长度小于索引值，返回 NULL.
    return (p[0] == ZIP_END || index &gt; 0) ? NULL : p;
}
</code></pre>
<a class="header" href="print.html#a提取节点内容-ziplistget" id="a提取节点内容-ziplistget"><h3>提取节点内容 <code>ziplistGet</code></h3></a>
<p>得到一个节点的指针后，可以得到其指向的内容，这个操作由 <code>ziplistGet</code> 来完成</p>
<pre><code class="language-c">unsigned int ziplistGet(unsigned char *p, unsigned char **str, unsigned int *slen, long long *sval) {
    zlentry entry;
    if (p == NULL || p[0] == ZIP_END) return 0;
    if (sstr) *sstr = NULL;
    
    entry = zipEntry(p);
    if (ZIP_IS_STR(entry.encoding)) {
        if (sstr) {
            *slen = entry.len;
            *sstr = p + entry.headersize;
        }
    }
    else {
        if (sval) {
            *sval = zipLoadInteger(p + entry.headersize, entry.encoding);
        }
    }
    
    return 1;
}
</code></pre>
<a class="header" href="print.html#a遍历列表" id="a遍历列表"><h2>遍历列表</h2></a>
<p><code>TODO</code></p>
<a class="header" href="print.html#a级联表单实用类" id="a级联表单实用类"><h1>级联表单实用类</h1></a>
<pre><code class="language-javascript">// A -&gt; B 是一对多的关联

// 在 对象 A 中的方法
var context = getContext(this);
var data = {} // 根据当前模块获取要添加的对象信息


var or = ObjRel(&quot;ab&quot;) // 通过唯一的 key 来获取 or
or.setMaster(context, data);

// 在对象 B 的方法
var context = getContext(this);
var data = {} // 根据当前模块获取要添加的对象信息

var or = ObjRel(&quot;ab&quot;); // key 一样，那对象 A 中获取到的 or 跟当前的 or 就应该是同一个
or.setSubObj(context, data);

var ob2 = ObjRel(&quot;bc&quot;);

// 最终添加个按钮，点击时只要调用
// 就能把之前设好的各个对象进行关联
var or = ObjRel(&quot;ab&quot;);
or.save();
</code></pre>
<a class="header" href="print.html#redis" id="redis"><h1>Redis</h1></a>
<a class="header" href="print.html#redis-源码分析-一-基本定义" id="redis-源码分析-一-基本定义"><h1>Redis 源码分析 (一) 基本定义</h1></a>
<p>[TOC]</p>
<a class="header" href="print.html#redis-object" id="redis-object"><h2>Redis Object</h2></a>
<p>Redis 自身使用了一套对象系统来构建整个架构，比如我们</p>
<pre><code class="language-sh">set &quot;name&quot; &quot;sinsay&quot;
</code></pre>
<p>会构建两个字符串对象，一个保存 key 一个保存 value，然后会再构建一个 HASH 对象，用来保存对应的键值对。
各种对象里面根据当前保存的信息选择对应的编码，例如之前实现过的，当我们要保存一个列表时，会首先创建一个 LIST 对象，这个对象明确了他可以做 LIST 对应的操作，但具体的实现则取决于这个 LIST 对象里到底保存了什么元素，如果里面保存的是少量的数字类型，那 Redis 就会使用 INTSET 编码来作为元素的容器，以达到节省内存的目的，如果保存的是大量的字符串，则会转换为 LINKEDLIST 编码，用链表来保存所有的元素。</p>
<p>具体的编码跟对象类型如下所示：</p>
<a class="header" href="print.html#a对象类型" id="a对象类型"><h4>对象类型</h4></a>
<table><thead><tr><th>对象名      </th><th> 对象类型</th></tr></thead><tbody>
<tr><td>字符串对象  </td><td> REDIS_STRING</td></tr>
<tr><td>列表对象    </td><td> REDIS_LIST</td></tr>
<tr><td>哈希对象    </td><td> REDIS_HASH</td></tr>
<tr><td>集合对象    </td><td> REDIS_SET</td></tr>
<tr><td>有序集合对象 </td><td> REDIS_ZSET</td></tr>
</tbody></table>
<a class="header" href="print.html#a对象编码" id="a对象编码"><h4>对象编码</h4></a>
<table><thead><tr><th>编码名   </th><th> 编码类型</th></tr></thead><tbody>
<tr><td>原始字符串 </td><td> REDIS_ENCODING_RAW</td></tr>
<tr><td>数字      </td><td> REDIS_ENCODING_INT</td></tr>
<tr><td>哈希表    </td><td> REDIS_ENCODING_HT</td></tr>
<tr><td>压缩表  </td><td> REDIS_ENCODING_ZIPMAP</td></tr>
<tr><td>链表      </td><td> REDIS_ENCODING_LINKEDLIST</td></tr>
<tr><td>压缩列表  </td><td> REDIS_ENCODING_ZIPLIST</td></tr>
<tr><td>整数集合   </td><td> REDIS_ENCODING_INTSET</td></tr>
<tr><td>跳跃表    </td><td> REDIS_ENCODING_SKIPLIST</td></tr>
<tr><td>压缩字符串 </td><td> REDIS_ENCODING_EMBSTR</td></tr>
</tbody></table>
<a class="header" href="print.html#a具体实现" id="a具体实现"><h2>具体实现</h2></a>
<a class="header" href="print.html#a对象跟编码的定义" id="a对象跟编码的定义"><h3>对象跟编码的定义</h3></a>
<pre><code class="language-c">#define REDIS_STRING 0
#define REDIS_LIST   1
#define REDIS_SET    2
#define REDIS_ZSET   3
#define REDIS_HASH   4

#define REDISENCODING_RAW        0
#define REDISENCODING_INT        1
#define REDISENCODING_HT         2
#define REDISENCODING_ZIPMAP     3
#define REDISENCODING_LINKEDLIST 4
#define REDISENCODING_ZIPLIST    5
#define REDISENCODING_INTSET     6
#define REDISENCODING_SKIPLIST   7
#define REDISENCODING_EMBSTR     8

typedef struct redisObject {
    unsigned type:4;
    unsigned encoding:4;
    unsigned lru:REDIS_LRU_BITS;
    int refcount;
    void *ptr;
} robj;
</code></pre>
<a class="header" href="print.html#a服务器信息" id="a服务器信息"><h2>服务器信息</h2></a>
<p>Redis 在服务器上体现为一个 <code>redisServer</code> 结构，其中保存了 <code>redisDb</code> 的指针用于实现同时多个 <code>redis</code> 的数据库并存。
我们先看看具体定义</p>
<pre><code class="language-c">struct redisServer {
    // 这里分成了很多部分，我们先简单的看看通用部分
    pid_t pid;              // 主进程 id
    char *configfile;       // 配置文件
    int hz;                 // 后面解释，服务器后台检查的频率
    redisDb *db;            // 最重要的存储 DB
    dict *commands;         // 命令表
    dict *orig_commands;    // 原始命令表
    aeEventLoop *el;        // 事件循环句柄
    unsigned lruclock:REDIS_LRU_BITS; // lru 
    int shutdown_asap;      // ？？？
    int activerehashing;    // rehash 标示符
    char *requirepass;      // 传递给 AUTH 命令的信息
    char *pidfile;          // PID 文件
    int arch_bits;          // 架构标示符（32位 OR 64位)
    int cronloops;          // 后台 Cron 运行次数
    char runid[REDIS_RUN_ID_SIZE+1]; // 每次执行 Redis 都会产生不同的 id
    int sentinel_mode;      // 是否启动哨兵模式
    
    // 配置信息
    dbnum;                  // 启用的 Db 个数
};
</code></pre>
<p>Ok, 以上即是通用部分，是我们当前最关心的部分，主要包括了一些主要的执行信息，比如进程信息，后台逻辑检查频率，以及最重要的 Db 信息。</p>
<p>接下来我们看看服务器的启动步骤。</p>
<pre><code class="language-c">int main (int argc, char **argv) {
    struct timeval tv;

#ifdef INIT_SETPROCTITLE_REPLACEMENT
    spt_init(argc, argv);
#endif
    // 设置区域信息
    setlocale(LC_COLLATE, &quot;&quot;);
    
    // 设置内存分配信息
    zmalloc_enable_thread_safeness();
    zmalloc_set_oom_handler(redisOutOfMemoryHandler);
    
    // 初始化随机数种子
    srand(time(NULL)^getpid());
    gettimeofday(&amp;tv, NULL);
    
    // 初始化字典种子
    dictSetHashFunctionSeed(tv.tv_sec ^ tv.tv_usec ^ getpid());
    
    // 初始化服务器配置, 包括监听端口，DB 数等
    server.sentinel = checkForSentinelMode(argc, argv);
    initServerConfig();
    
    // 如果设置了 sentinel 模式，则设置配置信息
    if (server.sentinel_mode) {
        initSentinelConfig();
        initSentinel();
    }
    
    // 处理命令行参数信息，包括读取设置配置文件信息，获取版本信息等
    if (argc &gt;= 2) {
        // ...
    }
    
    // 是否以后台进程运行
    if (server.daemonize) daemonize();
    
    // 初始化服务
    initServer();
    
    // 如果是以后台进程运行，则创建 Pid File
    if (server.daemonize) createPidFile();
    
    // 设置一些启动信息
    redisSetProcTitle(argv[0]);
    redisAsciiArc();
    checkTcpBacklogSettings();
    
    // 正常启动
    if (!server.sentinel_mode) {
        redislLog(REDIS_WARNING, &quot;...&quot;);
        
        // 读取静态化的数据
        loadDataFromDisk();
        
        // 如果开启了分布式模式，这里我们先只考虑单机部分
        if (server.cluster_enabled) {
            if(verifyClusterConfigWithData() == REDIS_ERR) {
                redisLog(&quot;...&quot;);
                exit(1);
            }
        }
        // 如果绑定的 ip 数已大于零，说明已经可以监听客户端的连接了
        if (server.ipfd_count &gt; 0) {
            redisLog(&quot;...&quot;);
        }
        // unixsocket 模式
        if (server.sofd &gt; 0) {
        }
    }
    else {
        // 哨兵模式启动
        sentinelIsRunning()
    }
    
    // 到这里已经初始化完成，开始进入监听模式，等候客户端发过来的命令
    aeSetBeforeSleepProc(server.el, beforeSleep);
    aeMain(server.el);
    aeDeleteEventLoop(server.el);
    return 0;
}
</code></pre>
<p>启动过程完毕，从 aeMain 开始，服务器已经初始化完毕，并且开始监听对应的端口，ae 的具体细节我们就不分析了，这是网络库的一部分，具体的实现就是：
在客户端连上来时，创建一个 <code>redisClient</code>，并且绑定了一个处理事件：当这个 <code>redisClient</code> 可读时调用，这个事件是 <code>readQueryFromClient</code>。
大致的代码为：</p>
<pre><code class="language-c">static void acceptCommonHandler(int fd, int flags) {
    redisClient *c;
    // 当连接进来时，调用 acceptCommonHandler，然后创建一个
    // redisClient 实例，并在 createClient 中绑定了读取事件
    if ((c = createClient(fd)) == NULL) {
        // process err
    }
    // ...
}

redisClient *createClient(int fd) {
    redisClient *c = zmalloc(sizeof(redisClient));
    if (fd != -1) {
        // ...
        // 将 fd 绑定到当前 client, 然后绑定 readQueryFromCleint,
        // 当 fd 可读时触发
        if (aeCreateFileEvent(server.el, fd, AE_READABLE,
            readQueryFromClient, c) == AE_ERR) {
            // process error   
        }
    }
}

void readQueryFromClient(aeEventLoop *el, int fd, void *privdata, int mask) {
    redisClient *c = (redisClient *) privdata;
    
    // process multi bulk request
    if (c-&gt;reqtype == REDIS_REQ_MULTIBULK &amp;&amp; c-&gt;multibulklen &amp;&amp; c-&gt;bulklen != -1 &amp;&amp; c-&gt;bulklen &gt;= REDIS_MBULK_BIG_ARG) {
        int remaining = (unsigned)(c-&gt;bulklen + 2) - (sdslen(c-&gt;querybuf);
        if (remaining &lt; readlen) readlen = remaining;
    }
    
    // 从客户端读取内容，并保存到 redisClient.querybuf 中
    qblen = sdslen(c-&gt;querybuf);
    if (c-&gt;querybuf_peak &lt; qblen) c-&gt;querybuf_peak = qblen;
    c-&gt;querybuf = sdsMakeRoomFor(c-&gt;querybuf, readlen);
    nread = read(fd, c-&gt;querybuf + qblen, readlen);
    
    // 然后是一系列的错误处理
    // ...
    
    // 读取完毕后，处理 buff
    processInputBuffer(c);
    server.current_client = NULL;
}

</code></pre>
<p>在 <code>ProcessInputBuffer</code> 中，将 buffer 中的命令解析出来，创建为 <code>redisObject</code>，并放到 <code>redisClient</code> 的 argc 跟 argv 中</p>
<a class="header" href="print.html#redis-内容篇" id="redis-内容篇"><h1>Redis 内容篇</h1></a>
<p>[TOC]</p>
<p>在经过了之前对 Redis 的几个基础类型的分析后，我们开始查看其具体实现，首先嘛，任何程序跟应用总是需要启动的，所以我们从其 <code>redis.h/redis.c</code> 开始。</p>
<pre><code class="language-c">#include &quot;ae.h&quot;        /* Event driven programming library */
#include &quot;sds.h&quot;       /* Dynamic safe strings */
#include &quot;dict.h&quot;      /* Hash tables */
#include &quot;adlist.h&quot;    /* Linked lists*/
#include &quot;zmalloc.h&quot;   /* total memory usage aware version of malloc/free */
#include &quot;anet.h&quot;      /* Networkiing the easy way */
#include &quot;ziplist.h&quot;   /* Compact list date structure */
#include &quot;intset.h&quot;    /* Compact integer set structure */
#include &quot;version.h&quot;   /* Version macro */
#include &quot;util.h&quot;      /* Misc function useful in many place*/
#include &quot;latency.h&quot;   /* Latency monitor API */
#include &quot;sparkline.h&quot; /* ASII graphs API */
</code></pre>
<p>除去我们已经分析过的基础类型，其他的都是 Redis 相关逻辑实现，而注释也写的非常清楚</p>
<ul>
<li><code>ae.h</code> 是事件驱动 lib, 所有的 redis 操作应当都由此 lib 进行管理、调用</li>
<li><code>anet.h</code> 网络库的封装</li>
<li><code>zmalloc.h</code> 内存管理，方便 redis 对内存的使用量进行监控</li>
<li><code>version.h</code> 版本信息</li>
<li><code>latency.h</code>    延时管理器</li>
<li>其他
<ul>
<li><code>util.h</code>       使用函数</li>
<li><code>sparkline.h</code>  程序启动的图形输出</li>
</ul>
</li>
</ul>
<p>由上面的基本信息可知，核心流程基本是由 anet 来管理网络连接，在接收到客户端的请求后，将请求内容转发给 ae，ae 则根据请求的类型，调用相关的处理函数。</p>
<a class="header" href="print.html#a启动服务器" id="a启动服务器"><h2>启动服务器</h2></a>
<p>接下来看看 Redis 是如何启动的, 以下是经过简单整理的启动函数</p>
<pre><code class="language-c">int main (int argc, char **argv) {
    struct timeval tv;
    
    setlocale(LC_COLLATE, &quot;&quot;);
    zmalloc_enable_thread_safeness();
    zmalloc_set_oom_handler(redisOutOfMemoryHandler);
    srand(time(NULL)^getpid());
    gettimeofday(&amp;tv, NULL);
    dictSetHashFunctionSeed(tv.tv_sec ^ tv.tv_usec ^ getpid());
    server.sentinel_mode = checkForSentinelMode(argc, argv);
    initSErverConfig();
    
    if (server.sentinel_mode) {
        initSentinelConfig();
        initSentinel();
    }
    
    // 命令行处理...
    // 略过
    if (server.daemonize) daemonize();
    // 初始化服务
    initServer();
    if (server.daemonize) createPidFile();    
}
</code></pre>
<p>首先看看服务是怎么初始化的</p>
<pre><code class="language-c">void initServer(void) {
    int j;
    
    gignal(SIGHUP, SIG_IGN);
    gignal(SIGPIPE, SIG_IGN);
    setupSingalHandlers();
    
    if (server.syslog_enabled) {
        openlog(server.syslog_ident, LOG_PID | LOG_NDELAY | LOG_NOWAIT, server.syslog_facility);
    }
    
    // 当前进程 ID
    server.pid = getpid();
    // 当前处理的客户端
    server.current_client = NULL;
    // 客户端列表
    server.clients = listCreate();
    // 待关闭的客户端
    server.clients_to_close = listCreate();
    // 从服务器
    server.slaves = listCreate();
    // 监控器
    server.monitors = listCreate();
    server.slaveseldb = -1;
    // 非堵塞客户端
    server.unblocked_clients = listCreate();
    server.ready_keys = listCreate();
    server.clients_waiting_acks = listCreate();
    server.get_ack_from_slaves = 0;
    server.clients_paused = 0;
    
    
    // 创建共享变量，如命令的表示，静态字符串等
    createShareObjects();
    // 调整文件描述符限制, 实现是如果支持设置最大文件描述符，则从最大的值开始尝试，直到设置成功
    adjustOpenFilesLimit();
    
    // redis 的核心：事件驱动
    server.el = aeCreateEventLoop(server.maxclients + REDIS_EVENTLOOP_FDSET_INCR);
    
    // 保存具体数据的数据库
    server.db = zmalloc(sizeof(redisDb) * server.dbnum);
    
    // 开始监听指定的端口
    if (server.port != 0 &amp;&amp; 
        listenToPort(server.port, server.ipfd, &amp;server.ipfd_count) == REDIS_ERR)
        exit(1);
        
    if (server.unixsocket != NULL) {
        // ... unix 域的 socket
    }
    
    // 如果没监听任何端口，则直接退出
    if (server.ipfd_count == 0 &amp;&amp; server.sofd &lt; 0) {
        redisLog(REDIS_WARNING,  &quot;Configured to not listen anywhere, exiting.&quot;);
        exit(1);
    }
    
    // 初始化数据库
    // 这里初始化各个 dict 的关键就是定义的各种 dictType, 里面描述了对应的 key value 的复制、删除等实现。说完这部分再看看大致的实现
    for (j = 0; j &lt; server.dbnum; j++) {
        server.db[j].dict = dictCreate(&amp;dbDictType, NULL);
        server.db[j].expires = dictCreate(&amp;keyptrDictType, NULL);
        server.db[j].blocking_keys = dictCreate(&amp;keyListDictType, NULL);
        server.db[j].ready_keys = dictCreate(&amp;setDictType, NULL);
        server.db[j].watched_keys = dictCreate(&amp;keylistDictType, NULL);
        server.db[j].eviction_pool = evictionPoolAlloc();
        server.db[j].id = j;
        server.db[j].avg_ttl = 0;
    }
    
    // 用于 pubsub 的 channels,
    // 然后设置了对应 list 的处理函数
    server.pubsub_channels = dictCreate(&amp;keylistDictType, NULL);
    server.pubsub_patterns = listCreate();
    listSetFreeMethod(server.pubsub_patterns, freePubsubPattern);
    listSetMatchMethod(server.pubsub_patterns, listMatchPubsubPattern);
    
    // 然后是其他的配置信息
    // 已经处理过 Cron 的次数
    server.cronloops = 0;
    // rdb 跟 aof 是 Redis 的两种静态化机制
    server.rdb_child_pid = -1;
    server.aof_child_pid = -1;
    server.rdb_child_type = REDIS_RDB_CHILD_TYPE_NONE;
    aofRewriteBufferReset();
    server.aof_buf = sdsempty();
    server.lastsave = time(NULL);
    server.lastbgsave_try = 0;
    server.rdb_save_time_last = -1;
    server.rdb_save_time_start = -1;
    server.dirty = 0;
    resetServerStata();
    
    // redis 的状态信息，包括启动时间等
    server.stat_starttime = time(NULL);
    server.stat_peak_memory = 0;
    server.resident_set_size = 0;
    server.lastbgsave_status = REDIS_OK;
    server.aof_last_write_status = REDIS_OK;
    server.aof_last_write_errno = 0;
    server.repl_good_slaves_count = 0;
    updateCachedTime();
    
    // 接下来启动定时器，定时触发 serverCron 操作
    if (aeCreateTimeEvent(server.el, 1 server.cron, NULL, NLL) == AE_ERR) {
        redisPanic(&quot;Can't craete the serverCron time event.&quot;);
        exit(0);
    }
    
    // 然后绑定对应 fd 的网络事件，在 fd 可读时触发 acceptTcpHandler 函数
    for (j = 0; server.ipfd_count; j++) {
        if (aeCreateFileEvent(server.el, server.ipif[j], AE_READABLE, acceptTcpHandler, NULL) == AE_ERR) {
            redisPainc(&quot;Unrecoverable error create server.ipfd file event&quot;);
        }
    }
    
    // 这里绑定 unix 域的 fd 可读时触发 acceptUnixHandler 函数
    if (server.sofd &gt; 0 &amp;&amp; aeCreateFileEvent(server.el, 
        server.sofd, AE_READABLE, acceptUnixHandler, NULL) == AE_ERR)
        redisPainc(&quot;Unrecoverable error create server.sofd file event.&quot;);
     
    // 如果启动了 aof 静态化机制，则打开 aof 对应的文件
    if (server.aof_sate == REDIS_AOF_ON) {
        server.aof_fd = open(server.aof_filename,
            O_WRONLY | O_APPEND | O_CREAT, 0644);
        if (server.aof_fd == -1) {
            redisLog(REDIS_WARNING, &quot;Can't open the append-only file: %s&quot;, strerrno(errno));
            exit(1);
        }
    }
    
    // 在 32 位的机器上，限制最多使用的内存量为 3GB
    if (server.arch_bits == 32 &amp;&amp; server.maxmemory == 0) {
        redisLog(REDIS_WARNING, &quot;Warning: 32bit instance detected but no memory limit set, Stting 3 GB maxmemory limit with 'noeviction' policy now.&quot;);
        server.maxmemory = 3072LL * (1024 * 1024);
        server.maxmemory_policy = REDIS_MAXMEMORY_NO_EVICTION;
    }
    
    // 如果启动了主从机制，则初始化主从信息
    if (server.cluster_enabled) clusterInit();
    // 初始化复制机制
    replicationScriptCacheInit();
    // 初始化 Lua 脚本环境
    scriptingInit();
    // 初始化延迟日志
    slowLogInit();
    // 初始化 latency 列表 
    latencyMonitorInit();
    // 初始化 Backgrond IO，主要用于落地数据
    bioInit();
}
</code></pre>
<p>在上面的初始化过程中，我们把当前应该关注的跟可以忽略的都写出来了，其实是为了留个印象，以后遇到的时候可以知道是用来干啥的。
除去 <code>Cluster</code>、<code>AOF</code>、<code>RDB</code> 等，剩余的基本都是 Redis 基础服务的核心，如初始化时通过 <code>aeCreateTimeEvent</code> 注册的的 <code>serverCron</code> 事件，以及注册到监听端口的处理函数 <code>acceptTcpHandler</code>, 暂时可以把它理解为一切的开始：因为 Redis 的工作方式就是等待一条命令，处理命令然后返回结果。</p>
<a class="header" href="print.html#a建立连接" id="a建立连接"><h2>建立连接</h2></a>
<p>在这里我们先不管 ae 的实现，直接查看 <code>acceptTcpHandler</code>，这里是作为 Redis 接收客户端链接的入口：</p>
<pre><code class="language-c">void acceptTcpHandler(aeEventLoop *el, int fd, void *privdata, int mask) {
    int cport, cfd, max = MAX_ACCEPTS_PER_CALL;
    char cip[REDIS_IP_STR_LEN];
    REDIS_NOTUSED(el);
    REDIS_NOTUSED(mask);
    REDIS_NOTUSED(privdata);
    
    while (max--) {
        // 以非堵塞的方式接受 tcp 连接，fd 是监听的端口 fd, cip 跟 cport 用来保存客户端对应的 fd 跟端口
        cfd = anetTcpAccept(server.neterr, fd, cip, sizeof(cip), &amp;cport);
        if (cfd == ANET_ERR) {
            if (errno != EWOULDBLOCK)
                redisLog(REDIS_WARNING,
                    &quot;Accepting client connection: %s&quot;, server.neterr);
            return;
        }
        
        redisLog(REDIS_VERBOSE, &quot;Accepted %s:%d, cip, cport);
        // 接收连接成功，将其加入 server 的客户端列表
        acceptCommonHandler(cfd, 0);
    }
}
</code></pre>
<p>上面的函数会在有新的客户端连接上来是触发，而具体的逻辑则负责把新的连接通过 <code>acceptCommonHandler</code> 创建一个 <code>redisClient</code> 实例加入 server 进行管理，接下来看看其实现及 <code>redisClient</code> 的定义</p>
<pre><code class="language-c">
typedef struct redisClient {
    uint64_t id;  // 客户端的唯一标示符
    int fd;
    redisDb *db;
    int dictid;
    robj *name;   // 客户端的名称
    sds querybuf;
    size_t querybuf_peak;
    int argc;
    robj **argv;
    struct redisCommand *cmd, *lastcmd;
    int reqtype;
    int multibulklen;
    long bulklen;
    list *reply;
    unsigned long reply_bytes;
    int sentlen;
    
    time_t ctime;
    time_t lastinteraction;
    time_t obuf_soft_limit_reached_time;
    int flags;    // REDIS_SLAVE | REDIS_MONITOR | REDIS_MULTI
    int authenticated;
    
    int replstate;  // repl 的状态，只有在当前进程是 slave 时才有用
    int repl_put_online_on_ack;
    int repldbfd;
    off_t repldboff;
    off_t repldbsize;
    sds replpreamble;
    long long reploff;
    long long repl_ack_off;
    long long repl_ack_time;
    long long psync_initial_offset;
    
    char replrunid[REDIS_RUN_ID_SIZE + 1];
    int slave_listening_port;
    int slave_capa;
    multiState mstate;
    int btype;
    blockingState bpop;
    long long woff;
    list *watched_keys;
    dict *pubsub_channels;
    list *pubsub_patterns;
    sds peerid;
    
    int bufpos;
    char buf[REDIS_REPLY_CHUNK_BYTES];
} redisClient;

static void acceptCommonHandler(int fd, int flags) {
    redisClient *c;
    // 创建 client 示例
    if ((c = createClient(fd)) == NULL) {
        redisLog(REDIS_WARNING,
            &quot;Error registering fd event for the new client : %s (fd=%d)&quot;,
            strerror(errno), fd);
        close(fd);
        return;
    }

    // 限制最大链接数    
    if (listLength(server.clients) &gt; server.maxclients) {
        char *err = &quot;-ERR max number of clients reached\r\n&quot;;
        
        if (write(c-&gt;fd, err, strlen(err)) == -1) {
        }
        server.stat_rejected_conn++;
        freeClient(c);
        return;
    }
    server.stat_numconnections++;
    c-&gt;flags |= flags;
}

redisClient *createClient(int fd) {
    redisClient *c = zmalloc(sizeof(redisClient));
    
    if (fd != -1) {
        // 将客户端设置为非堵塞的
        anetNonBlock(NULL, fd);
        // 不延迟发送客户端的回应
        anetEnableTcpNoDelay(NULL, fd);
        // 设置 keepalive 以检查客户端断开的状况
        if (server.tcpkeepalive)
            anetKeepAlive(NULL, fd, server.tcpkeepalive);
            
        // 注册事件，当客户端有请求过来时，调用 readQueryFromClient
        if (aeCreateFileEvent(server.el, fd, AE_READABLE,
                readQueryFromClient, c) == AE_ERR) {
            close(fd);
            zfree(c);
            return NULL;       
        }
        
        // 接下来是对 client 的一些常规设置, 以下会忽略一些跟现在无关的选项
        selectDb(c, 0); // 设置使用第一个 db
        c-&gt;id = server.next_client_id++;
        c-&gt;fd = fd;
        c-&gt;name = NULL;
        // ...
        c-&gt;querybuf = sdsempty(); // 请求的缓冲
        c-&gt;argc = 0;
        c-&gt;argv = NULL; // 上面两个是保存具体操作的参数
        c-&gt;cmd = c-&gt;lastcmd = NULL;
        c-&gt;reply = listCreate(); // 给客户端的 response
        c-&gt;reply_bytes = 0;
        listSetFreeMethod(c-&gt;reply, decrRefCountVoid);
        listSetDupMethod(c-&gt;reply, dupClientReplyValue);
        // ...
        // 最后添加 client 到 server 的 clients 进行管理
        if (fd != -1) listAddNodeTail(server.clients, c);
        initClientMultiState(c);
        
    }
}
</code></pre>
<p>创建客户端的操作基本完成，在这个时候客户端已经与服务端建立连接，随时准备客户端发送请求。</p>
<a class="header" href="print.html#redis-的类型定义" id="redis-的类型定义"><h2>Redis 的类型定义</h2></a>
<p>如何处理请求留到下一篇章进行分析，接下来看一下上面遇到的各种 redis 的内置类型定义。</p>
<pre><code class="language-c">#define REDIS_LRU_BITS 24

typedef struct redisObject {
    unsigned type:4;
    unsigned encoding:4;
    unsigned lru:REDIS_LRU_BITS;
    int refcount;
    void *ptr;
} robj;

typedef struct redisDb {
    dict *dict;         /* The keyspace for thsi DB */
    dict *expires;      /* Timeout of keys with a timeout set */
    dict *blocking_keys;
    dict *ready_keys;
    dict *watched_keys;
    struct evictionPoolEntry *eviction_pool;
    int id;
    long long avg_ttl;
} redisDb;

typedef struct redisServer {
    // 这里的东西太多，就不一一列出了，等要用到的时候再进行分析吧
} redisServer;
</code></pre>
<a class="header" href="print.html#redis-内容篇-1" id="redis-内容篇-1"><h1>Redis 内容篇</h1></a>
<p>[TOC]</p>
<a class="header" href="print.html#a解析请求" id="a解析请求"><h2>解析请求</h2></a>
<p>在前文我们看到了当客户端发送命令过来时，是由 <code>ae</code> 调用 <code>readQueryFromClient</code> 进行处理，接着我们来看看这个函数是如何处理客户端发送过来的数据的。</p>
<pre><code class="language-c">void readQueryFromClient(aeEventLoop *el, int fd, void *privdata, int mask) {
    int nread, readlen;
    size_t qblen;
    REDIS_NOTUSED(el);
    REDIS_NOTUSED(mask);

    server.current_client = c;
    readlen = REDIS_IOBUF_LEN;

    // 处理大数据包
    if (c-&gt;reqtype == REDIS_REQ_MULTIBULD &amp;&amp; c-&gt;multibulklen &amp;&amp;
         c-&gt;bulklen != -1 &amp;&amp; c-&gt;bulklen &gt;= REDIS_MBULK_BIG_ARG) {
        int remaining = (unsigned)(c-&gt;bulklen + 2) - sdslen(c-&gt;querybuf);
        // 如果剩余的数据量少于默认的缓存数，则使用较小的缓存大小
        if (remaining &lt; readlen) readlen = remaining;
    }

    qblen = sdslen(c-&gt;querybuf);
    if (c-&gt;querybuf_peak &lt; qblen) c-&gt;querybuf_peek = qblen;
    // 扩展缓存空间，以存放即将读取的数据
    c-&gt;querybuf = sdsMakeRoomFor(c-&gt;querybuf, readlen);
    // c-&gt;querybuf + qblen 是跳过已经使用的缓存内容
    nread = read(fd, c-&gt;querybuf + qblen, readlen);

    // 错误处理，如果因为信号中断，则设置 nread 为 0，并等待下次重读
    // 如果远程客户端断开，则服务端也断开连接
    if (nread == -1) {
        if (errno == EAGAIN) {
            nread = 0;
        }
        else {
            readLog(REDIS_VERBOSE, &quot;Reading from client: %s&quot;, strerrno(errno));
            freeClient(c);
            return;
        }
    }
    else if(nread == 0) {
        readisLog(REDIS_VERBOSE, &quot;Client closed connection&quot;);
        freeClient(c);
        return;
    }

    if (nread) {
        // 调整 querybuf 已使用的数据量
        sdsIncrLen(c-&gt;querybuf, nread);
        // 更新状态信息
        c-&gt;lastinteraction = server.unixtime;
        if (c-&gt;flags &amp; REDIS_MASTER) c-&gt;reploff += nread;
        server.stat_net_input_bytes += nread;
    }
    else {
        server.current_client = NULL;
        return;
    }

    // 如果当前客户端请求的缓存大于系统设置的上限，则中断此连接, 并输出当前连接信息
    // 比如当前客户端是攻击者？
    if (sdslen(c-&gt;querybuf) &gt; server.client_max_querybuf_len) {
        sds ci = catClientInfoString(sdsempty(), c), bytes = sdsempty();
        bytes = sdscatrepr(bytes, c-&gt;querybuf, 64);
        redisLog(REDIS_WARNING, &quot;Closing client that reached max query buffer length&quot;);
        sdsfree(ci);
        sdsfree(bytes);
        freeClient(c);
        return;
    }

    // 开始处理请求
    processInputBuffer(c);
    server.current_client = NULL;
}
</code></pre>
<p>上面使用了最正统的 socket 处理方式，读取数据，做出错误处理，验证客户端的安全性（缓存长度），然后把接收到的数据丢给 <code>processInputBuffer</code> 进行处理。
接下来我们看看它是怎么做的。</p>
<pre><code class="language-c">void processInputBuffer(redisClient *c) {
    // 处理直到 buf 中没有数据
    while (sdslen(c-&gt;querybuf)) {

        // 如果是从服务器，并且由于某些原因处于暂停状态，则不处理客户端的命令
        if (!(c-&gt;flags &amp; REDIS_SLAVE) &amp;&amp; clientsArePaused()) return;

        // 如果当前客户端正在某个处理步骤的中间，则不处理命令
        if (c-&gt;flags &amp; REDIS_BLOCKED) return;

        // 如果当前客户端在回复后就要关闭，则不处理命令
        if (c-&gt;flags &amp; REDIS_CLOSE_AFTER_REPLY) return;

        // 确认当前客户端的请求模式, 单行或多行模式
        if (!c-&gt;reqtype) {
            if (c-&gt;querybuf[0] == '*') {
                c-&gt;reqtype = REDIS_REQ_MULTIBULK;
            }
            else {
                c-&gt;reqtype = REDIS_REQ_INLINE;
            }
        }

        // 然后再根据模式处理接收到的数据
        if (c-&gt;reqtype == REDIS_REQ_INLINE) {
            if (processInlineBuffer(c) != REDIS_OK) break;
        }
        else if(c-&gt;reqtype == REDIS_REQ_MULTIBULK) {
            if (processMultibulkBuffer(c) != REDIS_OK) break;
        }
        else {
            redisPanic(&quot;Unknown request type&quot;);
        }

        // 如果是 Multibulk 模式，则可能会出现 argc &lt;= 0 的情况
        if (c-&gt;argc == 0) {
            resetClient(c);
        }
        else {
            // 开始处理命令
            if (processCommand(c) == REDIS_OK)
                resetClient(c);
        }
    }
}
</code></pre>
<p>在进入处理具体命令的逻辑之前，我们先看看，是如何组织命令信息的，Inline 跟 MultiBulk 两种方式分别实现在 <code>processInlineBuffer</code> 跟 <code>processMultibulkBuffer</code> 中，我们分别来看看。</p>
<a class="header" href="print.html#a解析单个命令请求" id="a解析单个命令请求"><h3>解析单个命令请求</h3></a>
<pre><code class="language-c">int processInlineBuffer(redisClient *c) {
    char *newline;
    int argc, j;
    sds *argv, aux;
    size_t querylen;

    // 获取当前命令行
    newline = strchr(c-&gt;querybuf, '\n');

    // 错误处理
    if (newline == NULL) {
        if (sdslen(c-&gt;querybuf) &gt; REDIS_INLINE_MAX_SIZE) {
            addReplyError(c, &quot;Protocol error: too big inline request&quot;);
            setProtocoError(c, 0);
        }
    }

    // 处理 \r\n 的情况，如果 \n 不是在 querybuf 的第一个字节且 \n 的前一个字节为 \r, 则 newline 回退一个字节
    if (newline &amp;&amp; newline != c-&gt;querybuf &amp;&amp; *(newline - 1) == '\r')
        newline--;

        // 得到命令的长度
        querylen = newline - (c-&gt;querybuf);
        // 得到确切的命令, 按空格或 &quot;&quot; 进行分割，比如
        // cmd &quot;hello world&quot; im sinsay
        // 可得到 [ cmd, &quot;hello world&quot;, im, sinsay ]
        aux = sdsnewlen(c-&gt;querybuf, querylen);
        argv = sdssplitargs(aux, &amp;argc);
        sdsfree(aux);
        if (argv == NULL) {
            addReplyError(c, &quot;Protocol error: unblanced quotes in request&quot;);
            setProtocolError(c, 0);
            return REDIS_ERR;
        }

        // 从 slave 发过来的空的 newline 表示要刷新 ack 信息
        if (querylen == 0 &amp;&amp; c-&gt;flags &amp; REDIS_SLAVE)
            c-&gt;repl_ack_time = server.unixtime;

        // 调整 querybuf, 使其跳过已获取到的 newline, 类似 substr 的操作
        sdsrange(c-&gt;querybuf, querylen + 2, -1);

        // 分配存放参数的内存块
        if (argc) {
            if (c-&gt;argc) zfree(c-&gt;argv);
            c-&gt;argv = zmalloc(sizeof(robj*) * argc);
        }

        // 这里创建的 argv 每个都是属于 robj 类型
        for (c-&gt;argc = 0, j = 0; j &lt; argc; j++) {
            if (sdslen(argv[j])) {
                c-&gt;argv[c-&gt;argc] = createObject(REDIS_STRING, argv[j]);
                c-&gt;argc++;
            }
            else {
                sdsfree(argv[j]);
            }
        }

        zfree(argv);
        return REDIS_OK;
}
</code></pre>
<p>上面唯一要重视的就是，<code>argv</code> 数组中的参数都是通过 <code>createObject</code> 创建的 <code>robj</code> 类型。<code>robj</code> 的结构在上篇的末尾已经介绍过，但我们还不了解各个字段的含义，现在我们看看到底是如何创建这个结构的。</p>
<pre><code class="language-c">robj *createObject(int type, void *ptr) {
    robj *o = zmalloc(sizeof(*o));
    o-&gt;type = type;
    o-&gt;encoding = REDIS_ENCODING_RAW;
    o-&gt;ptr = ptr;
    o-&gt;refcount = 1;
    o-&gt;lru = LRU_CLOCK();
    return o;
}
</code></pre>
<p>从上面的调用方式基本可以得到:</p>
<p>type 表示 obj 的类型，从定义我们可得到大致上有以下几种类型</p>
<pre><code class="language-c">#define REDIS_STRING 0
#define REDIS_LIST   1
#define REDIS_SET    2
#define REDIST_ZSET  3
#define REDIS_HASH   4
</code></pre>
<p>这不就是 redis 支持的数据结构么，OK。</p>
<p>encoding 这里看到的是 REDIS_ENCODING_RAW, 确定不了什么，再看看相关定义：</p>
<pre><code class="language-c">/* Objects encoding. Some kind of objects like String and Hashes can be
 * internally represented in multiple ways. The `encoding` field of the object
 * is set to one of this fields for this object */
#define REDIS_ENCODING_RAW        0   // Raw representation
#define REDIS_ENCODING_INT        1   // Encoded as integer
#define REDIS_ENCODING_HT         2   // Encoded as hash table
#define REDIS_ENCODING_ZIPMAP     3   // Encoded as zipmap
#define REDIS_ENCODING_LINKEDLIST 4   // Encoded as regular linked list
#define REDIS_ENCODING_ZIPLIST    5   // Encoded as ziplist
#define REDIS_ENCODING_INTSET     6   // Encoded as intset
#define REDIS_ENCODING_SKIPLIST   7   // Encoded as skiplist
#define REDIS_ENCODING_EMBSTR     8   // Embedded sds string encoding
</code></pre>
<p>再次可以确定，encoding 表示的是当前 obj 的具体实现，也就是这个 obj 可以是 REDIS_LIST 的类型，但他的具体实现可以使用 ZIPLIST 或 LINKEDLIST 之类的（~~我猜的~~）</p>
<p>而 ptr 就是具体实现的指针，具体能怎么使用这个 ptr, 取决于 type 跟 encoding.</p>
<p><code>refcount</code> 跟 <code>lru</code> 就基本可以从字面意思看出，一个是用来共享变量的，一个是用来 Least Recently Used 算法，可能是用来排除长时间不使用的 key 之类的。</p>
<p>到了这一步, 已经解析好了所有由客户端发出来的命令，并打包到 client 的 argc 跟 argv 字段 中，时刻准备着处理 :)</p>
<a class="header" href="print.html#a解析多个命令请求" id="a解析多个命令请求"><h3>解析多个命令请求</h3></a>
<p>另外一种情况是 reqtype 是 MULTIBULK 的时候，需要处理多个命令，我们看看实现做了什么。</p>
<pre><code class="language-c">int processMultibulkBuffer(redisClient *c) {
    chr *newline = NULL;
    int pos = 0, ok;
    long long ll;

    // 获取当前请求是由多少个命令组成的
    if (c-&gt;multibulklen == 0) {
        redisAssertWithInfo(c, NULL, c-&gt;argc == 0);

        newline = strchr(c-&gt;querybuf, '\r');
        if (newline == NULL) {
            // 异常处理
            return REDIS_ERR;
        }

        // querybuf 应该还包含 \n，所以 -2, 这是确认长度是正确的
        if (newline - (c-&gt;querybuf) &gt; ((signed)sdslen(c-&gt;querybuf) - 2)) {
            return REDIS_ERR;
        }

        redisAssertWithInfo(c, NULL, c-&gt;querybuf[0] == '*');
        ok = string2ll(c-&gt;querybuf + 1, newline - (c-&gt;querybuf + 1), &amp;ll);

        ...
    }
}
</code></pre>
<p>Ok, 在继续下去之前，我们从最后一句得到一个信息，也就是命令的格式：</p>
<pre><code class="language-json">    *NUMBER\r\n????
</code></pre>
<p>以 * 开头，以 \r\n 结尾的中间段，保存的是一个数字，用来表示之后的长度，具体是什么长度呢，继续分析。</p>
<pre><code class="language-c">        // ...
        // 长度不能超过 1024 * 1024
        if (!ok || ll &gt; 1024 * 1024) {
            // 异常处理
            return REDIS_ERR;
        }

        // pos 指向下一个命令的开始位置
        pos = (newline - c-&gt;querybuf) + 2;

        // 如果解析出来的命令数为0，则截断缓存后直接返回
        if (ll &lt;= 0) {
            sdsrange(c-&gt;querybuf, pos, -1);
            return REDIS_OK;
        }

        c-&gt;multibulklen = ll;

        // 分配内存用于存放命令参数
        if (c-&gt;argv) zfree(c-&gt;argv);
        c-&gt;argv = zmalloc(sizeof(robj *) * c-&gt;multibulklen);
    }

    redisAssertWithInfo(c, NULL, c-&gt;multibulklen &gt; 0);
    // 更具刚刚得到的命令数，逐个获取命令信息
    while (c-&gt;multibulklen) {
        // 获取当前命令的长度
        if (c-&gt;bulklen == -1) {
            newline = strchr(c-&gt;querybuf + pos, '\r');
            if (newline == NULL) {
                // 错误处理
                if (sdslen(c-&gt;querybuf) &gt; REDIS_INLINE_MAX_SIZE) {
                    return REDIS_ERR;
                }
                break;
            }

            // 如果 newline 位于 querybuf 的开头，则说明没东西可以处理了
            if (newline - (c-&gt;querybuf) &gt; ((signed)sdslen(c-&gt;queryuf) - 2)) {
                break;
            }

            // pos 现在当前命令的开始
            // 确认当前命令以 $ 开头
            if (c-&gt;querybuf[pos] != '$') {
                // 错误处理
                return REDIS_ERR;
            }

            ok = string2ll(c-&gt;querybuf + pos + 1, newline - (c-&gt;querybuf + pos + 1, &amp;ll);
            if (!ok || ll &lt; 0 || ll &gt; 512 * 1024 * 1024) {
                return REDIS_ERR;
            }

            // 获取到长度信息，更新 pos，指向下一个命令的位置
            pos += newline - (c-&gt;querybuf + pos) + 2;
            if (ll &gt;= REDIS_MBULK_BIG_ARG) {
                size_t qblen;

                // 如果现有的空间不够存放接下来的命令信息，则扩容 sds
                sdsrange(c-&gt;querybuf, pos, -1);
                pos = 0;
                qblen = sdslen(c-&gt;querybuf);
                if (qblen &lt; (size_t)ll + 2)
                    c-&gt;querybuf = sdsMakeRoomFor(c-&gt;querybuf, ll + 2 - qblen);
            }

            c-&gt;bulklen = ll;
        }
    }
</code></pre>
<p>再次暂停，从上面的分析我们基本得到了 BULK 模式下的消息结构</p>
<pre><code class="language-json">    *命令个数\r\n命令长度\r\n命令....
</code></pre>
<p>ok，可以继续了。</p>
<pre><code class="language-c">    // 如果 buffer 的长度不足以构成一个包，则跳出处理流程
    if (sdslen(c-&gt;querybuf) - pos &lt; (unsigned)(c-&gt;bulklen + 2)) {
        break;
    }
    else {
        // 如果已经处理到了最后
        if (pos == 0 &amp;&amp; c-&gt;bulklen &gt;= REDIS_MBULK_BIG_ARG &amp;&amp;
            (signed) sdslen(c-&gt;querybuf) == c-&gt;bulklen + 2) {
            c-&gt;argc[c-&gt;argc++] = createObject(REDIS_STRING, c-&gt;querybuf);
            sdsIncrLen(c-&gt;querybuf, -2);
            c-&gt;querybuf = sdsempty();
            c-&gt;querybuf = sdsMakeRoomFor(c-&gt;querybuf,c-&gt;bulklen + 2);
            pos = 0;
        }
        else {
            c-&gt;argv[c-&gt;argc++] =
                createStringObject(c-&gt;querybuf + pos, c-&gt;bulklen);
            pos += c-&gt;bulklen + 2;
        }
        c-&gt;bulklen = -1;
        c-&gt;multibulklen--;
    }

    if (pos) sdsrange(c-&gt;querybuf, pos, -1);
    if (c-&gt;multibulklen == 0) return REDIS_OK;

    return REDIS_ERR;
</code></pre>
<p>跟单行的请求一样，只是这里会同时处理多个请求，接下来我们可以真正的进入处理请求的流程了。</p>
<a class="header" href="print.html#a处理请求" id="a处理请求"><h2>处理请求</h2></a>
<p>上面的处理已经把所有的请求信息，都放进了当前 client 的 argc 跟 argv 中，接下来只需要遍历 argv 即可获取对应的参数跟数据进行处理了。</p>
<pre><code class="language-c">int processCommand(redisClient *c) {
    // 如果客户端发过来的是 quit 命令，则直接回复退出成功，并把客户端标示为 REDIS_CLOSE_AFTER_REPLY
    if (!strcasecmp(c-&gt;argv[0]-&gt;ptr, &quot;quit&quot;)) {
        addReply(c, shared.ok);
        c-&gt;flags |= REDIS_CLOSE_AFTER_REPLY;
    }
}
</code></pre>
<p>这时候我们可以确定刚刚处理命令时遇到的对 REDIS_CLOSE_AFTER_REPLY 的处理了，也就是说，如果客户已明确标示要断开连接了，那接下来所有的操作我们都可以直接忽略。 OK 继续。</p>
<pre><code class="language-c">    // 根据用户发送的命令，获取具体的 cmd
    c-&gt;cmd = c-&gt;lastcmd = lookupCommand(c-&gt;argv[0]-&gt;ptr);
    // 然后是一系列的错误检查，如果没有找到对应的命令，则说明命令格式错误
    // 然后如果参数的数量不一致的话，也视为出错
    if (!c-&gt;cmd) {
        flagTransction(c);
        addReplyErrorFormat(c, &quot;unknown command '%s'&quot;,
            (char *)c-&gt;argv[0]-&gt;ptr);
        return REDIS_OK;
    }
    else if ((c-&gt;cmd-&gt;arity &gt; 0 &amp;&amp; c-&gt;cmd-&gt;arity != c-&gt;argc) ||
                (c-&gt;argc &lt; -c-&gt;cmd-&gt;arity)) {
        flagTransaction(c);
        addReplyErrorFormat(c, &quot;wrong number of arguments for '%s' command&quot;,
            c-&gt;cmd-&gt;name);
        return REDIS_OK;
    }
</code></pre>
<p>接下来就是具体处理命令了，在开始处理之前我们先看看 redisCommand 的具体定义</p>
<pre><code class="language-c">struct redisCommand {
    // 命令名
    char *name;
    // 命令的函数指针
    redisCommandProc *proc;
    // 命令的参数数
    int arity;
    // 命令的标志，字符串表示形式，每个字符一个标示
    char *sflags;
    // 命令的标志，位模式，由 sflags 决定
    int flags;
    // 用于 Cluster 的转发，暂时不管
    redisGetKeysProc *getkeys_proc;
    // 以下几个不明，待分析
    int firstkey;
    int lastkey;
    int keystep;
    long long microseconds, calls;
}

typedef void redisCommandProc(redisClient *c);
</code></pre>
<p>然后继续处理命令</p>
<pre><code class="language-c">    // 检查服务是否需要验证，如果需要验证，则检查是否已通过验证，因为还没通过验证的客户端，只接受验证命令 authCommand
    if (server.requirepass &amp;&amp; !c-&gt;authenticated &amp;&amp; c-&gt;cmd-&gt;proc != authCommand) {
        flagTranscation(c);
        addReply(c, shared.noautherr);
        return REDIS_OK;
    }

     // 如果处于 Cluster 模式，则只处理以下两种情况
     //     1. 命令是从 Master 发过来的
     //     2. 命令没有参数
     if (server.cluster_enabled &amp;&amp;
         !(c-&gt;flags &amp; REDIS_MASTER) &amp;&amp;
         !(c-&gt;flags &amp; REDIS_LUA_CLIENT &amp;&amp;
           server.lua_caller-&gt;flags &amp; REDIS_MASTER) &amp;&amp;
         !(c-&gt;cmd-&gt;getkeys_proc == NULL &amp;&amp; c-&gt;cmd-&gt;firstkey = 0)) {
        int hashslot;

        // 如果当前的 Cluster 模式是正常的，执行 Redirect 操作，
        // 否则处理错误
        if (server.cluster-&gt;state != REDIS_CLUSTER_OK) {
            flagTranscation(c);
            clusterRedirectClient(c, NULL, REDIS_CLUSTER_REDIR_DOWN_STATE);
            return REDIS_OK;
        }
        else {
            int error_code;
            clusterNode *n = getNodeByQuery(c, c-&gt;cmd, c-&gt;argc,  &amp;hashslot, &amp;error_code);
            if (n == NULL || n != server.cluster-&gt;myself) {
                flagTranscation(c);
                clusterRedirectClient(c, n, hashslot, error_node);
                return REDIS_OK;
            }
        }
     }

     // 如果设置了内存限制，则检查内存使用状况
     if (server.maxmemory) {
        // 这里开始执行内存回收策略，遍历所有的 DB，根据 DB 配置的策略选择对应的 key 进行回收
        int retval = freeMemoryIfNeeded();
        if ((c-&gt;cmd-&gt;flags &amp; REDIS_CMD_DENYOOM) &amp;&amp; retval == REDIS_ERR) {
            flagTranscation(c);
            addReply(c, shared.oomerr);
            return REDIS_OK;
        }
     }

     // 当 bgsave 或者 aof 机制出错时，拒绝执行 &quot;Write&quot; 操作，也就是拒绝执行任何会导致数据写入磁盘的操作（包括 PING)
     if (((server.stop_writes_on_bgsave_err &amp;&amp;
           server.saveparamslen &gt; 0 &amp;&amp;
           server.lastbgsave_status == REDIS_ERR) ||
           server.aof_last_write_status == REDIS_ERR) &amp;&amp;
         server.masterhost == NULL &amp;&amp;
         (c-&gt;cmd-&gt;flags &amp; REDIS_CMS_WRIETE ||
          c-&gt;cmd-&gt;proc == pingCommand)) {

          flagTranscation(c);
          if (server.aof_last_write_status == REIDS_OK)
            addReply(c, shared.bgsaveerr);
          else
            addReplySds(c,
                sdscatprintf(sdsempty(),
                &quot;-MISCONF Errors writeing to the AOF fiel: %s\r\n&quot;,
                strerror(server.aof_last_write_errno)));
          return REDIS_OK;
      }

      // 如果设置了 min_slaves_to_write 最小从机数并且现在可用的从机少于这个数字，则拒绝 Write 操作
      if (server.masterhost == NULL &amp;&amp;
          server.repl_min_slaves_to_write &amp;&amp;
          server.repl_min_slaves_max_log &amp;&amp;
          c-&gt;cmd-&gt;flags &amp; REDIS_CMD_WRITE &amp;&amp;
          server.repl_good_slaves_count &lt; server.repl_min_slaves_to_write) {
          flagTranscation(c);
          addReply(c, shared.noreplicaserr);
          return REDIS_OK;
      }

      // 如果是从机，并且是只读的从机，则拒绝执行 Write 操作
      if (server.masterhost &amp;&amp; server.repl_slave_ro &amp;&amp;
          !(c-&gt;flags &amp; REDIS_MASTER) &amp;&amp;
          c-&gt;cmd-&gt;flags &amp; REDIS_CMD_WRITE) {
          addReply(c, shared.roslaveerr);
          return REDIS_OK;
      }

      // 如果当前客户端是 PUBSUB 模式，则只接受跟 pubsub 相关的操作
      if (c-&gt;flags &amp; REDIS_PUBSUB &amp;&amp;
          c-&gt;cmd-&gt;proc != pingCommand &amp;&amp;
          c-&gt;cmd-&gt;proc != subscribeCommand &amp;&amp;
          c-&gt;cmd-&gt;proc != unsubscribeCommand &amp;&amp;
          c-&gt;cmd-&gt;proc != psubscribeCommand &amp;&amp;
          c-&gt;cmd-&gt;proc != punsubscribeCommand) {
          addReplyError(c, &quot;only (P)SUBSCRIBE / (P)UNSUBSCRIBE / QUIT allowed in this context&quot;);
          return REDIS_OK;
      }

      // 如果从机的状态未跟主机同步，则只接受 INFO 跟 SLAVEOF 操作
      if (server.masterhost &amp;&amp; server.repl_state != REDIS_REPL_CONNECTED &amp;&amp;
          server.repl_server_state_data == 0 &amp;&amp;
          !(c-&gt;cmd-&gt;flags &amp; REDIS_CMD_STALE)) {
          flagTranscation(c);
          addReply(c, shared.masterdownerr);
          return REDIS_OK;
      }

      // 如果 Redis 服务仍在初始化阶段，则回复对应的信息给客户端
      if (server.loading &amp;&amp; !(c-&gt;cmd-&gt;flags &amp; REDIS_CMD_LOADING)) {
        addReply(c, shared.loadingerr);
        return REDIS_OK;
      }

      // 如果执行的 lua 脚本太慢了，则限制只能执行有限的操作
      if (server.lua_timeout &amp;&amp;
          c-&gt;cmd-&gt;proc != authCommand &amp;&amp;
          c-&gt;cmd-&gt;proc != replconfCommand &amp;&amp;
          !(c-&gt;cmd-&gt;proc == shutdownCommand &amp;&amp;
            c-&gt;argc == 2 &amp;&amp;
            tolower(((char *)c-&gt;argv[1]-&gt;prt)[0] == 'n') &amp;&amp;
          !(c-&gt;cmd-&gt;proc == scriptCommand &amp;&amp;
            c-&gt;argc == 2 &amp;&amp;
            tolower(((char *)c-&gt;argv[1]-&gt;ptr)[0] == 'k')) {

            flagTranscation(c);
            addReply(c, shared.slowscripterr);
            return REDIS_OK;
        }

        // 正式执行命令。。。
        // 不容易啊。。。。。
        // 如果当前处于多命令 MULTI 状态，则如果不是要求执行或取消已经放入队列的命令的话，将新命令加入执行队列
        if (c-&gt;flags &amp; REDIS_MULTI &amp;&amp;
            c-&gt;cmd-&gt;proc != execCommand &amp;&amp; c-&gt;cmd-&gt;proc != discardCommand &amp;&amp;
            c-&gt;cmp-&gt;proc != multiCommand &amp;&amp; c-&gt;cmd-&gt;proc != watchCommand) {
            queueMultiCommand(c);
            addReply(c, shared.queued);
        }
        else {
            call(c, REDIS_CALL_FULL);
            c-&gt;woff = server.master_repl_offset;
            if (listLength(server.ready_keys))
                handleClientBlockedOnLists();
        }

        return REDIS_OK;
}
</code></pre>
<p>到了这里，终于把接收请求跟对请求跟命令进行分析部分写完了，最后我们再来看看，执行 <code>Multi</code> 模式的命令是怎么加入队列的。</p>
<pre><code class="language-c">void queueMultiCommand(redisClient *c) {
    multiCmd *mc;
    int j;
    c-&gt;mstat.commands = zrealloc(c-&gt;mstate.commands,
        sizeof(multiCmd) * (c-&gt;mstate.count + 1));
    mc = c-&gt;mstate.commands + c-&gt;mstate.count;
    mc-&gt;cmd = c-&gt;cmd;
    mc-&gt;argc = c-&gt;argc;
    mc-&gt;argv = zmalloc(sizeof(robj *) * c-&gt;argc;
    memcpy(mc-&gt;argv, c-&gt;argv, sizeof(robj *) * c-&gt;argc);
    for (j = 0; j &lt; c-&gt;argc; j++)
        increRefCount(mc-&gt;argv[j]);
    c-&gt;mstate.count++;
}
</code></pre>
<p>这里可以看到，client 会维护一个 mstate 来保存当前命令的列表，在有新的命令进来时，将其加入命令列表，并增加对应对象的引用信息。引用信息是 robj 用来共享变量所使用的。</p>
<p>最后剩下的，就是命令具体的执行方式了。</p>
<a class="header" href="print.html#redis-内容篇-2" id="redis-内容篇-2"><h1>Redis 内容篇</h1></a>
<p>[TOC]</p>
<a class="header" href="print.html#a执行命令" id="a执行命令"><h2>执行命令</h2></a>
<a class="header" href="print.html#a上文回顾--执行逻辑" id="a上文回顾--执行逻辑"><h3>上文回顾 &amp;&amp; 执行逻辑</h3></a>
<p>在前面我们分析了解析部分的内容，解析完所有的信息后，最终我们来到了 <code>processCommand</code> 这个函数的最后，</p>
<pre><code class="language-c">void processCommand(redisClient *c) {
    // ...
    // Exec the command
    if (c-&gt;flags &amp; REDIS_MULTI &amp;&amp;
        c-&gt;cmd-&gt;proc != execCommand &amp;&amp; c-&gt;cmd-&gt;proc != discardCommand &amp;&amp;
        c-&gt;cmd-&gt;proc != multiCommand &amp;&amp; c-&gt;cmd-proc != watchCommand) {
        queueMultiCommand(c);
        addReply(c, shared.queued);   
    }
    else {
        call(c, REDIS_CALL_FULL);
        c-&gt;woff = server.master_repl_offset;
        if (listLength(server.ready_keys))
            handleClientBlockedOnLists();
    }
}
</code></pre>
<p>从上面可以看到，最终执行命令，是由 <code>call</code> 这个调用来实现的，所以我们跟进去看看它到底做了什么。</p>
<pre><code class="language-c">void call(redisClient *c, int flags) {
    long long dirty, start, duration;
    int client_old_flags = c-&gt;flags;
    
    // 如果有监控器在监控当前服务，则将当前要执行的命令信息发送给监控器
    if (listLength(server.monitors) &amp;&amp;
        !server.loading &amp;&amp;
        !(c-&gt;cmd-&gt;flags &amp; (REDIS_CMD_SKIP_MONITOR | REDIS_CMD_ADMIN))) {
        replicationFeedMonitors(c, server.monitors, c-&gt;db-&gt;id, c-&gt;argv, c-&gt;argc);   
    }
    
    // 清除除了 FORCE_AOF 跟 FORCE_REPL 以外的 flag
    c-&gt;flags &amp;= ~(REDIS_FORCE_AOF | REDIS_FORCE_REPL);
    redisOpArrayInit(&amp;server.also_propagete);
    // 开始记录执行状态
    dirty = server.dirti;
    start = ustime();
    
    c-&gt;cmd-&gt;proc();
    
    duration = ustime() - start;
    dirty = server.dirty - dirty;
    if (dirty &lt; 0) dirty = 0;
 
    // ...   
}
</code></pre>
<p>接下来保存调用状况，并且根据 flags 决定是否要执行一系列操作</p>
<ul>
<li>
<p>slow log</p>
<ul>
<li>根据调用的耗时，决定是否将耗时较长的命令记录到 Slow log 中</li>
</ul>
</li>
<li>
<p>propagete</p>
<ul>
<li>所谓的 propagete 就是：</li>
<li>是否执行 AOF 将当前命令的记录下来</li>
<li>是否需要将命令分发到 Slaves，让他们执行相同的命令，以保持主从一致的 状态</li>
</ul>
</li>
</ul>
<a class="header" href="print.html#a命令指针列表" id="a命令指针列表"><h3>命令指针列表</h3></a>
<p>上述分析完毕，我们开始进入具体的 Redis 命令，因为上文的 <code>c-&gt;cmd-&gt;proc()</code> 已经正式执行了 <code>client</code> 发送过来的命令。
首先我们看看共有哪些命令, 这些命令分别包含了些什么信息，从之前的分析我们首先可以得到，命令会说明自己的参数个数、函数指针等信息。</p>
<pre><code class="language-c">struct redisCommand redisCommandTable[] = {
    {&quot;get&quot;,getCommand,2,&quot;rF&quot;,0,NULL,1,1,1,0,0},
    {&quot;set&quot;,setCommand,-3,&quot;wm&quot;,0,NULL,1,1,1,0,0},
    {&quot;setnx&quot;,setnxCommand,3,&quot;wmF&quot;,0,NULL,1,1,1,0,0},
    {&quot;setex&quot;,setexCommand,4,&quot;wm&quot;,0,NULL,1,1,1,0,0},
    {&quot;psetex&quot;,psetexCommand,4,&quot;wm&quot;,0,NULL,1,1,1,0,0},
    {&quot;append&quot;,appendCommand,3,&quot;wm&quot;,0,NULL,1,1,1,0,0},
    {&quot;strlen&quot;,strlenCommand,2,&quot;rF&quot;,0,NULL,1,1,1,0,0},
    {&quot;del&quot;,delCommand,-2,&quot;w&quot;,0,NULL,1,-1,1,0,0},
    {&quot;exists&quot;,existsCommand,-2,&quot;rF&quot;,0,NULL,1,-1,1,0,0},
    {&quot;setbit&quot;,setbitCommand,4,&quot;wm&quot;,0,NULL,1,1,1,0,0},
    {&quot;getbit&quot;,getbitCommand,3,&quot;rF&quot;,0,NULL,1,1,1,0,0},
    {&quot;setrange&quot;,setrangeCommand,4,&quot;wm&quot;,0,NULL,1,1,1,0,0},
    {&quot;getrange&quot;,getrangeCommand,4,&quot;r&quot;,0,NULL,1,1,1,0,0},
    {&quot;substr&quot;,getrangeCommand,4,&quot;r&quot;,0,NULL,1,1,1,0,0},
    {&quot;incr&quot;,incrCommand,2,&quot;wmF&quot;,0,NULL,1,1,1,0,0},
    {&quot;decr&quot;,decrCommand,2,&quot;wmF&quot;,0,NULL,1,1,1,0,0},
    {&quot;mget&quot;,mgetCommand,-2,&quot;r&quot;,0,NULL,1,-1,1,0,0},
    {&quot;rpush&quot;,rpushCommand,-3,&quot;wmF&quot;,0,NULL,1,1,1,0,0},
    {&quot;lpush&quot;,lpushCommand,-3,&quot;wmF&quot;,0,NULL,1,1,1,0,0},
    {&quot;rpushx&quot;,rpushxCommand,3,&quot;wmF&quot;,0,NULL,1,1,1,0,0},
    {&quot;lpushx&quot;,lpushxCommand,3,&quot;wmF&quot;,0,NULL,1,1,1,0,0},
    {&quot;linsert&quot;,linsertCommand,5,&quot;wm&quot;,0,NULL,1,1,1,0,0},
    {&quot;rpop&quot;,rpopCommand,2,&quot;wF&quot;,0,NULL,1,1,1,0,0},
    {&quot;lpop&quot;,lpopCommand,2,&quot;wF&quot;,0,NULL,1,1,1,0,0},
    {&quot;brpop&quot;,brpopCommand,-3,&quot;ws&quot;,0,NULL,1,1,1,0,0},
    {&quot;brpoplpush&quot;,brpoplpushCommand,4,&quot;wms&quot;,0,NULL,1,2,1,0,0},
    {&quot;blpop&quot;,blpopCommand,-3,&quot;ws&quot;,0,NULL,1,-2,1,0,0},
    {&quot;llen&quot;,llenCommand,2,&quot;rF&quot;,0,NULL,1,1,1,0,0},
    {&quot;lindex&quot;,lindexCommand,3,&quot;r&quot;,0,NULL,1,1,1,0,0},
    {&quot;lset&quot;,lsetCommand,4,&quot;wm&quot;,0,NULL,1,1,1,0,0},
    {&quot;lrange&quot;,lrangeCommand,4,&quot;r&quot;,0,NULL,1,1,1,0,0},
    {&quot;ltrim&quot;,ltrimCommand,4,&quot;w&quot;,0,NULL,1,1,1,0,0},
    {&quot;lrem&quot;,lremCommand,4,&quot;w&quot;,0,NULL,1,1,1,0,0},
    // 由于太多，所以我们暂时只列出一部分...
};
</code></pre>
<p>上面包括了常用的部分 <code>redis</code> 命令，我们选第一个最常用的 <code>get</code> 命令开始说明，首先上面列表是 <code>redisCommand</code> 的数组，而 <code>redisCommand</code> 我们之前已经说明过了，这里再针对上面列出的各个信息进行简单说明。</p>
<pre><code class="language-c">struct redisCommand {
    char *name;
    redisCommandProc *proc;
    int arity;
    char *sflags;
    int flags;
    redisGetKeysProc *getkeys_proc;
    int firstkey;
    int lastkey;
    int keystep;
    long long microseconds, calls;
};
</code></pre>
<p>从上面的结构定义，再代入</p>
<pre><code class="language-c">{&quot;get&quot;,getCommand,2,&quot;rF&quot;,0,NULL,1,1,1,0,0}, 
</code></pre>
<p>即可得到：</p>
<blockquote>
<p><code>get</code> 是命令的名称，</p>
<p><code>getCommand</code> 是具体调用的函数指针，</p>
<p><code>arity</code> 这个命令需要 2 个参数，</p>
<p><code>rF</code> 是他的标签，接下来的 0 是 flags 字段，他会由 redis 根据标签计算得出具体的值</p>
<p>接下来是 <code>firstkey</code>、<code>lastkey</code>, <code>keystep</code> 三个字段，他们分别表示，从第几个参数索引开始是对应到命令的具体参数，最后一个索引又是多少，每隔多少个 argv 对应一个参数</p>
<p>最后的 microseconds, calls 则是用来分析所用，他们保存了该命令总共调用了多少次，调用了这么多次共耗时多少。</p>
</blockquote>
<p>最后，在开始执行对应的命令前，附上 <code>redis</code> 对 flag 的说明：</p>
<pre><code class="language-c"> /*
 * This is the meaning of the flags:
 *
 * w: write command (may modify the key space).
 * r: read command  (will never modify the key space).
 * m: may increase memory usage once called. Don't allow if out of memory.
 * a: admin command, like SAVE or SHUTDOWN.
 * p: Pub/Sub related command.
 * f: force replication of this command, regardless of server.dirty.
 * s: command not allowed in scripts.
 * R: random command. Command is not deterministic, that is, the same command
 *    with the same arguments, with the same key space, may have different
 *    results. For instance SPOP and RANDOMKEY are two random commands.
 * S: Sort command output array if called from script, so that the output
 *    is deterministic.
 * l: Allow command while loading the database.
 * t: Allow command while a slave has stale data but is not allowed to
 *    server this data. Normally no command is accepted in this condition
 *    but just a few.
 * M: Do not automatically propagate the command on MONITOR.
 * k: Perform an implicit ASKING for this command, so the command will be
 *    accepted in cluster mode if the slot is marked as 'importing'.
 * F: Fast command: O(1) or O(log(N)) command that should never delay
 *    its execution as long as the kernel scheduler is giving us time.
 *    Note that commands that may trigger a DEL as a side effect (like SET)
 *    are not fast commands.
 */
</code></pre>
<a class="header" href="print.html#getset" id="getset"><h2>GET/SET</h2></a>
<a class="header" href="print.html#get" id="get"><h3>GET</h3></a>
<blockquote>
<p><code>{ &quot;get&quot;, getCommand, 2, &quot;rF&quot;, 0, NULL, 1, 1, 1, 0, 0 }</code>
名称: get
调用: getCommand
参数个数: 2
标志: rF Read And Fast
参数从 1 开始，到 1 结束，步进为 1
后续的接口不再逐个说明，但会列出结构</p>
</blockquote>
<p>get 命令从 <code>redis</code> 中根据 key 值获取对应的 value, key 跟 value 都必须是字符串。</p>
<pre><code class="language-c">void getCommand(redisClient *c) {
    getGenericCommand(c);
}

int getGenericCommand(redisClient *c) {
    robj *o;
    
    if ((o == lookupKeyReadOrReply(c, c-&gt;argv[1], shared.nullbulk)) == NULL)
        return REDIS_OK;
        
    if (o-&gt;type != REDIS_STRING) {
        addReply(c, shared.wrongtypeerr);
        return REDIS_ERR;
    }
    else {
        addReply(c, o);
        return REDIS_OK;
    }
}
</code></pre>
<p>从上面可以大致看出，搜索一个 key 的操作被封装在了 <code>lookupKeyReadOrReply</code>，同时可以看到的是 argv[1] 确实是作为参数传递给了 <code>lookupKeyReadOrReply</code>, 在搜完之后对得到的结果的类型进行验证，因为 Get 支持设置 String。接着是看看 <code>lookupKeyReadOrReply</code></p>
<pre><code class="language-c">robj *lookupKeyReadOrReply(redisClient *c, robj *key, robj *reply) {
    robj *o = lookupKeyRead(c-&gt;db, key);
    if (!o) addReply(c, reply);
    return o;
}

int expireIfNeeded(redisDb *db, robj *key) {
    mstime_t when = getExpire(db, key);
    mstime_t now;
    
    if (when &lt; 0) return 0;
    now = server.lua_caller ? server.lua_time_start : mstime();
    
    if (server.masterhost != NULL) return now &gt; when;
    
    if (now &lt;= when) return 0;
    
    server.stat_expiredKeys++;
    propagateExpire(db, key);
    notifyKeyspaceEvent(REDIS_NOTIFY_EXPIRED,
        &quot;expired&quot;, key, db-&gt;id);
    return dbDelete(db, key);
}

robj *lookupKeyRead(redisDb *db, robj *key) {
    robj *val;
    expireIfNeeded(db, key);
    val = lookupKey(db, key);
    if (val == NULL)
        server.stat_keyspace_misses++;
    else
        server.stat_keyspace_hits++;
    return val;
}
</code></pre>
<p>具体实现的 <code>lookupKeyRead</code> 首先检查了当前需要检索的 key 是否已经过期，如果还没过期或者根本没有设置过期则直接返回，否则将过期的 key 传播到其他的 slaves, 最后将其从数据库中删除。
在处理 <code>propagateExpire</code> 时还会根据是否开启了 AOF,如果开启了，则将操作记录到 AOF 文件中。
最后是具体的查找动作</p>
<pre><code class="language-c">robj *lookupKey(redisDb *db, robj *key) {
    dictEntry *de = dictFind(db-&gt;dict, key-&gt;prt);
    if (de) {
        robj *val = dictGetVal(de);
        
        if (server.rdb_child_pid == -1 &amp;&amp; server.aof_child_pid == -1)
            val-&gt;lru = LRU_CLOCK();
        return val;
    }
    else {
        return NULL;
    }
}
</code></pre>
<p>最终的查找动作落在了 <code>dictFind</code> 身上，也就是直接进行了一次 Hash 查找，具体的实现在 <code>dict.h</code> 我们已经详细说明过了。
在查到目标 value 后，如果当前没有在后台处理 rdb 或 aof 机制，则更新 value 对应的 lru，可用于内存满载时的调整机制。</p>
<a class="header" href="print.html#reply-preview" id="reply-preview"><h4>REPLY Preview</h4></a>
<p>在上面我们看到了 <code>addReply</code> 调用，这是 redis 用来回复信息给客户端使用的接口，在这里我们先简单的理解为，调用这个接口后，相应的数据就会回复给客户端即可，因为这里面涉及的东西较多，也跟具体的 get 逻辑无关，所以后面再另开篇章描述。</p>
<a class="header" href="print.html#mget" id="mget"><h3>MGET</h3></a>
<blockquote>
<p>{ &quot;mget&quot;, mgetCommand, -2, &quot;r&quot;, 0, NULL, 1, -1, 1, 0, 0 }</p>
</blockquote>
<p>跟 get 一样，但是支持同时获取多个 key 的值。其实就是重复实现 m 次 get 的逻辑，不同的是，对于这种多返回值的操作，redis 会先返回即将返回的值的个数，然后再逐个返回，简化了服务端的处理，也给了客户端较大的自由度来组织数据。</p>
<pre><code class="language-c">void mgetCommand(redisClient *c) {
    int j;
    addReplyMultiBulkLen(c, c-&gt;argc - 1);
    for (j = 1; j &lt; c-&gt;argc; j++) {
        robj *o = lookupKeyRead(c-&gt;db, c-&gt;argv[j]);
        if (o == NULL) {
            addReply(c, shared.nullbulk);
        }
        else {
            if (o-&gt;type != REDIS_STRING) {
                addReply(c, shared.nullbulk);
            }
            else {
                addReplyBulk(c, o);
            }
        }
    }
}
</code></pre>
<a class="header" href="print.html#set" id="set"><h3>SET</h3></a>
<blockquote>
<p>{ &quot;set&quot;, setCommand, -3, &quot;wm&quot;, 0, NULL, 1, 1, 1, 0, 0 }</p>
</blockquote>
<p><code>set</code> 命令将 key 对应的 value 保存到 redis 中。同时还支持在设置的同时，设置 NX XX EX PX 标签，分别用于指示：</p>
<blockquote>
<p>EX 以秒为单位设置超时时间
PX 以毫秒为单位，设置超时时间
XX 当数据库中存在 key 时才进行 set 操作
NX 当数据库中不存在 key 时才进行 set 操作</p>
</blockquote>
<pre><code class="language-c">void setCommand(redisClient *c) {
    int j;
    robj *expire = NULL;
    int unit = UNIT_SECONDS;
    int flags = REDIS_SET_NO_FLAGS;
    
    // 获取额外的操作标签，即上面提到的 EX、PX 等
    for (j = 3; j &lt; c-&gt;argc; j++) {
        char *a = c-&gt;argv[j]-&gt;ptr;
        robj *next = (j == c-&gt;argc - 1) ? NULL : c-&gt;argv[j + 1];
        // 处理 NX
        if ((a[0] == 'n' || a[0] == 'N') &amp;&amp;
            (a[1] == 'x' || a[1] == 'X') &amp;&amp; a[2] == '\0') {
            flags |= REDIS_SET_NX;
        }
        // 处理 XX
        else if ((a[0] == 'x' || a[0] == 'X') &amp;&amp;
                 (a[1] == 'x' || a[1] == 'X') &amp;&amp; a[2] == '\0') {
            flags |= REDIS_SET_XX;       
        }
        // 处理 EX 超时
        else if ((a[0] == 'e' || a[0] == 'E') &amp;&amp;
                 (a[1] == 'x' || a[1] == 'X') &amp;&amp; a[2] == '\0' &amp;&amp; next) {
            unit = UNIT_SECONDS;
            expire = next;
            j++;        
        }
        else if ((a[0] == 'p' || a[0] == 'P') &amp;&amp;
                 (a[1] == 'x' || a[1] == 'X') &amp;&amp; a[2] == '\0' &amp;&amp; next) {
            unit = UNIT_MILLSECONDS;
            expire = next;
            j++       
        }
        else {
            addReply(c, shared.syntaxerr);
            return;
        }        
    }   
    
    c-&gt;argv[2] = tryObjectEncoding(c-&gt;argv[2]);
    setGenericCommand(c, flags, c-&gt;argv[1], c-&gt;argv[2], expire, unit, NULL, NULL);
}
</code></pre>
<p>这里在进入真正的 Set 命令前，尝试将 <code>argv[2]</code> 进行了一次编码，这里的编码是之前说明过的，尝试编码为数字，短字符串跟长字符串等。
然后我们进入 set 的逻辑</p>
<pre><code class="language-c">void setGenericCommand(redisClient *c, int flags, robj *key, robj *val, robj *expire, int unit, robj *ok_reply, robj *abort_reply) {
    long long milliseconds = 0;
    
    if (expire) {
        // 从 obj 中获取对应的数值
        if (getLongLongFromObjectOrReply(c, expire, &amp;milliseconds, NULL) != REDIS_OK)
            return;
        if (milliseconds &lt;= 0) {
            addReplyErrorFormat(c,  &quot;invalid expire time in %s&quot;, c-&gt;cmd-&gt;name);
            return;
        }
        // 如果是以 秒 为单位
        if (unit == UNIT_SECONDS) milliseconds *= 1000;
    }
    
    // 根据标签进行检查
    if ((flags &amp; REDIS_SET_NX &amp;&amp; lookupKeyWrite(c-&gt;db, key) != NULL) ||
        (flags &amp; REDIS_SET_XX &amp;&amp; lookupKeyWriete(c-&gt;db, key) == NULL)) {
        addReply(c, abort_reply ? abort_reply : shared.nullbulk);
        return;   
    }
    
    setKey(c-&gt;db, key, val);
    
    // 从这里可以判断，dirty 是用来记录数据在上次静态化到现在之间被修改的次数
    server.dirty++;
    // 这里则可以窥探到 过期机制
    if (expire) setExpire(c-&gt;db, key, mstime() + milliseconds);    
    notifyKeyspaceEvent(REDIS_NOTIFY_STRING, &quot;set&quot;, key, c-&gt;db-&gt;id);
    if (expire) notifyKeyspaceEvent(REDIS_NOTIFY_GENERIC,
                    &quot;expire&quot;,
                    key,
                    c-&gt;db-&gt;id);
    addReply(c, ok_reply ? ok_reply : shared.ok);
}
</code></pre>
<p>上面主要的逻辑是 <code>setKey</code> 它负责具体保存 key 到 redis 中，而这里的两个 notify 则会将状态改变的消息推送给订阅了对应的 key 状态提醒的用户。这里面主要只是构建一个推送的 key，然后通过 <code>pubsub</code> 接口推送，我们在介绍 pubsub 时再进行说明。</p>
<pre><code class="language-c">void setKey(redisDb *db, robj *key, robj *val) {
    // 查找 key 是否存在，存在则覆盖，不存在则添加
    // 这里的 lookupKeyWrite 跟之前的 lookupKeyRead 没什么区别
    if (lookupKeyWrite(db, key) == NULL) {
        dbAdd(db, key, val);
    }
    else {
        dbOverwrite(db, key, val);
    }
    
    // 添加引用
    incrRefCount(val);
    // 如果已过期，则删除过期的 key
    removeExpire(db, key);
    singalModifiedKey(db, key);
}

void dbAdd(redisDb *db, robj *key, robj *val) {
    sds copy = sdsdup(key-&gt;ptr);
    int retval = dictAdd(db-&gt;dict, copy, val);
    
    redisAsservWithInfo(NULL, key, retval == REDIS_OK);
    if (val-&gt;type == REDIS_LIST) signalListAsReady(db, key);
    if (server.cluster_enabled) slotToKeyAdd(key);
}
</code></pre>
<p><code>dbAdd</code> 只是简单的调用了 dict 对应的函数，这里要注意的只有
<code>signalListAsReady</code>, 因为 redis 提供了堵塞的相关接口，如 <code>lpushx</code>, 他只会在 key 对应的 list 已经存在时才能 push 成功，所以这里的 <code>signalListAsReady</code> 则负责将当前的 key 加到 ready_keys 中，好让 redis 能通知客户端 lpushx 的结果。</p>
<p>接下来是覆盖操作</p>
<pre><code class="language-c">void dbOverwrite(redisDb *db, robj *key, robj *val) {
    dictEntry *de = dictFind(db-&gt;dict, key-&gt;prt);
    redisAssertWithInfo(NULL, key, de != NULL);
    dictReplace(db-&gt;dict, key-&gt;prt, val);
}
</code></pre>
<p>也只是简单的调用了 dict 的接口，并做了一些 assert。</p>
<a class="header" href="print.html#setnx" id="setnx"><h3>SETNX</h3></a>
<p>跟 set 同效，但只有当对应的 key 不存在时，才会 set 成功，具体的实现跟 set 并无二致，只是在调用前设置了对应的 flag: <code>REDIS_SET_NX</code></p>
<pre><code class="language-c">void setnxCommand(redisClient *c) {
    c-&gt;argv[2] = tryObjectEncoding(c-&gt;argv[2]);
    setGenericCommand(c, REDIS_SET_NX, c-&gt;argv[1], c-&gt;argv[2], NULL, 0, shared.cone, shared.czero);
}
</code></pre>
<a class="header" href="print.html#setex" id="setex"><h3>SETEX</h3></a>
<p>跟 set 同效，在设置 key 对应的 value 时，同时还会设置过期时间。</p>
<pre><code class="language-c">void setexCommand(redisClient *c) {
    c-&gt;argv[3] = tryObjectEncoding(c-&gt;argv[3]);
    setGenericCommand(c, REDIS_NO_FLAGS, c-&gt;argv[1], c-&gt;argv[3, c-&gt;argv[2], UNIT_SECONDS, NULL, NULL);
}
</code></pre>
<a class="header" href="print.html#mset--msetnx" id="mset--msetnx"><h3>MSET / MSETNX</h3></a>
<p>批量设置新的键值对</p>
<pre><code class="language-c">void msetCommand(redisClient *c) {
    msetGenericCommand(c, 0);
}

void msetnxCommand(redisClient *c) {
    msetGenericCommand(c, 1);
}

void msetGenericCommand(redisClient *c, int nx) { 
    int j, busykeys = 0;
    
    if ((c-&gt;argc % 2) == 0) {
        addReplyError(c, &quot;wrong number of arguments for MSET&quot;);
        return;
    }
    
    // 如果设置了 nx 标志，则说明所有需要设置的 key 都不可以存在于 redis 中
    if (nx) {
        for (j = 1; j &lt; c-&gt;argc; j += 2) {
            if (lookupKeyWrite(c-&gt;db, c-&gt;argv[j]) != NULL) {
                busykeys++;
            }
        }
        if (busykeys) {
            addReply(c, shared.czero);
        }
    }
    
    for (j = 1; j &lt; c-&gt;argc; j += 2) {
        c-&gt;argv[j + 1] = tryObjectEncoding(c-&gt;argv[j + 1]);
        setKey(c-&gt;db, c-&gt;argv[j], c-&gt;argv[j + 1]);
        notifyKeyspaceEvent(REDIS_NOTIFY_STRING, &quot;set&quot;,
            c-&gt;argv[j], d-&gt;db-&gt;id);
    }
    
    server.dirty += (c-&gt;argc - 1) / 2;
    addReply(c, nx ? shared.cone : shared.ok);
}
</code></pre>
<p>上面的 <code>msetCommand</code> 转调的是 <code>msetGenericCommand</code>，同时传递了 0 作为 nx 参数，而在 <code>msetGenericCommand</code> 中 nx 是作为一个标识符，用于指示即将插入的键值对能否已存在 redis 之中。 0 则表示不做限制。</p>
<p>而对于 <code>msetnxCommand</code> 来说则需要限制，所以传递了 1。</p>
<a class="header" href="print.html#basic" id="basic"><h2>BASIC</h2></a>
<p>下面列出的是跟类型无关的基础指令</p>
<a class="header" href="print.html#del" id="del"><h3>DEL</h3></a>
<p>删除 key 对应的 value，主要的实现为，接收变长的参数列表，列表中都为 redis 中的 key, 把所有的 key 都从 redis 中删掉，然后提醒订阅了这些 key 状态的客户端</p>
<pre><code class="language-c">void delCommand(redisClient *c) {
    int deleted = 0, j;
    
    for (j = 1; j &lt; c-&gt;argc; j++) {
        expireIfNeeded(c-&gt;db, c-&gt;argv[j]);
        if (dbDelete(c-&gt;db, c-&gt;argv[j])) {
            // 如果删除成功，则提醒订阅了这些 key 的客户端
            signalModifiedKey(c-&gt;db, c-&gt;argv[j]);
            notifyKeyspaceEvent(REDIS_NOTIFY_GENERIC,
                &quot;del&quot;, c-&gt;argv[j], c-&gt;db-&gt;id);
            server.dirty++;
            deleted++;
        }
    }
    addReplyLongLong(c, deleted);
}
</code></pre>
<a class="header" href="print.html#exists" id="exists"><h3>EXISTS</h3></a>
<p>判断对应的 key 是否存在于 redis 中。</p>
<pre><code class="language-c">void existsCommand(redisClient *c) {
    long long count = 0;
    int j;
    
    for (j = 1; j &lt; c-&gt;argc; j++) {
        expireIfNeeded(c-&gt;db, c-&gt;argv[j]);
        if (dbExists(c-&gt;db, c-&gt;argv[j])) count++;
    }
    
    addReplyLongLong(c, count);
}
</code></pre>
<a class="header" href="print.html#ping" id="ping"><h3>PING</h3></a>
<a class="header" href="print.html#echo" id="echo"><h3>ECHO</h3></a>
<a class="header" href="print.html#auto" id="auto"><h3>AUTO</h3></a>
<a class="header" href="print.html#shutdown" id="shutdown"><h3>SHUTDOWN</h3></a>
<a class="header" href="print.html#bgsave" id="bgsave"><h3>BGSAVE</h3></a>
<a class="header" href="print.html#list" id="list"><h2>LIST</h2></a>
<a class="header" href="print.html#lpushlpop-lpushxlpopx" id="lpushlpop-lpushxlpopx"><h3>LPUSH/LPOP <em>(LPUSHX/LPOPX)</em></h3></a>
<a class="header" href="print.html#rpushrpop-rpushxrpopx" id="rpushrpop-rpushxrpopx"><h3>RPUSH/RPOP <em>(RPUSHX/RPOPX)</em></h3></a>
<a class="header" href="print.html#linsert" id="linsert"><h3>LINSERT</h3></a>
<a class="header" href="print.html#llen" id="llen"><h3>LLEN</h3></a>
<a class="header" href="print.html#lindex" id="lindex"><h3>LINDEX</h3></a>
<a class="header" href="print.html#lrange" id="lrange"><h3>LRANGE</h3></a>
<a class="header" href="print.html#ltrim" id="ltrim"><h3>LTRIM</h3></a>
<a class="header" href="print.html#lset" id="lset"><h3>LSET</h3></a>
<a class="header" href="print.html#string" id="string"><h2>STRING</h2></a>
<a class="header" href="print.html#set-1" id="set-1"><h2>SET</h2></a>
<a class="header" href="print.html#distributed" id="distributed"><h1>Distributed</h1></a>
<a class="header" href="print.html#mapreduce-以简易的方式在庞大集群中处理数据" id="mapreduce-以简易的方式在庞大集群中处理数据"><h2>MapReduce: 以简易的方式在庞大集群中处理数据</h2></a>
<p>[TOC]</p>
<a class="header" href="print.html#a摘要-abstract" id="a摘要-abstract"><h2>摘要 Abstract</h2></a>
<p><code>MapReduce</code> 是一个用来处理或生成大型数据集的编程模型的具体实现. 用户通过 <code>map</code> 函数对 <em>键值对</em> 的输入进行处理，从而产生一系列的中间 <em>键值对</em>, 然后 <code>reduce</code> 函数合并所有 <em>键</em> 相同的 <em>值</em>. 许多现实中的任务都能够使用这个模型来表达，就像我们将在本论文中介绍的。</p>
<p>使用这个函数式风格实现的函数，能够在一个使用普通商业计算机的集群中，自动的并行执行. 运行时会对输入的数据进行分区、调度到集群中的机器中运行，并处理机器的机及机器之间的通信问题. 这能够使程序员在不具有 <em>并行化</em> 或 <em>分布式系统</em>的经验时，也能够非常容易的将整个分布式系统的资源利用起来.</p>
<p>我们对 <code>MapReduce</code> 的实现能在使用普通商业计算机组成的庞大集群中运行且具有高度的可扩展性：一个典型的 <code>MapReduce</code> 计算能够在数以千计的机器上处理巨大的数据量。程序员们也会发现整个系统是如此的易于使用：每天都会有数以百计的 <code>MapReduce</code> 程序被实现并转换为上千个 <code>MapReduce</code> 的任务在 <code>Google</code> 的集群中运行。</p>
<a class="header" href="print.html#a1-介绍-introduction" id="a1-介绍-introduction"><h2>1. 介绍 Introduction</h2></a>
<p>在过去的五年中，包括作者在内的许多谷歌的员工为了各种目的实现了数以百计的程序用来处理大量的原始数据，比如处理网页的爬虫、页面请求日志等，为了处理各种不同类型及来源的数据，比如倒排索引、各种网页的图结构的表示方式，每个站点中各个页面的概要等，以及每天都最频繁执行的查询。这些任务中的大部分都是比较简单的。但是，因为这些计算所需输入的数据往往都是非常巨大的，所以程序必须能够分布在数以百计或千计的机器上去运行，才能够在可接受的时间内完成。问题在于，让程序能够并行化，让数据分布到各个机器上以及处理分布任务的失效等，比原本需要实现的计算逻辑更为复杂。</p>
<p>为了应对这个难题，我们设计了一个允许我们表达原本简单的计算，同时隐藏背后复杂的并行化、容错、数据分布及负载均衡的新的抽象。这个抽象是从 <code>Lisp</code> 及类似的函数式编程语言中的 <code>map</code> 及 <code>reduce</code> 模型启发而来的。我们发现大部分的计算都类似于通过将逻辑上的 <code>记录</code> 作为 <code>map</code> 操作的输入，然后产生一个中间态的 <code>key/value</code> 集合，然后将集合中所有 <code>key</code> 相同的记录作为同一个 <code>reduce</code> 操作的参数，最终能够合理的将这些数据聚合起来。将用户实现的 <code>map</code> 跟 <code>reduce</code> 操作应用到这个函数式模型中，允许我们可以很简单的将计算并行化，并使用函数的可重入性作为基础机制来支持容错。</p>
<p>我们以上设计的提供了简单且强大的接口，让我们能够自动的并行化及得到可扩展的分布式计算能力，结合这个接口的实现使我们得到了能够高性能的运行在普通商业计算机集群上。</p>
<p>第二节将介绍基础的编程模型并提供几个实例；第三节将介绍一个<code>MapReduce</code> 的实现能够适配我们现有的集群计算环境；第四节将介绍几个我们精炼出来的有用的编程模型。第五节则是确认了我们的实现在各种多样化的任务中的性能；第六节介绍了在 Google 中使用 <code>MapReduce</code> 的经验，其中包括了重写产品级的索引系统；第七节讨论了一些相关及未来的工作。</p>
<a class="header" href="print.html#a2-编程模型-programming-model" id="a2-编程模型-programming-model"><h2>2. 编程模型 Programming Model</h2></a>
<p>计算任务会获取一个 <code>Key/Value</code> 的键值对集合，然后产生一个 <code>Key/Value</code> 的键值对集合作为输出。而 <code>MapReduce</code> 库的用户则负责将计算任务表达为两个函数： <code>Map</code> 及 <code>Reduce</code>。</p>
<p><code>Map</code> 是由用户实现的一个接收一个键值对并输出一个中间态的键值对的函数。<code>MapReduce</code> 库则将这些中间态的值通过相同的 <code>Key</code> 进行关联及分组，并传递给下一步的 <code>Reduce</code> 函数。</p>
<p><code>Reduce</code> 函数一样是由用户实现，接收一个来自中间态的 <code>Key</code>，及一个由该 <code>Key</code> 组合得到的 <code>Value</code> 集合。他负责合并这些 <code>Value</code> 为一个更小的结果集合。典型的场景是每个 <code>Reduce</code> 的调用会产生 0 或 1 个输出。从中间态得到的 <code>Value</code> 集合通过迭代器的方式传递给用户实现的 <code>Reduce</code> 函数，这种方式使我们能够处理大于内存的数据。</p>
<a class="header" href="print.html#a21-示例-example" id="a21-示例-example"><h3>2.1 示例 Example</h3></a>
<p>考虑一个问题，为统计一大批的文档集合中，各个词出现的次数。用户可能会实现类似下面的伪代码：</p>
<pre><code class="language-java">map(String key, String value):
    // key: document name
    // value: document contents
  for each word w in value:
    EmitIntermediate(w, &quot;1&quot;)
      
reduce(String key, Iterator values):
    // key: a word
    // values: a list of counts
    int result = 0;
    for each v in values:
    result += ParseInt(v);
    Emit(AsString(result))
</code></pre>
<p><code>Map</code> 函数将每个词作为 key，及将其关联的统计数（即是 example 中的 &quot;1&quot;）通过 <code>EmitItermediate</code> 提交出来。而 <code>Reduce</code> 函数则计算所有词的个数总和，并将总和与该词一起通过 <code>Emit</code> 提交。</p>
<p>除此之外，用户写的代码去填充 <code>MapReduce</code> 规则对象中定义中，输入文件、输出文件一些其他的参数。然后用户将规则对象作为参数开始启动 <code>MapReduce</code> 函数，这个时候，用户实现的代码将会跟由 <code>C++</code> 实现的 <code>MapReduce</code> 库进行链接。附录 A 包含该实例的完整代码。</p>
<a class="header" href="print.html#a22-类型-types" id="a22-类型-types"><h3>2.2 类型 Types</h3></a>
<p>尽管上一小节的伪代码是用字符串来描述输入跟输出，概念上，<code>Map</code> 跟 <code>Reduce</code> 函数提供给用户的是关联的数据类型：</p>
<pre><code>map    (k1, v1)				 -&gt; list(k2, v2)
reduce (k2, list(v2))  -&gt; list(v2)
</code></pre>
<p>举例来说，输入的 <code>Keys</code> 跟 <code>Values</code> 是被描绘成跟输出的的 <code>Keys</code> 跟 <code>Values</code> 不同的领域，并且，中间态的 <code>Keys</code> 跟 <code>Values</code> 跟输出的 <code>Keys</code> 跟 <code>Values</code> 则是相同领域的。</p>
<p>我们的 <code>C++</code> 实现传递了 <code>String</code> 给用户实现的函数，并且将类型转换等工作交给了用户自己去实现。</p>
<a class="header" href="print.html#a23-更多示例-more-examples" id="a23-更多示例-more-examples"><h3>2.3 更多示例 More Examples</h3></a>
<p>下面这些有趣的小程序可以轻易地用 <code>MapReduce</code> 的计算来表示。</p>
<p><strong>Distributed Grep</strong>: <code>Map</code> 函数提交每一个符合指定模式的行，而 <code>Reduce</code> 函数则只是简单的将中间态的数据作为最终输出。</p>
<p><strong>Count of URL Access Frequency</strong>: <code>Map</code> 函数处理页面的请求日志，然后输出如 (URL, 1) 。<code>Reduce</code> 函数则将输入的所有相同 URL 的 <code>Values</code> 统计起来，然后输入 （URL, total count) 的键值对。</p>
<p><strong>Reverse Web-Link Graph</strong>: <code>Map</code> 函数输出由 <code>Target</code>, <code>Source</code> 组成的键值对，表示每个在 <code>Source</code> 页面找到的链接 <code>Target</code>。<code>Reduce</code> 函数连接每个 <code>Target</code> 对应的 <code>Source</code> 到列表中，从而得到所有链接到 <code>Target</code> 的 <code>Source</code> 页面，并用如下形式表示 <code>(Target, list(Source))</code>。</p>
<p><strong>Term-Vector per Host</strong>: 词向量以 <code>(Word, Frequency)</code>键值对的形式总结了一篇文档或一系列文档中最重要的词语。<code>Map</code> 为每篇输入的文档函数提交 <code>(Hostname, Term Vertor)</code>形式的键值对，其中的 <code>Host</code> 是从文档的 Url 解析得到的。<code>Reduce</code> 函数则以 <code>Host</code> 为键，将每篇文档的词向量叠加起来，并丢弃那些少见或价值不高的词，最终输出以 <code>(Hostname, Term Vector)</code> 形式的键值对。</p>
<p><strong>Inverted Index</strong>: <code>Map</code>函数解析文档信息，并提交一系列的 <code>(Word, Document ID)</code> 键值对。<code>Reduce</code> 函数接收所有指定词的键值对，根据文档的 <code>ID</code> 排序后，提交以 <code>Word</code> 为 <code>Key</code> 以 <code>Document ID</code> 列表为 <code>Value</code> 的键值对列表 <code>(Word, list(Document ID))</code> 集合。这个输出的键值对集合组成了一个简单的倒排索引，可以使用它来定位每个词所在的文档。</p>
<p><strong>Distributed Sort</strong>: <code>Map</code> 函数提前每条记录的 <code>Key</code>, 并以 <code>(Key, Record)</code> 的记录提交。<code>Reduce</code> 函数则原封不动的提交接收到的键值对。这个计算依赖于在 4.1 节讨论的分区机制及在 4.2 节提到的排序特性。</p>
<a class="header" href="print.html#a3-实现-implement" id="a3-实现-implement"><h2>3. 实现 Implement</h2></a>
<p>按照 <code>MapReduce</code> 的接口来提供各种不同的实现是可行的，但是否正确的选择则取决于当时的环境。举例来说，一种实现可能很适合小型的共享内存的机器，另一种可能适合 <code>NUMA</code> 多处理器的机器，还有一种适合大量的通过网络连接的机器集群。</p>
<p>在这一节将描述一种在 Google 内部用的非常广泛的实现，他所使用的环境是：以普通商用计算机通过以太网连接起来的庞大集群。在这个环境中：</p>
<ol>
<li>使用常见的 X86 双核，2-4 GB 内存且运行着 Linux 系统的机器</li>
<li>普通的商用网络硬件，常见的是 100 M/S 及 1 G/S 的级别，但是其平均的速率在这两者之间。</li>
<li>一个包含了上百或上千台机器的集群，因此机器的失效也是常见的。</li>
<li>每台机器上都安装了普通的 IDE 硬盘，内部开发的分布式文件系统则用来管理这些硬盘中的数据，这个文件系统使用 Replication 的方式为这些不安全的硬件来提供高可用性及可靠性。</li>
<li>用户提交任务到分布式系统，每个任务由一系列的 Task 组成，这个任务由调度器安排到集群中的一系列机器中运行。</li>
</ol>
<a class="header" href="print.html#a31-运行概览-execution-overview" id="a31-运行概览-execution-overview"><h3>3.1 运行概览 Execution Overview</h3></a>
<p><img src="./mapreduce_note.assets/image-20200419011527655.png" alt="image-20200419011527655" /></p>
<p><code>Map</code> 阶段自动的将输入数据分成 M 个分区，并发布到多台机器上，分块后的数据能够并行的在不同的机器上处理。<code>Reduce</code> 阶段则将使用中间态的 Key 集合按照分区函数分成 R 个实例（如使用 <code>hash(key) mod R</code> 的方式），分区数 R 及分区函数由用户提供。</p>
<p>附图 1 展示了我们实现的 <code>MapReduce</code> 版本的总体流程，当一个用户的程序调用了 <code>MapReduce</code> 函数，就会触发下列列出的一系列操作（下面操作的序号与附图中的序号是相符的）：</p>
<ol>
<li><code>MapRecude</code> 库首先将输入数据分成 M 份，每份数据在 16到 64 MB 之间，（这取决于用户所传递的参数信息），然后在集群的机器中启动该程序的多个副本。</li>
<li>其中称为 <code>Master</code> 的副本是特殊的，其他的 <code>Worker</code> 副本则是执行 <code>Master</code> 分配的任务。在这里将会有 M 跟 <code>Map</code> 任务及 R 个 <code>Reduce</code> 任务将会被分配。<code>Master</code> 为空闲的 <code>Worker</code> 分配 <code>Map</code> 或 <code>Reduce</code> 任务。</li>
<li>一个被分配到 <code>Map</code> 任务的 <code>Worker</code> 将读取与其任务相关的数据块，并将数据解析成对应的键值对，然后传递给用户定义的 <code>Map</code> 函数，然后由 <code>Map</code> 函数产生的键值对信息则被缓存在内存中。</li>
<li>这些缓存文件，将周期性的被写回磁盘，并按照分区函数被分为 R 份。这些被写回磁盘的文件路径信息将传递回 <code>Master</code> ，由它负责管理并将这些路径信息发送给负责  <code>Reduce</code> 任务的副本。</li>
<li>一个被分配到 <code>Reduce</code> 任务的副本会从 <code>Master</code> 获取到对应的路径信息，然后通过远程过程调用(RPC) 的方式从 <code>Map</code> 副本的机器中读取数据。当 <code>Reduce</code> 副本读取到完整的中间态数据后，他将按照中间态的 <code>Key</code>s  进行排序，然后得到数据按照 <code>Key</code> 进行分组的结果。排序这部分是必须的，因为通常情况下很多不同的 <code>Key</code> 会映射到同一个 <code>Reduce</code> 任务。当中间态的数据比内存更大时，将使用外部排序算法。</li>
<li><code>Reduce</code> 副本迭代已排序的中间态数据并预期每个分组的数据都有唯一的 <code>Key</code>，然后他将 <code>Key</code> 及其对应的数据传递给用户实现的 <code>Reduce</code> 函数。<code>Reduce</code> 函数输出的数据将通过附加的方式输出到文件。</li>
<li>当所有的 <code>Map</code> 及 <code>Reduce</code> 任务都完成了，<code>Master</code> 将唤醒用户的程序，这时 <code>MapReduce</code> 调用将从用户的代码返回。</li>
</ol>
<p>在任务成功执行之后，<code>MapReduce</code> 的输出被分成了 R 个文件（每个 <code>Recuce</code> 任务一个，名称规则则由用户提供），常见情况下，用户无需自己将这 R 个文件合并成一个，他们通常会被作为其他 <code>MapReduce</code> 任务输入文件，或是通过其他的能够处理多个文件分布式程序来使用它们。</p>
<a class="header" href="print.html#a32-master-data-structures" id="a32-master-data-structures"><h3>3.2 Master Data Structures</h3></a>
<p><code>Master</code> 副本维护着许多的数据结构，如每个 <code>Map</code> 及 <code>Task</code> 任务的状态(idle, in-progress, completed)，及每个 <code>Worker</code> 副本的标识。</p>
<p><code>Master</code> 还充当着在 <code>Map</code> 任务及 <code>Reduce</code> 任务中传递中间态文件信息的管道。因此 <code>Master</code> 需要维护 R 个，由 <code>Map</code> 任务输出的文件路径及尺寸的中间态文件的信息。这些文件的信息会在每个 <code>Map</code> 任务完成的时候更新。这些信息将会被增量的推送给那些 <code>in-progress</code> (处理中) 状态的 <code>Reduce</code> 任务。</p>
<a class="header" href="print.html#a33-容错-fault-tolerance" id="a33-容错-fault-tolerance"><h3>3.3 容错 Fault Tolerance</h3></a>
<p><code>MapReduce</code> 库就是为了帮助人们在成百上千的机器上处理大批量的数据，所以它必须能够优雅的处理机器的失效问题。</p>
<a class="header" href="print.html#worker-failure" id="worker-failure"><h4>Worker Failure</h4></a>
<p><code>Master</code> 会周期性的 <code>Ping</code> 每个 <code>Worker</code> 副本。如果在指定的时间内没有收到回复，<code>Master</code> 会将其设置为 <code>failed</code> 失效。当一个 <code>Worker</code> 副本完成他手上的 <code>Map</code> 任务时，会将其自身的状态设置回 <code>idle</code>，这样他就能继续接受调度去执行其他的任务了。类似的，如果一个 <code>Map</code> 或 <code>Reduce</code> 任务执行失败了，当前的 <code>Worker</code> 也会设置自身为 <code>idle</code> 状态以接受新的调度。</p>
<p>当一个 <code>Worker</code> 失效时，他的 <code>Map</code> 任务失败后会会重新调度运行，因为他原本的输出是在 <code>Worker</code> 本地的磁盘上，所以失败时那些输出已经无法再次访问了。一个 <code>Reduce</code> 任务则无需重新执行，因为他的输出是保存到全局的文件系统中的。</p>
<p>当一个 <code>Map</code> 任务首次执行在 <code>Worker</code> A 副本上，然后接着在 <code>Worker</code> B 副本上执行时（因为 A 失效了）， 所有负责 <code>Reduce</code> 任务的 <code>Worker</code> 副本会接收到他在 <code>Worker</code> B 上重新执行的通知，这样 <code>Reduce</code> 任务就不会尝试从 <code>Worker</code> A 中读取数据，而是会尝试从 <code>Worker</code> B 中读取。</p>
<p><code>MapReduce</code> 能够应付大批量的 <code>Worker</code> 节点失效的情形。比如当前正在处理一个 <code>MapReduce</code> 工作，集群的网络出问题导致 80 台机器在数分钟内无法连通。<code>MapReduce</code> 的 <code>Master</code> 通过重运行那些原本运行在失效节点上的任务，使进度能够继续推进，最终完成整个 <code>MapReduce</code> 工作，</p>
<a class="header" href="print.html#master-failure" id="master-failure"><h4>Master Failure</h4></a>
<p>可以通过让 <code>Master</code> 周期性的将我们上文中提到的数据结构保存为检查点。如果 <code>Master</code> 节点失效了，新的副本能够通过最新的检查点来恢复状态。然而，因为只有一个 <code>Master</code> 节点，它的失效是非常罕见的。因此我们现在的实现中会在 <code>Master</code> 节点失效时终端整个 <code>MapReduce</code> 的执行。客户端可以在需要的时候自己来检查并且重新尝试运行这个 <code>MapReduce</code> 工作。</p>
<a class="header" href="print.html#semantics-in-the-presence-of-failures" id="semantics-in-the-presence-of-failures"><h4>Semantics in the Presence of Failures</h4></a>
<p>当用户提供的 <code>Map</code> 及 <code>Reduce</code> 操作对于输入的参数是具有确定性的*（即函数的输出只跟输入的参数有关，类似纯函数）<em>，我们的分布式实现能够产生与程序在没有出错时顺序执行时同样的输出</em>（也就是分布式运行的结果能够保持跟顺序执行的结果一致）*。</p>
<p>我们依赖于对 <code>Map</code> 跟 <code>Reduce</code> 输出的原子性的提交来达到这个目标。每个执行中的任务都将它的输出写入到私有的临时文件中。 <code>Reduce</code> 任务产生一个这样的文件，<code>Map</code> 任务则产生 R 个这样的文件（为每个 <code>Reduce</code>产生一个）。当一个 <code>Map</code> 任务完成时，<code>Worker</code> 节点会发送消息通知 <code>Master</code> 节点，这个消息中就会包含这 R 个临时文件的名称。如果 <code>Master</code> 节点收到已通知过的任务完成消息，他会忽略这个消息。否则的话他就将这 R 个文件的名称信息保存到他自己的数据结构中。</p>
<p>当 <code>Reduce</code> 任务完成时，运行该任务的 <code>Worker</code> 节点会自动的将临时文件重命名为最终的数据文件。如果一个相同的任务同时在多个机器上执行了，会在最终的输出文件上产生多次重命名调用。我们依赖于底层文件系统的原子性的重命名操作来保证最终的输出文件只会被一个 <code>Reduce</code> 任务执行重命名一次。</p>
<p>大部分的 <code>Map</code> 跟 <code>Reduce</code> 操作是具有确定性的，所以我们提供了跟顺序执行具有相同的语义，因此这让开发人员能够很容易的来解释或定义程序的行为。当 <code>Map</code> 与/或 <code>Reduce</code> 是不具有确定性的，我们提供了稍弱一些但仍然合理的语义，对于非确定性的操作来说，一个 <code>Reduce</code> 任务  R~1~ 的输出跟按照顺序执行的输出是等价的。==然而，对于另一个不同的 <code>Reduce</code> 任务的R~2~ 可能会符合 R~2~ 使用不同的顺序执行的输出。??==</p>
<p>考虑有 <code>Map</code> 任务 M 跟 <code>Reduce</code> 任务 R~1~ 跟 R~2~ 。让 e(R~i~) 表示 R~i~ 的一次提交。这是会出现较弱的语义因为 e(R~1~) 可能会读到 M 的某次执行输出，而 e(R~2~) 可能会读到 M 的另外一次执行输出。</p>
<a class="header" href="print.html#a34-本地化-locality" id="a34-本地化-locality"><h3>3.4 本地化 Locality</h3></a>
<p>网络带宽在我们的计算环境中是相对缺乏的。我们会通过一个事实来尝试节约带宽，那就是我们的输入数据是使用 GFS 保存在集群的本地的磁盘上的。GFC 会将每个文件分割成 64MB 的块并在不同的机器上保存数个副本（通常是 3 个）。而<code>MapReduce</code> 的 <code>Master</code> 则根据每个输入文件每个块在集群中的分布信息尝试在拥有相关文件块的机器上调度 <code>Map</code> 任务。当没法实现的时候，则尝试在拥有相关文件块机器的邻近机器上调度 <code>Map</code> 任务（比如处于相同网络交换机的机器），在运行一个大型的 <code>MapReduce</code> 任务时，这是对于集群中的 <code>Worker</code> 节点来说是一个重要的优化，大部分的输入数据都能够在本地读取，从而无需耗费网络带宽。</p>
<a class="header" href="print.html#a35-任务粒度-task-granularity" id="a35-任务粒度-task-granularity"><h3>3.5 任务粒度 Task Granularity</h3></a>
<p>如前面所说的，我们将 <code>Map</code> 阶段分割为 M 个，将 <code>Reduce</code> 阶段分割为 R 个。理想情况下， M 跟 R 应该远大于 <code>Worker</code> 节点的数量。每个 <code>Worker</code> 节点处理多个任务能动态的提升负载均衡，也能够加快 <code>Worker</code> 失效时的恢复速度：大部分的 <code>Map</code> 任务能够分布到集群所有 <code>Worker</code> 节点中去完成。</p>
<p>这里对于 M 跟 R 实际的大小在我们的实现里有着实际的界限。<code>Master</code> 节点需要做 O(M+R) 次调度抉择，及在内存中保存 O(M * R) 个状态信息。（当然，这个常数因子对内存的消耗是非常小的，在 O(M * R) 中的每一个 Map/Reduce 任务的键值对只需消耗大约一个字节）</p>
<p>而且R 常常是由用户进行限制的，因为每个 Reduce 的最终输出都是一个独立的文件。实际上，我们趋向于根据让每个单独的任务接收 16MB 到 64MB 的输入的原则来定义 M（这样我们在上一届所提的本地化优化，就能发挥他的作用了），然后我们把 R 定义为 所期望 <code>Reduce</code> 在多少个 <code>Worker</code> 节点上运行的数目乘以一个较小的常数。在期望使用 2,000 个 <code>Worker</code> 个节点时，我们常常定义 M = 200,000 及 R = 5,000。</p>
<a class="header" href="print.html#a36-后备任务-backup-tasks" id="a36-后备任务-backup-tasks"><h4>3.6 后备任务 Backup Tasks</h4></a>
<p>掉队者(Straggler) 是一个常见的会放大整个 <code>MapReduce</code> 操作运行时长的因素：在整个计算过程中，有一些机器耗费了比寻常更长的时长去执行他的 <code>Map</code> 或 <code>Reduce</code> 任务。掉队者可能因为许多的原因出现。比如一个磁盘损坏了的机器会导致他的读取性能从 30 MB/s 下降到 1 MB/s。集群中的调度器或调度其他的任务在同一个机器上执行，造成 CPU，内存，磁盘 或网络带宽的竞争。一个我们最近遇到的问题是机器的初始化代码配置错误导致处理器缓存被禁用：在受影响的机器上的计算效率下降了超过一百倍。</p>
<p>我们使用了一个通用的机制来缓解掉队者问题。当一个 <code>MapReduce</code> 整体接近完成时，<code>Master</code> 节点会为那些还在运行中的任务启动后备任务。这些任务会在他的主任务或后备任务完成时被标记为完成。我们还对这个机制进行了些调整来保证他不会占用整个集群的太多计算资源（几个百分点）。最后我们发现它显著的减少了一些大型 <code>MapReduce</code> 任务的时间。比如我们将在 5.3 节介绍的 <code>Sort</code> 程序，在禁用该机制的情况下他多耗费了 44% 的时间。</p>
<a class="header" href="print.html#a4-精炼-refinements" id="a4-精炼-refinements"><h2>4. 精炼 Refinements</h2></a>
<p>尽管使用基础的 <code>Map</code> 跟 <code>Reduce</code> 函数已经能够满足大部分的需求，但我们发现有一些扩展支持是非常有用的，在这一节将介绍它们。</p>
<a class="header" href="print.html#a41-分区函数-patitioning-function" id="a41-分区函数-patitioning-function"><h3>4.1 分区函数 Patitioning Function</h3></a>
<p>用户的 <code>MapReduce</code> 任务提供了他们希望 <code>Reduce</code> 函数输出的文件数 R。数据通过分区函数对中间态数据的 Key 进行分区后传递到 <code>Reduce</code> 函数中。默认提供的分区函数使用了哈希的方式 <em>(如 <code>hash(key) mod R</code>)</em>，这使数据能够均衡分布的输出到各分区。在某些场景中，能够使用其他的分区函数可能会更好，比如最终输出的 <code>Key</code> 是 <code>URL</code>，我们希望同一个站点的链接最终能够集中在同一个输出文件中。为了支持类似这样的场景，我们允许用户提供一个自定义的 分区函数。比如使用 <code>hash(Hostname(urlkey)) mod R</code>  作为分区函数就能够保证同一个站点的数据能够输出到同一个文件中。</p>
<a class="header" href="print.html#a42-顺序保证-ordering-guarantees" id="a42-顺序保证-ordering-guarantees"><h3>4.2 顺序保证 Ordering Guarantees</h3></a>
<p>在给定的一个分区中，中间态的键值对数据能够确保以升序的方式进行处理。这个保证让每个分区生成一个有序的最终结果集变得更加简易。这在用户需要对最终输出文件进行随机访问或要求数据有序的情况下是非常有用的。</p>
<a class="header" href="print.html#a43-合并函数-combiner-function" id="a43-合并函数-combiner-function"><h3>4.3 合并函数 Combiner Function</h3></a>
<p>在某些情况下，一些由 <code>Map</code> 任务产生的中间态的 Key 存在着很明显的重复，且用户提供的 <code>Reduce</code> 函数是可交换且与其相关的。一个较好的例子就是在 2.1 节介绍的单词统计的例子。如果单词的出现的频率遵循 <code>Zipf</code> 分布，那每个 <code>Map</code> 任务都会产生成百上千的类似 <code>(the, 1)</code> 键值对的记录。这些记录将通过网络发送到 <code>Reduce</code> 函数来产生一个总值。我们允许用户提供一个可选的<code>合并</code>函数，在数据被通过网络发送出去之前进行合并。</p>
<p>这个 <code>合并</code> 函数在每个运行了 <code>Map</code> 任务的机器上执行。常见的情况是 <code>合并</code> 函数的实现跟 <code>Reduce</code> 函数是几乎一样的。他们之间的唯一区别是 <code>MapReduce</code> 库对他们输出结果的处理方式。<code>Reduce</code> 函数的输入结果被写到最终的输出文件中。<code>合并</code> 函数的输出结果则被写到中间态的文件中，然后被发送到 <code>Reduce</code> 任务中去。</p>
<p>部分的合并显著的提升了 <code>MapReduce</code> 操作的性能，附录中提供了使用 <code>合并</code>函数的例子。</p>
<a class="header" href="print.html#a44-输入与输出的类型-input-and-output-types" id="a44-输入与输出的类型-input-and-output-types"><h3>4.4 输入与输出的类型 Input and Output Types</h3></a>
<p><code>MapReduce</code> 库提供了读取不同类型的输入文件的支持，比如 <code>文本</code> 类型的输入将每行数据当成一个键值对：<code>Key</code> 是该文件的偏移量， <code>Value</code>则是每一行的内容。另一种常见的支持格式类型则是按照 Key 排序的一系列的键值对。每种输入类型的实现知道如何按区间切分数据为一系列有意义的 <code>Map</code> 任务（如文本模式则以行为区间来分割数据）。用户可以通过实现 <code>reader</code> 接口来为一些新的输入类型增加支持，尽管大部分的用户都只是使用少数的几个预定义输入类型。</p>
<p>一个 <code>reader</code> 的实现不一定非得从文件中读取数据，也可以很容易的实现一个从数据库中读取记录或从内存中的某些数据结构读取记录的 <code>reader</code>。</p>
<p>以同样的方式，我们提供了一系列的输出类型支持用于产生不同的数据，并且用户也能够轻易的添加一种新的输出类型。</p>
<a class="header" href="print.html#a45-副作用-side-effects" id="a45-副作用-side-effects"><h3>4.5 副作用 Side-effects</h3></a>
<p>有些情况下，<code>MapReduce</code> 的用户发现输出一些辅助文件便于他们后续的 <code>Map</code> 或 <code>Reduce</code> 操作。我们依赖于应用的 <code>Writer</code> 去保证这些副作用的操作具有原子性及幂等性。如 <code>Writer</code> 将输出写到临时文件并在完成整个输出后，原子性的重命名这个临时文件到最终文件来。</p>
<p>我们并不提供这种在单个任务中产生多个输出文件的类似原子二阶段提交的支持。因此对于这种会产生多个输出文件且文件间有一致性要求的任务，都必须具有确定性。这个限制在实践中还没产生过什么问题。</p>
<a class="header" href="print.html#a46-忽略错误记录-skipping-bad-records" id="a46-忽略错误记录-skipping-bad-records"><h3>4.6 忽略错误记录 Skipping Bad Records</h3></a>
<p>有时用户提供的 <code>Map</code> 或 <code>Reduce</code> 函数可能存在 Bug， 导致在处理某些记录时会让程序崩溃。这些 Bug 会导致程序无法完成。通常的做法是去修复这些 Bug，但有些时候这不一定可行：也许这个 Bug 来自于一些我们无法触及源码的第三方库。而且，忽略某些出错的记录也是一个可以接受的选择，比如在对一个大数据集进行统计分析。由此我们体统了一个可选的模式，用于在 <code>MapReduce</code> 库执行时检测那些必然会导致程序崩溃的记录，在遇到这些记录时选择忽略他们，从而让程序能够正常完成。</p>
<p>每个 <code>Worker</code> 进程都会使用信号来处理程序的段错误及总线错误信息。在执行 <code>Map</code> 或 <code>Reduce</code> 操作前，<code>MapRecude</code> 库会在全局变量中保存当前正在处理函数的序号。如果用户的代码触发了对应的信号，该进程会通过 UPD 发送一个包含了当前序号的消息给 <code>Master</code> 副本。当<code>Master</code>副本收到有节点因为这条记录失败的了超过一次，在下一次运行这一趟任务时，他将该条记录标识为应当忽略的信息一并发送给 <code>Map</code> 或 <code>Reduce</code> 任务。</p>
<a class="header" href="print.html#a47-本地执行-local-execution" id="a47-本地执行-local-execution"><h3>4.7 本地执行 Local Execution</h3></a>
<p>对 <code>Map</code> 或者 <code>Reduce</code> 函数进行调试可能会比较困难，因为他们实际运行在一个可能具有数千台机器的分布式系统上，而且具体运行在哪个机器还得依照 <code>Master</code> 副本的调度来决定。为了提高调试、性能测试及小规模扩展的测试，我们开发了 <code>MapReduce</code> 的另一个实现版本：他们只会在本地机器上顺序的运行每个任务。对任务流的控制则归还给了用户，可以把它当成一个普通的 <code>Map</code> 任务。用户在启动程序时只需要提供一个特殊的标识，这样他们就可以在 <code>MapReduce</code> 中进行他们常用的调试或测试了。</p>
<a class="header" href="print.html#a48-状态信息-status-information" id="a48-状态信息-status-information"><h3>4.8 状态信息 Status Information</h3></a>
<p><code>Master</code> 节点同时还运行着一个 HTTP 服务，用来提供一些用户可读的信息页面。这些信息页面提供了整个计算的进度信息，比如有哪些任务已经完成了，有哪些任务正在运行，输入了多少字节数，中间态的数据占了多少字节，输出了多少字节数，处理速度等等。这些页面还包含连接到每个任务的标准输出跟标准错误输出文件的链接。用户可以使用这些数据来推测任务大概会花费多少时间，需不需要为计算添加更多的资源。这些信息同样还能用来分析任务的性能在什么情况下会运行的比预估的慢。</p>
<p>还有，最顶层的状态信息页还展示了哪些 <code>Worker</code> 失效了，以及他们失效时正在运行哪些 <code>Map</code> 或 <code>Reduce</code> 任务。这些信息对于我们用来分析程序的 Bug 来说是非常有用的。</p>
<a class="header" href="print.html#a49-计数器-counters" id="a49-计数器-counters"><h3>4.9 计数器 Counters</h3></a>
<p><code>MapReduce</code> 库还提供了帮助我们用于统计一些事件信息的计数器支持。比如，用户的代码可能想要统计一共处理了多少个词，或索引了多少篇德文的文档。</p>
<p>用户代码通过创建一个命名的计数器对象来使用这个机制，并可以在用户实现的 <code>Map</code> 或 <code>Reduce</code> 函数中去递增这些计数器。看个例子：</p>
<pre><code class="language-cpp">Counter* uppercase;
uppercase = GetCounter(&quot;uppercase&quot;);

map(String name, String contents):
    for each word w in contents:
        if (IsCapitalized(w)):
            uppercase-&gt;Increment();
        EmitIntermediate(w, &quot;1&quot;);
</code></pre>
<p>这些运行在 <code>Worker</code> 节点上的计数器的值，会周期性的提交给 <code>Master</code> 节点（通过附加在对 <code>Master</code> Ping 检查的响应中）。<code>Master</code> 节点则统计这些来自成功运行的 <code>Map</code> 或 <code>Reduce</code> 任务计数器，并在最后计算完成时将统计的信息返回给用户。当前的计数器值同样会在 状态信息页面上展示，用于让人可以通过计数器的值来得到执行进度。在统计计数器的值时，<code>Master</code> 节点会检查计数器的值是否来自于重复执行的 <code>Map</code> 或 <code>Reduce</code> 任务，以防止重复统计（重复执行可能来自于备用任务机制或因为失败而重复执行的任务）。</p>
<p><code>MapReduce</code> 库自身也会维护一些计数器，比如输入的键值对数目以及输出的键值对数目。</p>
<p>用户会发现计数器机制对于 <code>MapReduce</code> 任务的行为检查是非常有用的。比如，对于某些 <code>MapReduce</code> 操作，用户的代码可能想确认确认输出的键值对数目跟输入的键值对数目是否相等，或是计算德文相关的文档在整体文档中的比例。</p>
<a class="header" href="print.html#a5-性能-performance" id="a5-性能-performance"><h2>5. 性能 Performance</h2></a>
<p>在这一节中我们使用两个跑在庞大集群的计算任务来验证 <code>MapReduce</code> 的性能。其中一个在大约 1TB 的数据中查询一个常见的模式。另一个则是对大约 1TB 的数据进行排序。</p>
<p>这两个是从真实程序中挑选出的两个典型的 <code>MapReduce</code> 程序 -- 其中一个对数据进行重组为另一种数据；另一个程序从一个大量的数据集中提取出少量我们感兴趣的数据。</p>
<a class="header" href="print.html#a51-集群配置-cluster-configuration" id="a51-集群配置-cluster-configuration"><h3>5.1 集群配置 Cluster Configuration</h3></a>
<p>所有的程序都将运行在这个由大约 1800 台机器组成的集群中。其中每一台机器使用的配置都是：2GHz Inter Xeon Processos with Hyper Threaading, 4GB 的内存， 两块 160GB 的 IDE 硬盘，千兆的网卡。这些机器在一个树状的二级路由网路中，路由的顶部带宽大概为 100-200 Gbps。并且，它们都处于同一个网络中，所以每个机器之间的 往返延时(RTT round-trip time) 都少于 1 毫秒。</p>
<p>这 4GB 的内存中，有 1-1.5GB 将被其他的集群中的任务所使用。这些将要执行的任务都被安排在周末的下午，这个时间段中大部分的计算机资源，如 CPU，硬盘跟网络带宽都比较闲余。</p>
<a class="header" href="print.html#a52-查找-grep" id="a52-查找-grep"><h3>5.2 查找 <code>Grep</code></h3></a>
<p>查找程序 <code>grep</code> 将会扫描 10^10^ 条 100Byte 大小的，符合三个指定特征模式的记录（大概有 92,337 条记录）。输入的文件分块尺寸为 64MB(M = 15000) 且最终输入的文件只有一个（R = 1）。</p>
<p>附图 2 展示了计算过程的时间曲线。Y 轴展示了某个时刻下对输入数据进行扫描的速率。速率则由于不断的将 <code>MapReduce</code> 计算分配给更多的机器而得到逐步的提高。在分配给了 1764 台机器的时候，整体的速率达到了 30GB 每秒。在 <code>Map</code> 任务完成后整体的速率则开始下降，并在 80 秒是趋近于零。最终整个任务从开始到完成花费了大约 150 秒。这其中包含了整个程序启动以及分发的开销所使用的 1 分钟，以及操作 GFS 去打开这 1000 个输入文件以及获取其相应的信息用来做本地化优化的开销。</p>
<a class="header" href="print.html#a53-排序-sort" id="a53-排序-sort"><h3>5.3 排序 Sort</h3></a>
<p>排序程序对 10^10^ 个 100 字节的记录进行排序。该程序以 TeraSort 的基准为模型。</p>
<p>该排序程序包含了不到 50 行的用户代码。包括了 3 行的 <code>Map</code> 函数，用来从输入的文本行中提取大小为 10 个字节的 Key, 及提价以该 Key 及文本行构成的键值对。我们使用了内建的 <code>Identity</code> 函数作为 <code>Reduce</code> 操作符。这个函数将中间态的键值对信息直接作为最终的输出。最终输出被写入到了一组双向复制的 GFS 文件中。（也就是最终输出的数据量为 2TB，因为输入为 1TB）</p>
<p>像之前提到的，输入的数据被分割成以 64MB 为块的文件（M = 15000）。然后将最终排好序的结果，以记录的 Key 作为分区函数的根据，输出到 4000 个文件中（R = 4000）。</p>
<p>我们在这个测试中的分区函数对数据的分布已经有了充足的认知。对于通常的排序程序，我们会增加一个预处理的 <code>MapReduce</code> 操作用来对数据的 Keys 的分布进行采样，并用采样的信息来决定最终输出的文件数。</p>
<p>附图3(a) 展示了排序程序正常执行结果。左上的图片展示了对输入数据的读取速率，这个速率在最高峰时达到了 13GB/s 并在所有的 <code>Map</code> 任务完成的 200 秒这个节点前已相当快的速度降下来。该读取的速率较之 查找 程序是慢的多的。这是因为排序的 <code>Map</code> 任务需要花费他们进一般的时间跟 I/O 带宽将数据写入到本地的中间态文件中。查找程序的中间态文件跟它比起来是几乎可忽略不计的。</p>
<p>中间的图片展示了数据由 <code>Map</code> 函数传递到 <code>Reduce</code> 函数的带宽速率。这个清洗的操作在第一个 <code>Map</code> 任务完成是就开始进行。图形中的第一个波峰来自于第一批 1700 个 <code>Reduce</code> 处理任务（整个 <code>MapReduce</code> 任务被分配给了 1700 台机器，每台机器最多只能运行一个 <code>Reduce</code> 任务。在接近 300 秒时第一批 <code>Reduce</code> 被完成，在第一批 <code>Reduce</code> 任务完成后会继续清洗剩余的 <code>Reduce</code> 任务。所有的清洗在 600 秒左右全部完成。</p>
<p>左下的图片则展示了 <code>Reduce</code> 任务将最终数据写入到磁盘的速率。我们发现在第一次清洗及第一次写入间是有延迟的 ，因为这时机器还在忙于对中间态的数据进行排序。写入持续维持在 2-4GB/s 一段时间，所有的写入则在 850 秒时完成。加上启动时的负载，整个任务花费了 891 秒，这个成绩跟当前最佳 1057 秒的 TeraSort 性能是比较接近的。</p>
<p>还有一些注意点：输入的速率高于清洗的速率及最终输出的速率是因为我们的本地化优化 -- 大部分的数据都是从本地磁盘读取，得以绕过相关的网络带宽限制。清洗的速率高于最终数据的速率是因为在输出步骤需要将已排序的数据写入两次文件系统（我们为最终的输出使用了两个副本用来保持高可靠性及可用性）。写两个副本是由底层使用的文件系统提供的高可用性的保障。如果底层的文件系统使用可靠的编码来替代使用副本的方式，也可以降低对网络带宽的消耗。</p>
<p><img src="./mapreduce_note.assets/image-20200505013319855.png" alt="image-20200505013319855" /></p>
<a class="header" href="print.html#a54-备用任务的效果-effect-of-backup-tasks" id="a54-备用任务的效果-effect-of-backup-tasks"><h3>5.4 备用任务的效果 Effect of Backup Tasks</h3></a>
<p>在附图 3(b) 中，我们展示了在排序程序中将备用任务机制禁用的结果。执行的整个流程跟附图 3(a) 非常接近，除了一点，在最后接近完成时，有非常长的拖尾任务，在 960 秒后，有 5 个 <code>Reduce</code> 任务未完成。然而这些掉队者在 300 秒后仍未完成。整个执行的耗时被延长到 1283 秒，比开启了备用任务机制时多花了 44% 的时间。</p>
<a class="header" href="print.html#a55-机器失效-machine-failures" id="a55-机器失效-machine-failures"><h3>5.5 机器失效 Machine Failures</h3></a>
<p>在附图 3(c) 中，我们展示了在排序程序过程中，在几分钟内杀掉了 1746 个 <code>Worker</code> 中的 200 个。集群中的调度器马上为我们在这些机器上又重启了新的 <code>Worker</code> 进程。（我们只是杀掉了这些进程，机器还是正常运行着的）。</p>
<p><code>Worker</code> 进程被杀导致了输入的速率出现了负数，因为那些之前已经完成的 <code>Map</code> 任务消失了，所以其相关的任务需要被重新来完成。这些需要重新运行的 <code>Map</code> 任务在相对快的时间内被执行。整个计算过程，包括启动时的负载，最终在 933 秒时完成。（只是在正常运行的情况下增加了 %5 的运行时间）</p>
<a class="header" href="print.html#a5-经验-experience" id="a5-经验-experience"><h2>5. 经验 Experience</h2></a>
<p>我们在在 2003 年的 2 月实现了第一版的 <code>MapReduce</code> 库，然后再 2003 年的 8 月做了一些重要的强化，其中包括了本地化优化，认读在多个 <code>Worker</code> 运行中的动态负载均衡等。从那时起， <code>MapReduce</code> 对我们所在工作中能处理问题的能力带来了非常多的惊喜。他被用在了谷歌中的多个领域中，包括：</p>
<ul>
<li>大规模的机器学习问题</li>
<li>Google News 跟 Froogle 产品中的集群问题</li>
<li>为常见查询提取数据用来生成报表</li>
<li>为一些新的实验跟产品提供提取网页信息的能力。（如从全局的网页中提取地理位置信息用于本地化查询）</li>
<li>大规模的图运算</li>
</ul>
<p><img src="./mapreduce_note.assets/image-20200505154947043.png" alt="image-20200505154947043" /></p>
<p>附图 4 展示了在我们的代码管理系统中 <code>MapReduce</code> 程序的增长曲线。从 2003 早期的 0 个到 2004 9 月的接近 900 个。<code>MapReduce</code> 得到了巨大的成功，因为他让我们可以在学习半个小时后就书写出简单的程序，并让这些程序高效的运行在有上千台机器上。大大的加速了具体开发和原型之间的循环。更重要的是，他让那些对分布式或者并行系统完全没经验的人们轻易的用上了我们的集群资源。</p>
<p>在最后，<code>MapReduce</code> 库记录分析了每个任务对计算资源的使用情况，在表格 1 中，我们统计了再 2004 年 8 月各个程序在使用的 <code>MpaReduce</code> 的情况。</p>
<p><img src="./mapreduce_note.assets/image-20200505154959382.png" alt="image-20200505154959382" /></p>
<a class="header" href="print.html#a61-大规模的索引-large-scale-indexing" id="a61-大规模的索引-large-scale-indexing"><h3>6.1 大规模的索引 Large-Scale Indexing</h3></a>
<p><code>MapReduce</code> 一个最重要的使用是用它来完全重写产品级的索引系统，用于构建一些供谷歌的检索服务使用的数据结构。这个索引系统接收大量的储存于 GFS 的文档集合，这些文档集合来自于我们的采集系统。这些文档的原始文本的尺寸超过了 20TB。这个索引的过程需要 5 到 10 个 <code>MapReduce</code> 操作。使用 <code>MapReduce</code> （相对于之前的基于网络分发的分布式索引系统）带来了下面几点优点：</p>
<ul>
<li>索引的代码更清晰、简洁跟易于理解，因为代码中那些错误处理、分布式及并行化的部分已经由 <code>MapReduce</code> 库处理了。比如其中一个步骤中的 3800 行 C++ 代码被使用 <code>MapReduce</code> 编写的 700 行代码代替了。</li>
<li><code>MapReduce</code> 库的性能已经足够好了，所以我们能将与概念上不相关的部分分隔开，而不是将他们混合在在一起来避免额外的数据传输。这让我们能够更容易的去更改索引的步骤。比如，在旧系统中一个更改可能需要花费数个月的时间，在新的实现中则只需要几天。</li>
<li>索引的处理也变得更加容易操作，因为大部分因为机器故障、机器缓慢及网络延迟等问题都被 <code>MapReduce</code> 库自动解决了。更进一步的是，可以简单的通过为索引程序添加机器来提升性能。</li>
</ul>
<a class="header" href="print.html#a7-相关工作-related-work" id="a7-相关工作-related-work"><h2>7. 相关工作 Related Work</h2></a>
<p>很多的系统都提供了一种加以限制的编程模型，并通过该模型得以自动化的将程序并行化。==For example, an associative func- tion can be computed over all prefixes of an N element array in log N time on N processors using parallel prefix computations==。<code>MapReduce</code> 可以视为我们根据实际的经验对这些模型进行了简化的精华版。更重要的，我们实现了在上千个处理器中运行的容错机制，作为对比，大部分并行处理系统的实现只支持较小的规模，而且会将一些细节上的问题，如处理机器出错等问题交给开发人员去处理。</p>
<p>大部分的同步编程方法跟 MPI(跨语言通讯) 机制提供了较高层次的抽象来让开发人员能够较易的实现并行的代码。这些系统跟 <code>MapReduce</code> 显著的不同点就在于它提供了一个受限的编程模型用来自动将用户的代码并行化以及提供对用户来说透明的容错机制。</p>
<p>我们的本地化优化的设计灵感来自一些技术，如 <code>active disks</code>， 那些计算会被下推到靠近存储磁盘的处理元素那，从而节省了大量的 I/O 操作或网络传输。我们将程序运行在那些普通的直接连接硬盘设备的商业机器上，而不是直接运行在磁盘控制处理器上，不过大致上他们是一样的。</p>
<p>我们的后备任务机制跟 <code>Charlotte System</code> 的 <code>eager scheduling</code>  很像。其中一个 <code>eager scheduling</code> 的一个缺点是，当一个由他调度的任务不断的运行失败时，会导致整个运算都失效。我们则通过只是跳过错误记录的方式来修复这个缺点。</p>
<p><code>MapReduce</code> 的排序机制跟 NOW-Sort 的操作非常像。源机器(运行 Map 任务的 Worker) 对数据进行分区及排序后发送给 R 个 Reduce Worker。每个 Reduce Worker  在本地对接收到的数据进行排序(如果内存允许则使用纯内存的方式)。当然 NOW-Sort 并没有那些让 <code>MapReduce</code> 中由用户定义的类似 Map 跟 Reduce 的函数，所以没法有很高的实用性。</p>
<p><code>River</code> 提供了一个分布式队列，供程序间的信息传递。跟 <code>MapReduce</code> 类似， <code>River</code> 尝试提供一个平均性能优秀的方式去兼容多样的硬件或系统。<code>River</code> 通过小心的调度 磁盘 及 网络的传输来达到这个目标。<code>MapReduce</code> 使用了另一种方式，通过对编程模型加以限制，<code>MapReduce</code> 的 <code>Framework</code> 得以将任务拆分为大量的小粒度的任务。这些任务会被动态的调度在各个 <code>Worker</code> 中，所以越快速的机器能够处理约多的任务。这个受限的编程模型允许我们在程序的最后阶段通过冗余执行的方式来降低那些性能在平均性能以为的机器的计算时间。（比如机器较慢或被卡住的机器）</p>
<p><code>BAD-FS</code> 提供了跟 <code>MapReduce</code> 差异非常大的编程模型，不像 <code>MapReduce</code>，他将任务执行在跨广域网的机器上，这里有两个基本的相似点：(1) 两个系统都使用冗余执行来恢复失效的任务中丢失的数据。(2) 两个系统都使用了本地化调度来减少数据在网络中的传输。</p>
<p><code>TACC</code> 是一个被设计来构建高可用网络服务的系统。类似 <code>MapReduce</code>，他使用了重复执行的的机制来实现容错。</p>
<a class="header" href="print.html#a8-总结-conclusions" id="a8-总结-conclusions"><h2>8. 总结 Conclusions</h2></a>
<p><code>MapReduce</code> 的编程模型已经成功的在 Google 的各个不同场景中得到应用。我们将它的成功归结于几个原因。第一，就算是对于没有并行跟分布式系统经验的程序员来说这个编程模型也是易于使用的，因为他将并行的细节、错误处理、本地化优化跟负载均衡都隐藏在库的背后。第二，大型复杂的问题也很容易用 <code>MapReduce</code> 的模型来表达。比如用 <code>MapReduce</code> 来生成 Google 的检索服务所需使用的数据结构，排序数据，数据挖掘，机器学习以及其他各种系统等。第三，我们实现的 <code>MapReduce</code> 能够在由上千台机器组成的集群规模中运行。这个实现能够高效的使用这些机器资源，从而可以处理各种在 Google 中所需的大型计算问题。</p>
<p>我们还在这个实现中学习到了许多的东西。第一，加以限制的编程模型让并行化跟分布式计算以及容错更加易于实现。第二，网络带宽是稀缺资源，一些优化措施让我们能够显著的减少在网络中传输的数据：本地化优化让我们尽量从本地磁盘读取输入数据，及将中间态数据写入到本地磁盘，以实现节省带宽。第三，裁减的过慢的任务能用来减少一些过慢的机器及机器故障跟数据丢失的带来的问题。</p>
<p>[TOC]</p>
<a class="header" href="print.html#a对易于理解的一致性算法的研究扩展版" id="a对易于理解的一致性算法的研究扩展版"><h1>对易于理解的一致性算法的研究(扩展版)</h1></a>
<a class="header" href="print.html#a0-摘要-abstract" id="a0-摘要-abstract"><h2>0. 摘要 Abstract</h2></a>
<p>Raft 是一个用来管理 复制日志 的一致性算法。他产出的结果与 (Multi-)Paxos 是一样的，同时也保持跟 Paxos 一样高效，但他们是基于不同的数据结构的。不一样的数据结构让 Raft 更易于理解且更易于构建实际的系统。为了提高礼节性，Raft 将一致性切分为几个不同的元素，比如 Leader Election (领导者选举)，Log Replication (日志复制) 跟 Safety (安全性)，他还使用了更高级别的内聚来减少需要考虑的状态数量。从一些用户的学习结果观察得出 Raft 相较于 Paxos 更易于学生学习。Raft 同时还包含了一个新的机制来处理集群成员的变更，这是一个通过覆盖大多数节点(Overlapping Majorities) 来保证安全的机制。</p>
<a class="header" href="print.html#a1-介绍-introduction-1" id="a1-介绍-introduction-1"><h2>1. 介绍 Introduction</h2></a>
<p>一致性算法让一组机器在部分成员失效时仍能够保持整体正常运行。正因如此，他扮演者一个用来构建可靠的大规模软件系统的关键角色。Paxos 在上一个十年一直主导着关于一致性算法的讨论：大多数的一致性算法都是基于 Paxos 或是受其影响的，并且 Paxos 也成为了一致性知识教学的主导工具。</p>
<p>遗憾的是，尽管已经有大量的努力，Paxos 还是是太难以理解，并且他本身的结构也需要经过大量的调整才能应用到现实的系统中。这些让系统的构建者及学生都感到头痛。</p>
<p>在亲身经历了 Paxos 的折磨后，我们开始寻找一种能够在支持系统构建及教学上都能表现得更好的一致性算法。我们的目标不同于其他的一致性算法点在于，我们的首要目标是易于理解：我们能否定义一种相对于 Paxos 来说，更适合用来构建现实系统及教导学生的一致性算法？并且，我们希望这个算法对于系统开发者来说，仅凭直觉就能够明白。相较于让算法能够正常执行，更重要的是能够理解他为什么能够执行。</p>
<p>最终我们的工作产出了这个称为 Raft 的一致性算法。在设计该算法的过程中我们应用了许多技术来让他易于理解，如对其进行分解（Raft 可以分为 Leader Election (领导者选举), Log Replication (日志复制) 跟 Safey (安全性) 及减少状态空间 (相对于 Paxos, Raft 减少了不确定性及服务器间状态不一致的可能性)。在两个大学的具有 43 个学生课程中，同时学习了 Paxos 跟 Rust 之后，33 个学生可以较好的回答 关于 Raft 的问题。</p>
<p>Raft 类似于一些其他现存的一致性算法 (如 Oki 及 Liskov's Viewstamped) ，但 Raft 有几个新的特性：</p>
<ul>
<li>Strong leader (强领导者): Raft 使用了强领导者特性，举例来说，日志条目 (Log Entries) 只会从 Leader 流向其他的服务。这简化了日志复制的管理，也让 Raft 更容易理解</li>
<li>Leader Election (领导者选举): Raft 在选举中使用了随机计时器。这仅仅是在所有一致性算法都需要实现的心跳机制上增加了一点变化，但能够使得在冲突的解决跟修复更加快速跟简单</li>
<li>Membership Changes (成员变更): Raft 提供的集群成员变更的机制称为 Joint Consensus (共同共识), 他允许转换配置的过程中出现配置重叠的服务。这让服务集群在变更配置时让能够提供正常服务。</li>
</ul>
<p>我们相信 Raft 对于 Paxos 跟其他的一致性算法都是上好的，不管是以教学为目的还是以系统构建为目的。他比起其他算法来说更简单跟易于理解；他提供完整的描述给其他那些需要构建系统的人；他也有很多开源的实现；他的安全属性已经被证明了；他的运行效率相较于其他算法也是很好的。</p>
<p>在本篇论文的其他部分会介绍 复制状态机问题 (Replicated state machine problem)(第二节)；还介绍了 Paxos 的优劣 (第三节)；描述了该算法的可理解性（第四节), 完整的 Raft 一支性算法(第 5-8 节)； 运行 Raft (9 节); 以及一些其他的相关工作 (Section 10).</p>
<a class="header" href="print.html#a2-复制状态机-replicated-state-machines" id="a2-复制状态机-replicated-state-machines"><h2>2. 复制状态机 Replicated State Machines</h2></a>
<p>一致性算法是在复制状态机的背景中提出的。在这个背景中，一组服务器中的状态机计算出相同状态的副本，并以此得到了在某些服务器宕机情况下，依旧能保持正常服务的能力。复制状态机用来解决在分布式系统中的各种容错问题，比如一个大规模的系统的计算机集群中有只有一个 领导者(Leader)，就像 GFS、HDFS 跟 RAMCloud，使用了复制状态机来管理 领导者 的选举及存储配置信息来应对领导者的崩溃。使用复制状态机的例子还包括了 Chubby 跟 ZooKeeper。</p>
<blockquote>
<p>Figure 1: 复制状态机架构。</p>
<p>一致性算法管理着复制日志(Replicated Log), 日志中包含记录了来自客户端请求的状态机命令。状态机顺序的执行日志中的命令，所以每个状态机都能得到同样的输出。</p>
<p><img src="./Raft Note.assets/image-20200608131913561.png" style="width: 60%" ></p>
</blockquote>
<p>复制状态机典型的实现是使用如 <em>Figure 1</em> 中展示的 复制日志 (Replicated Log) 方式。每台服务器的日志中都包含了一系列将被他的状态机顺序执行的命令。所有的日志都以一样的顺序保存了相同的命令，所以所有的状态机会以相同的顺序去执行这一系列命令。因为每台状态机都是不可逆的，所以每一台相同状态的状态机按相同的顺序执行命令后会有相同的输出。</p>
<p>保持复制日志一致性是一致性算法的工作，服务器上的一致性模块从客户端接收命令并将其添加到自己的日志中。它和其他服务器上的一一致性模块通过通信来确保，就算有部分服务器失效，每个日志条目最终也会以相同的顺序保存。在命令被正确的复制后每台服务器会按照日志的顺序来处理他们，并且会将其输出返回给客户端。最终呈现出来的是，这些服务器看起来就像是一台可靠的状态机。</p>
<p>在实际系统中的一致性算法一般都会具有以下属性：</p>
<ul>
<li>确保 <strong>安全性</strong> (Safety) (绝不会返回一个错误的结果)，包括了所有 <em>非-拜占庭</em> 的情况。如网络延迟，分区，丢包，重复发送以及乱序等情况。</li>
<li>确保 <strong>高可用性</strong>，只要大多数的服务器是可用的且能够跟客户端以及彼此之间连通。因此，在一个由 5 台服务器组成的集群中能容忍其中的任意 2 台失效。假设这些服务器的失效是因为故障停机；他们也可以从已经持久化的数据中恢复正常并重新加入集群。</li>
<li>他们不会依赖于时序来保证日志的一致性：错误的时钟跟极端严重的消息延迟会才会导致集群不可用。</li>
<li>正常情况下，日志中的指令能够在大多数服务器响应后。在一轮 RPC 中被完成；少量的缓慢的服务器必须不会对整个系统的性能造成影响。</li>
</ul>
<a class="header" href="print.html#a3-paxos-的不足-whats-wrong-with-paxos-" id="a3-paxos-的不足-whats-wrong-with-paxos-"><h2>3. Paxos 的不足 What's Wrong With Paxos ?</h2></a>
<p>在过去的 10 年里，Leslie Lamport 的 Paxos 协议几乎就代表着一致性: 他是在教学中最常见的算法，同时也是许多其他一致性算法实现时的起点。</p>
<p>==TODO== 略过对 Paxos 的评论</p>
<a class="header" href="print.html#a4-可理解的设计-designing-for-understandability" id="a4-可理解的设计-designing-for-understandability"><h2>4. 可理解的设计 Designing For Understandability</h2></a>
<p>我们在设计 Raft 时有好些目标：它必须为系统构建提供完整且实际的基础，这让开发人员能够显著的减少设计工作；它必须在任何条件下都是安全的，对于常规操作条件下它都必须都是可用的；对于常规的操作它必须是高效的；但我们最大的目标，并且最困难的挑战是 -- 它必须是易于理解的。需要让大部分的读者都能较轻松的理解这个算法。此外，必须让开发人员对这个算法有直观的理解，这样在开发人员才能在具体的实现中对其进行扩展。</p>
<p>在设计 Raft 的过程中，我们做了许多设计上的取舍来达到现在的目标。在面对这些条件时，如何以可理解性作为基础条件来评估：解释这些评估有多难（例如，它的状态控件有多复杂，他们之间是否有微妙的区别），以及如何容易的让读者来完整的理解这些目标以及那些微妙的变化。</p>
<p>我们发现对可理解性的分析其实带了很大的主观性；尽管如此，我们使用了两种比较适当且通用的技术，第一种是众所周知的，称为问题分解：我们尽可能的将问题分解为许多能够独立解决、说明跟理解的小问题。比如将 Raft 分解为 Leader Election、Log Replication、Safty 跟 Membership Changes。</p>
<p>我们的第二个目标是通过减少需要考虑的状态类型来简化状态空间，以尽可能的来保持算法的清晰度及减少不确定性。具体点说，日志是不允许有空洞的（也就是必须是连续的），并且 Raft 限制了那些会导致日志不一致的可能。尽管在大多数情况下我们倾向于减少不确定性，但在某些特定的情形中不确定性却能够提高可理解性。尤其是，随机化会提高不确定性，但通过对各种可能的选择做类似的操作，能够减少状态空间处理的复杂度。我们使用了随机化来简化 Raft 的 Leader Election 算法。</p>
<a class="header" href="print.html#a5-raft-的一致性算法-the-raft-consensus-algorithm" id="a5-raft-的一致性算法-the-raft-consensus-algorithm"><h2>5. Raft 的一致性算法 The Raft Consensus Algorithm</h2></a>
<p>正如 Section 2 所介绍的，Raft 是一个用来管理复制日志的算法。Figure 2 总结了该算法以供备查，Figure 3 则列出了该算法的关键属性；Figure 中所列出的各种元素将会在后续的章节中讨论。</p>
<p>Raft 一致性的实现通过下列的能力能提供，首先他会选举出权威的 Leader, 并赋予该 Leader 绝对的权力来管理日志的复制。Leader 从客户中接收日志条目，将它们复制到其他的服务中，并在安全的时候，通知其他的服务将这些日志应用到他们的状态机中。通过 Leader 我们简化了复制日志的管理，举例来说，Leader 不需要跟其他的服务进行商议即可决定日志的存放位置，数据的流向也被简化成从 Leader 流向其他的服务。Leader 失效或者断线时，将在集群中选举出新的 Leader。</p>
<p>通过选举出 Leader，Raft 将一致性问题分解为三个相对独立的子问题，这些问题都将在接下来的章节中进行讨论：</p>
<ul>
<li><strong>Leader Election</strong>: 在现有的 Leader 失效时，必须选举出新的 Leader (Section 5.2).</li>
<li><strong>Log Replication</strong>: Raft 的 Leader 需要从客户端中接收新的日志条目并将他们复制到集群中，并强制其他的日志保持跟它一致</li>
<li><strong>Safety</strong>: Raft 中的 Safety 属性是指在 FIgure 3 中所说的：如果如果一个服务应用了一条日志到他的状态机中，其他的服务必然不会在相同的日志位置中应用其他不同的条目。Section 5.4 介绍了 Raft 是如何保障这个属性的，保障这个属性需要选举机制添加附加的限制，这些内容将在 Section 5.2 中介绍。</li>
</ul>
<p>介绍完该一致性算法后，本章将继续讨论关于系统中关于可用性的问题。</p>
<a class="header" href="print.html#a50-figure" id="a50-figure"><h3>5.0 Figure</h3></a>
<a class="header" href="print.html#figure2---state-and-rpc" id="figure2---state-and-rpc"><h4>Figure2 - State And RPC</h4></a>
<ul>
<li>
<p><strong>State</strong></p>
<p><strong>持久化在所有服务中的状态信息</strong> <em>(在处理 RPC 请求前从稳定的存储器中恢复)</em></p>
<ul>
<li><strong>currentTerm</strong> 该服务所看到的最新的 Term <em>(以 0 进行初始化，并单调的递增)</em></li>
<li><strong>votedFor</strong> 在当前 Term 中收到的 CandidateId(候选人 ID)</li>
<li><strong>log[]</strong> 日志条目数组，每个条目都存储了要应用到状态机的命令以及 Leader 接收到该条目时的 Term</li>
</ul>
<p><strong>所有服务都有的动态状态</strong></p>
<ul>
<li><strong>commitIndex</strong> 需要被提交的最新日志条目所在的索引值 <em>(以 0 进行初始化，并单调的递增)</em></li>
<li><strong>lastApplied</strong> 已被提交到状态机的最新日志条目的索引值 <em>(以 0 进行初始化，并单调的递增)</em></li>
</ul>
<p><strong>Leader 的动态状态</strong> <em>(在完成选举后进行重置)</em></p>
<ul>
<li><strong>nextIndex[]</strong> 记录了每个服务下一条需要发送的日志条目索引 <em>(以 Leader 最新日志条目的索引值 +1 初始化)</em></li>
<li><strong>matchIndex[]</strong> 记录了每个服务已复制的最新条目索引</li>
</ul>
</li>
<li>
<p><strong>AppendEntries RPC</strong></p>
<p>Leader 发起的用于复制日志条目给其他服务的请求，也会用做心跳</p>
<p><strong>参数</strong></p>
<ul>
<li><strong>term</strong> Leader 的 Term <em>(任期)</em></li>
<li><strong>leaderId</strong> Follower 可用来转发客户端的请求</li>
<li><strong>prevLogIndex</strong> 新日志条目之前的日志条目的索引</li>
<li><strong>prevLogTerm</strong> 新日志条目之前的日志条目的 Term</li>
<li><strong>entries[]</strong> 服务需要保存的新日志条目 <em>(当它为空时表示心跳请求)</em>，也可能同时发送多条日志来提高效率</li>
<li><strong>leaderCommit</strong> Leader 已提交的日志条目索引</li>
</ul>
<p><strong>返回值</strong></p>
<ul>
<li><strong>term</strong> 当前 Term，用于 Leader 更新自己的 Term</li>
<li><strong>success</strong> 当 Follower 的日志条目记录中的索引信息跟 <strong>prevLogIndex</strong> 及 <strong>prevLogTerm</strong> 匹配时返回 <code>true</code></li>
</ul>
<p><strong>接收者的实现</strong></p>
<ol>
<li>当 <strong>term</strong> &lt; <strong>currentTerm</strong> 时返回 <code>false</code></li>
<li>当服务的日志条目中 <strong>prevLogIndex</strong> 对应的日志条目不为 <strong>prevLogTerm</strong> 时返回 <code>false</code></li>
<li>如果已存在的日志条目跟 <strong>entries</strong> 中有冲突，删除已存在及其之后的日志</li>
<li>将新的日志条目添加在日志中</li>
<li>如果 <strong>leaderCommit</strong> &gt; <strong>commitIndex</strong>， 设 <strong>commitIndex</strong> 设为 <strong>leaderCommit</strong> 或新日志条目中的较小值</li>
</ol>
</li>
<li>
<p><strong>RequestVote RPC</strong></p>
<p>Candidates 通过发起该请求来获取选票</p>
<p><strong>参数</strong></p>
<ul>
<li><strong>term</strong> Candidate 的 Term</li>
<li><strong>candidateId</strong> 表示请求选票的 Candidate</li>
<li><strong>lastLogIndex</strong> Candidate 最新日志条目的索引</li>
<li><strong>lastLogTerm</strong> Candidate 最新日志条目的 Term</li>
</ul>
<p><strong>返回值</strong></p>
<ul>
<li><strong>term</strong> currentTerm，供 Candidate 更新自己的 Term</li>
<li><strong>voteGranted</strong> 当 Candidate 得到该服务的选票时为 <code>true</code></li>
</ul>
<p><strong>接收者的实现</strong></p>
<ol>
<li>当 <strong>term</strong> &lt; <strong>currentTerm</strong> 时返回 <code>false</code></li>
<li>如果 <strong>voteFor</strong> 为空或为 <strong>candidateId</strong>， 并且 Candidate 的日志信息跟自己的日志一样新则给予选票</li>
</ol>
</li>
<li>
<p><strong>Rules for Servers</strong> <em>(服务需遵守的规则)</em></p>
<p><strong>所有服务</strong></p>
<ul>
<li>如果 <strong>commitIndex</strong> &gt; <strong>lastApplied</strong>，递增 <strong>lastApplied</strong> 然后将索引为 <strong>lastApplied</strong> 的日志应用到状态机</li>
<li>如果 <em>RPC</em> 请求会返回结果的 <strong>Term</strong> &gt; <strong>currentTerm</strong>，将 currentTerm 设为 <strong>Term</strong>, 然后装换为 Follwer</li>
</ul>
<p><strong>Followers</strong></p>
<ul>
<li>响应 Candidate 跟 Leader 发送的 RPC 请求</li>
<li>如果在超时间隔内没有收到当前 Leader 发送的 <strong>AppendEntries</strong> RPC 请求或者为其他的 Candidate 投票则将自身转换为 Candidate</li>
</ul>
<p><strong>Candidates</strong></p>
<ul>
<li>在转换为 Candidate 时，触发选举
<ul>
<li>递增自身的 <strong>currentTerm</strong></li>
<li>为自己投票</li>
<li>重置选举的定时器</li>
<li>发送 <strong>RequestVote</strong> 请求给其他的服务</li>
</ul>
</li>
<li>如果选票被大多数服务接受，则将自身转换为 Leader</li>
<li>如果收到了新 Leader 的 <strong>AppendEntries</strong> 请求，将自身转换为 Follower</li>
<li>如果本次选举超时了，则启动新的选举</li>
</ul>
<p><strong>Leaders</strong></p>
<ul>
<li>在获选后，发送空的 <strong>AppendEntries</strong> 请求 <em>(作为心跳)</em> 给各个服务，并且在指定的间隔中不断重复来防止自身的任期超时</li>
<li>收到了来自客户端的命令时，将其封装为日志条目并保存到本地的日志中，并在将日志应用到状态机后给予客户端响应</li>
<li>如果最新的日志索引大于等于 <strong>nextIndex</strong> 对应 Follower 的索引，发送 <strong>AppendEntries</strong> 请求给对应的 Follower，并在请求中包含大于其 <strong>nextIndex</strong> 索引的日志条目
<ul>
<li>如果响应成功，更新该 Follower 的 <strong>nextIndex</strong> 及 <strong>matchIndex</strong> 信息</li>
<li>如果因为日志条目不一直导致响应失败，降低 <strong>nextIndex</strong> 的值之后重试</li>
</ul>
</li>
<li>如果存在一个大于 <strong>commitIndex</strong> 的 <strong>N</strong>，大多数服务的 <strong>matchIndex[i] &gt; N</strong> 且 <strong>Log[N].term == currentTerm</strong>, 则将 <strong>commitIndex</strong> 设为 N</li>
</ul>
</li>
</ul>
<a class="header" href="print.html#figure3-raft-property" id="figure3-raft-property"><h4>Figure3 Raft Property</h4></a>
<ul>
<li>
<p><strong>Election Safety</strong></p>
<p>在一个 Term 中最多只会有一个 Leader 被选举出来</p>
</li>
<li>
<p><strong>Leader Append-Only</strong></p>
<p>一个 Leader 永远不会覆盖跟删除自身的日志，他只会附加新的日志条目</p>
</li>
<li>
<p><strong>Log Matching</strong></p>
<p>如果两个服务的日志在某个索引位置的条目具有相同的 Term，则他们之间的日志从起始位置到该索引位置位置都应该是相同的</p>
</li>
<li>
<p><strong>Leader Completeness</strong></p>
<p>如果一个日志条目在一个 Term 中被提交了，则该条目必然会在其他有更高数值 Term 的 Leader 的日志中</p>
</li>
<li>
<p><strong>State Machine Safety</strong></p>
<p>如果一个服务将一个日志应用到了日志的一个索引位置后，其他的服务不会在相同的位置应用不同的日志条目。</p>
</li>
</ul>
<a class="header" href="print.html#a51-raft-基础-raft-basics" id="a51-raft-基础-raft-basics"><h3>5.1 Raft 基础 Raft basics</h3></a>
<p>一个 Raft 集群包含多个服务器；一个典型的数值是五，他允许我们的系统能够容许两台服务器失效。在任意的时刻集群中的每台服务器都应当处于下面三种状态之一: <em>Leader</em>、<em>Follower</em>、<em>Candidate</em>。一个常见的场景是，其中一个处于 <em>Leader</em> 状态，其他的都是 <em>Follower</em>。<em>Follower</em> 是被动的：他们并不会处理客户端的请求，只会对来自 <em>Leader</em> 及 <em>Candidate</em> 的请求做出响应。来自客户端的请求全都会交给 <code>Leader</code> 进行处理（如果客户端将请求发送给了 <code>Follower</code>， 则 <code>Follower</code> 会将请求转达给 <code>Leader</code>）。第三个 <code>Condidate</code> 状态是用来进行选举新 <em>Leader</em> 的，我们将在 5.2 节中进行介绍。Fiture 4 给我们展示了每个状态之间的转换，具体的转换细节，我们将在下面进行讨论。</p>
<blockquote>
<p><strong>Figure 4</strong></p>
<p>服务的状态，<em>Followers</em> 只会响应其他服务的请求。如果 <em>Follower</em> 没有收到请求，他会将自己转换为 <em>Candidate</em> 然后触发一次选举。当一个 <em>Candidate</em> 收到大多数服务器的选票时会转换为 <em>Leader</em>，<em>Leader</em> 则保持自己的状态，直到自己失效。</p>
<p><img src="./Raft Note.assets/image-20200905154253092.png" style="width: 80%"></p>
</blockquote>
<p>Raft 将时间严格的使用 Term 进行了切分，如 Figure 5。 Terms 是有序且连续的数值。如 5.2 节将介绍的，每次有一个或多个 <em>Candidate</em> 尝试转换为 <code>Leader</code> 时，都会产生一个新的 Term。如果一个 <em>Candidate</em> 赢得了某次选举，那他在这个 Term 期间都将作为集群的 <em>Leader</em>。在某些条件下，选举可能会产生选票瓜分的情况，这时将不会产生新的 <em>Leader</em>，因此一次新的选举将带着一个新的 Term 在短时间内重新举行。</p>
<blockquote>
<p><strong>Figure 5</strong></p>
<p>将时间切分为 Term，每个 Term 都是从一次选举中产生的。每次成功选举之后，一个 Leader 在他的 Term 期间会负责管理整个集群。当在一个 Term 期间没有选举出新的 Leader 时，则该次选举是失败的。每个服务观察到 Term 的转换时间可能都是不同的。</p>
<p><img src="./Raft Note.assets/image-20200905155025993.png" style="width: 70%" ></p>
</blockquote>
<p>不同的服务可能会在不同的时刻观察到 Term 的变化，在某些情况下一个服务可能没法观察到某次选举的过程，甚至是所有 Term 的变化。Terms 在 Raft 中扮演者逻辑时钟的角色，这为服务们提供了检查过期信息的能力，如判别过期的 Leader。每个服务都会保存一个严格递增的代表当前的 Term 的数值 <em>（current term number)</em>，我们将它命名为 <strong>CurrentTerm</strong>， CurrentTerm 会在服务之间的每次通讯中进行交换；如果一个服务的 <strong>CurrentTerm</strong> 比另外一个服务的小，则他会将自身的 <strong>CurrentTerm</strong> 转换为较大的那个。如果一个 <em>Candidate</em> 或者一个 <em>Leader</em> 发现自己的  <strong>CurrentTerm</strong> 已经过期了，他们会马上将自己转换为 <em>Follower</em>。如果一个服务接收到的请求是来自于一个过期的 Term，则他们会拒绝这个请求。</p>
<p>Raft 的服务之间通过远程过程调用 <em>(RPC)</em> 来进行通信，而且基础的一致性算法实现部分只需要两种类型的 RPC 调用，他们分别是 <em>Candidate</em>  用来发起一个选举的 <em>RequestVote</em> 调用，以及 <em>Leader</em> 用来复制日志及发送心跳的 <em>AppendEntries</em> 调用。在第 7 章时我们将会增加一种用来发送快照 <em>(snapshots)</em> 的调用。服务在没有收到调用的响应时会以定时的方式进行重试，并且每个服务都会以并行的方式来处理 RPC 请求以获得更好的性能。</p>
<a class="header" href="print.html#a52-leader-选举-leader-election" id="a52-leader-选举-leader-election"><h3>5.2 Leader 选举 Leader Election</h3></a>
<p>Raft 使用心跳的机制来触发 <em>Leader</em> 的选举。当服务启动时他们都处于 <em>Follower</em> 的状态，他们只要能一直收到 <em>Leader</em> 或者 <em>Candidate</em> 的合法的信息，就会一直保持 <em>Follower</em> 的状态。<em>Leader</em>  则通过周期性的发送心跳 <em>(不带日志信息的 AppendEntries 调用)</em> 给所有的 <em>Follower</em> 来保持自己的管理权。如果一个 <em>Follower</em> 在一个被称为 <em>Election Timeout</em> 的间隔中没有收到通信请求，他会假设这个时候已经没有可用的 <em>Leader</em> ，从而会发起一次新的选举来尝试成为 <em>Leader</em>。</p>
<p>要开始一轮选举，<em>Follower</em> 首先会递增自身的 <strong>currentTerm</strong> 并将自己转换为 <em>Candidate</em>；接着该服务会先把选票投给自己，然后以并行的方式给集群中的其他服务发送 <em>RequestVote</em> 请求。<em>Candidate</em> 会一直保持自己的状态，直到以下的三种情形之一发生：(a) 赢得选举， (b) 其他的服务宣布自己成为了 <em>Leader</em>，(c) 指定的间隔时间过去，但没有人成为新的 <em>Leader</em>。这些不同的结果我们在下面的段落分别介绍。</p>
<p>当一个 <em>Candidate</em> 在当前 Term 中得到了大多数服务的选票时便可赢得本次选举。每个服务在同一个 Term 中会按照先来先处理的原则投票给一个 <em>Candidate</em>，并且在同一个 Term 中只会投一次。<em>(5.4 节中会为选票添加多一个限制)</em>。<strong>大多数</strong> 这个规则保证了在一个 Term *最多只会有一个服务赢得选举 <em>(符合 Election Safety 属性)</em>。当一个 <em>Candidate</em> 赢得选举了，他会将状态转换为 <em>Leader</em>，接着发送心跳请求给所有的服务来确认自己的管理权，同时也为了防止出现新一轮的选举。</p>
<p><em>Candidate</em> 在等待选票的同时可能会收到来自其他服务的、表明自己成为 <em>Leader</em> 的 <em>AppendEntries</em> 请求。如果请求中 <em>Leader</em> 的 <em>Term</em> <em>(会在请求中包含)</em> 大于或等于本服务的 <strong>currentTerm</strong>，则 <em>Candidate</em> 能够确认该 <em>Leader</em> 是合法的，他会将自身从 <em>Candidate</em> 状态转换回 <em>Follower</em>。相反，如果请求中的 Term 小于 <em>Candidate</em> 自身的 <strong>currentTerm</strong>，则 <em>Candidate</em> 会拒绝该 RPC 请求，并继续保持 <em>Candidate</em> 状态。</p>
<p>第三种可能出现的结果是 <em>Candidate</em> 没有赢得或输了选举：如果有多个 <em>Follower</em> 同时成为 <em>Candidate</em>，选票有可能被瓜分导致没有 <em>Candidate</em> 能够获得大多数的选票。当发生这种状况时每个 <em>Candidate</em> 都会在到达超时时间后提升自己的 Term 并发送 <em>RequestVote</em> 开始新一轮的选举，如果没有其他的限制这个瓜分选票的情况可能会无限重复下去。</p>
<p>Raft 使用一个随机的选举超时时间来降低选票瓜分的可能性并让选举能快速完成。为了从一开始就防止选票瓜分，一开始触发选举的超时时间会在固定的区间中选择一个随机值 <em>(比如 150 - 300ms)</em>。这区别开了每个服务的超时时间，使得在大多数情况下在某个时刻只会有一个服务到达超时时间，这样他在赢得选举时就能够在其他服务超时前发送出心跳请求。相同的机制还用在了处理选票瓜分的状况，每个 <em>Candidate</em> 会在重试发起新一轮选举前选择一个随机的延迟，在延迟时间之后才触发新一轮的选举，这也减少了新一轮选举产生瓜分选票的可能性。在 9.3 章展示了该机制所达到的快速完成选举的效果。</p>
<p>选举是一个可理解性是如何指导我们进行设计选择的例子。在最开始，我们计划使用一种排名系统：每个 <em>Candidate</em> 会被赋予一个唯一的名次，这个名次将用来在竞争的 <em>Candidate</em> 中进行选择的依据。如果一个 <em>Candidate</em> 发现其他的 <em>Candidate</em> 的名次比自身高，他就会转换至 <em>Follower</em> 状态，这样的话名次较高的 <em>Candidate</em> 就能够比较容易的赢得下一次的选举。我们发现这个处理产生一个关于可用性的微妙问题 <em>(名次较低的服务在名次较高的服务失效时经过超时时间后会重新成为 Candidate, 但如果他处理的太快，可能会导致选举的过程被重置)</em>，我们对算法进行了多次的调整，但每次调整后都会产生新的边界状况。最终我们总结出随机的重试机制更加直观跟容易理解。</p>
<a class="header" href="print.html#a53-日志复制-log-replication" id="a53-日志复制-log-replication"><h3>5.3 日志复制 Log Replication</h3></a>
<p><em>Leader</em> 在被选举出来后才能够处理来自客户端的请求，每个请求都包含了需要被每个状态机副本执行的命令。<em>Leader</em> 将命令作为一条新的日志条目添加到日志中，然后并行的向其他服务发起 <strong>AppendEntries</strong> RPC请求，当条目被 <em>安全的复 制</em> 后 <em>(下面会说明说明是安全的复制)</em> ，<em>Leader</em> 将该条目中的命令应用到自身的状态机中，然后将该命令的执行结果返回给客户端。如果 <em>Follower</em> 崩溃或者很慢，又或者网络中的信息丢失了，<em>Leader</em> 会无限的发送出 <strong>AppendEntries</strong> 请求 <em>(就算在他已经将结果返回给客户端之后也是)</em> ，直到所有的 <em>Follower</em> 将所有的日志条目都存储到其本机的日志中。</p>
<p>日志被组织为 <em>Figure 6</em> 所<u>展示的那样。每个日志</u>条目都保存了状态机的命令以及接收该日志的 <em>Leader</em> 的 Term。该 Term 在日志中的作用是用于检测日志之间的不一致以及保证 <em>Figure 3</em> 中所列的属性。每个日志条目还包含了他在日志列表中的索引值。</p>
<blockquote>
<p><strong>Figure 6</strong></p>
<p>日志由带序号的顺序的日志条目组成。每个条目都包含了创建该条目的 Term 及要应用到状态机的命令。一个条目在能够安全的应用到状态机时，会被设置为已提交 <em>(committed)</em> 的状态。</p>
<p><img src="./Raft Note.assets/image-20200906232448282.png" style="width: 70%" ></p>
</blockquote>
<p><em>Leader</em> 决定何时能够安全的将日志条目应用到状态机中，这个被应用的日志条目被称为 <em>Commited</em> <em>已提交</em> 的。Raft 保证了已提交的日志是持久化的并且最终也会被所有其他服务应用到他们的状态机上。一个日志条目会在 <em>Leader</em> 将它复制到集群中的大多数服务之后提交 <em>(如 Figure 6 的 Entry 7)</em>。此时同时会把 <em>Leader</em> 中之前尚未提交的日志都处理为 <em>已提交</em>。 在 5.4 节会讨论 <em>Leader</em> 变更之后提交日志时的一些微妙之处，同时也说明了 Raft 提交日志的方式是安全的。 <em>Leader</em> 会记录他已提交的最高的日志索引位置，并在后续发送出的 <strong>AppendEntries</strong> 调用中包含该信息 <em>(包括心跳信息)</em>， 所以其他的服务也能够知道该提交状态。当 <em>Follower</em> 发现某些日志已被 <em>Leader</em> 提交后，他也会将对应的日志按顺序应用到自身的状态机中。</p>
<p>我们设计的 Raft 的日志机制为不同服务间的日志提供了高度一致性。这个机制不只用来简化了系统的行为及提供可预测性，同时也是保证安全性的重要元素。Raft 维持着下面的一些属性，这些属性提供组成了我们在 <em>Figure 3</em> 中列的 <em>Log Matching</em> 属性：</p>
<ul>
<li>如果两个日志中某个索引位置的日志条目具有相同的 Term，则他们也应当保存着相同的命令</li>
<li>如果两个日志中某个索引位置的日志条目具有相同的 Term，则该日志条目之前的日志条目应当都是相同的</li>
</ul>
<p>第一个属性由下面两点保证，1) <em>Leader</em> 在一个 Term 期间只会在同一个索引位置创建一个日志条目； 2) 日志条目绝对不会改变他在日志中的位置。第二个属性则由 <strong>AppendEntries</strong> 的一致性检查来保证。在发送一个 <strong>AppendEntries</strong> 请求时，<em>Leader</em> 会在其中带上新日志条目的上一个日志条目的索引位置及 Term 信息，如果 <em>Follower</em> 在日志指定的索引位置上找不到对应 Term 的日志条目，他将拒绝添加这个新的日志条目。这个一致性的检查首先保证初始化的日志状态要满足 <em>Log Matching</em> 属性，而一致性的检查保障了日志在扩展的时候仍能够保持该属性。最终的结果就是，只要 <strong>AppendEntries</strong> 返回了成功，<em>Leader</em> 就知道该 <em>Follower</em> 的日志直到最新添加的这个日志条目为止都是跟自己一致的。</p>
<p>执行常规的操作时，<em>Leader</em> 跟 <em>Follower</em> 的日志总是保持一致，所以 <strong>AppendEntries</strong> 的一致性检查是不会失败的。然而，<em>Leader</em> 的崩溃会导致日志不一致 <em>(原有的 Leader 可能还未同步他所有的日志条目给其他的服务)</em>。这个不一致的状态会导致一系列的 <em>Leader</em> 跟 <em>Follower</em> 的崩溃。<em>Figure 7</em> 说明了几种 <em>Follower</em> 跟新 <em>Leader</em> 的日志不一致的情况。<em>Follower</em> 可能会缺失 <em>Leader</em> 上已有的日志，他也可能比 <em>Leader</em> 多出了一些日志，或者两者同时存在。丢失及过多的日志条目间可能还会跨越多个 Term。</p>
<blockquote>
<p><strong>Figure 7</strong></p>
<p>当一个 <em>Leader</em> 得到他的管理权时，他可能会面临几种 <em>(a-f)</em> 处于不同状态的 <em>Follower</em> 。每个方框代表一个日志条目，其中的数字代表添加该日志时的 Term。<em>Follower</em> 可能会缺失某些日志条目 <em>(a-b)</em>，可能会多出一些未提交的日志条目 <em>(c-d)</em>，或者两者都有 <em>(e-f)</em>。</p>
<p>比如造成场景 <em>(f)</em> 的原因可以是在他成为 Term 2 的 <em>Leader</em> 时添加了一些日志，但在还没来得及提交之前就崩溃了；接着他马上重启又成为了 Term 3 的 <em>Leader</em>, 接着在 Term 3 期间又添加了一些日志，但是在 Term 2 跟 Term 3 的日志提交之前又崩溃，然后一直保持宕机状态。</p>
<p><img src="./Raft Note.assets/image-20200908210905429.png" style="width: 60%" ></p>
</blockquote>
<p>在 Raft 中，<em>Leader</em> 通过强制要求 <em>Follower</em> 的日志跟自身保持一致来处理不一致的状态。这意味着与 <em>Leader</em> 有冲突的 <em>Follower</em> 的日志会被强制重写为 与 <em>Leader</em> 一样。5.4 节会说明这种做法在添加多一个限制后是安全可靠的。</p>
<p>为了让 <em>Follower</em> 保持跟自身的一致性，<em>Leader</em> 需要找到他与 <em>Follower</em> 之间的最后一个一致的时间点，然后删除 <em>Follower</em> 在该时间点之后的所有日志。所有的这些操作由 <em>Follower</em> 在响应来自 <em>Leader</em> 的 <strong>AppendEntries</strong> 请求时发生。<em>Leader</em> 管理者每个 <em>Follower</em> 的 <strong>nextIndex</strong>，该字段代表了 <em>Leader</em> 应该发给该 <em>Follower</em> 的下一个日志条目的索引。当一个服务刚成为 <em>Leader</em> 时，他会将所有 <em>Follower</em> 的 <strong>nextIndex</strong> 初始化为自身日志中最新条目的索引 <em>(在 Figure 7 中体现为 11)</em>。如果一个 <em>Follower</em> 的日志跟 <em>Leader</em> 的并不一致，这时 <strong>AppendEntries</strong> 的一致性检查将在发出 <strong>AppendEntries</strong> 时失败。在失败之后，<em>Leader</em> 会降低该 <em>Follower</em> 对应的 <strong>nextIndex</strong> 然后继续发送 <strong>AppendEntries</strong> 进行重试，直到找到一个能让 <em>Leader</em> 跟该 <em>Follower</em> 保持一致的 <strong>nextIndex</strong>，这时 <strong>AppendEntries</strong> 请求会返回成功，<em>Follower</em> 会删除 <strong>nextIndex</strong> 之后的所有冲突的日志并将该次请求包含的日志条目附加到自身的日志中 <em>(如果有包含日志的话)</em>。在调用 <strong>AppendEntries</strong> 成功后，该 <em>Follower</em> 与 <em>Leader</em> 的日志就一致了，在当前 Term 中他们将一致保持一致。</p>
<blockquote>
<p>在需要的时候，能够优化这个协议来减少 <strong>AppendEntries</strong> 调用失败的次数。比如在拒绝 <strong>AppendEntries</strong> 请求时，<em>Follower</em> 可以将造成冲突的日志条目的 Term 及该 Term 的第一条日志的索引值包含在返回值中。有了这个信息，<em>Leader</em> 就能够调整该 <em>Follower</em> 的 <strong>nextIndex</strong> 为该 Term 之前的日志条目的索引位置；一个 <strong>AppendEntries</strong> 请求就能够处理一个 Term 期间的所有冲突日志。在实践中，我们并不确定该优化是否必须的，因为日志的不一致并不常见，而且一般也不会有很多不一致的日志条目。</p>
</blockquote>
<p>在这个机制中，<em>Leader</em> 不需要在刚刚取得管理权时执行特殊的操作来保持日志的一致性。他只需要执行通用的操作，然后日志就会在 <strong>AppendEntries</strong> 的一致性检查造成的一次次失败中回到正确的状态。<em>Leader</em> 也从来不会覆盖会删除自身的日志条目 <em>(保证了 Leader Append-Only 属性)</em> 。</p>
<p>这个日志复制机制展现出了我们在第二章中所涉及的一致性属性：Raft 能够保证在大多数服务正常的情况下接收、复制以及应用日志条目；在常见的情况下一个新的日志条目能够在对完成对集群中大多数服务的一次 <em>RPC</em> 调用后复制成功；一个单独的运行缓慢的 <em>Follower</em> 也不会对整体的性能造成影响。</p>
<a class="header" href="print.html#a54-安全性-safety" id="a54-安全性-safety"><h3>5.4 安全性 Safety</h3></a>
<p>前面的章节中说明了 Raft 是如何进行选举及进行日志复制的，然而，前面这些机制的解释还远远不足于确保状态机每个状态机都会以相同的顺序来执行日志条目中的命令。举例来说，<em>Follower</em> 可能在 <em>Leader</em> 提交一些日志时处于宕机状态，紧接着他通过选举成为了新的 <em>Leader</em>，那他就可能会用新的日志覆盖那些原 <em>Leader</em> 已经提交的日志；这就会导致不同的状态机应用了不同的命令序列。</p>
<p>本节通过为 Raft 算法添加一个节点成为 <em>Leader</em> 的限制来完善这个缺陷。这个限制确保了 每个 Term 的 <em>Leader</em> 都会完整的包含前一个 <em>Term</em> 的所有已提交的日志 <em>(即保持 Leader Completeness 属性)</em> 。通过这个限制，我们让提交的规则更加的完善。最后，我们提供了一个证明 <em>Leader Completeness</em> 属性的证据并展示了他是怎么让状态机保持正确行为的。  </p>
<a class="header" href="print.html#a541-选举的限制-election-restriction" id="a541-选举的限制-election-restriction"><h4>5.4.1 选举的限制 Election Restriction</h4></a>
<p>在所有的基于 <em>Leader</em> 的共识算法中，<em>Leader</em> 最终必须存储所有已提交的日志条目。在部分共识算法中，比如 <em>Viewstamped Replication</em>，一个节点可以在不包含所有已提交日志的状态下被选为 <em>Leader</em> 。这些算法通过额外的机制来找到缺失的日志条目并将其发送给新的 <em>Leader</em>, 比如在选举过程以及完成选举之后。不过这需要额外的重大机制及很高的复杂度。Raft 使用了一种较为简单的方式来保障所有当前 Term 之前已提交的日志都会在新的 <em>Leader</em> 中存在，避免了传输这些日志给 <em>Leader</em> 的操作。这意味着所有的日志条目都是单向流转的，只会从 <em>Leader</em> 流向 <em>Follower</em>，并且 <em>Leader</em> 永远不会覆盖日志中已经存在的日志条目。</p>
<p>Raft 通过投票处理防止那些未包含所有已提交日志的 <em>Candidate</em> 赢得选举。<em>Candidate</em> 必须获得集群中大多数的选票才能被选为 <em>Leader</em>，这意味着每个已提交的日志条目必须存在于在集群中的至少一个服务中。如果 <em>Candidate</em> 的日志相对于大多数的服务都 <strong>足够新</strong> <em>(足够新的定义在下面)</em>，那他就会包含所有已提交的日志。<strong>RequestVote</strong> 调用实现了该限制：该调用包含了该 <em>Candidate</em> 的日志信息，投票的服务如果比 <em>Candidate</em> 的日志更新，那他就会拒绝为其投票。</p>
<p>Raft 使用比较日志中最后一个日志条目的索引值及 Term 来判断两个日志的新旧程度。如果两个日志的最后一个日志条目具有不同的 Term，则具有较高 Term 的日志更新。如果两个日志的最后一个日志条目具有相同的 Term，则长度更长 <em>(索引值更大)</em> 的日志更新。</p>
<a class="header" href="print.html#a542-提交前任-term-的日志-comitting-entries-from-previous-terms" id="a542-提交前任-term-的日志-comitting-entries-from-previous-terms"><h4>5.4.2 提交前任 Term 的日志 Comitting Entries From Previous Terms</h4></a>
<p>如 5.3 节的说明，<em>Leader</em> 在确认大多数的服务都成功复制之后就能够提交其日志。如果这个时候 <em>Leader</em> 在提交日志之前宕机了，接下来的 <em>Leader</em> 会继续完成复制该日志的操作。然而，<em>Leader</em> 没办法马上推断出该这个来自于前一个 Term 已经复制到大多数服务中的日志条目是否已经被提交了。<em>Figure 8</em> 说明了这种情况中，一个已经被复制到大多数服务中的日志，还是可能会被新的 <em>Leader</em> 覆盖。</p>
<blockquote>
<p><strong>Figure 8</strong></p>
<p>下面的时间序列展示了为什么 Leader 没办法确认前一个 Term 所复制日志的提交情况。</p>
<ul>
<li><em>(a)</em> 中 S1 作为 <em>Leader</em> 然后部分的复制了索引为 2 的日志条目。</li>
<li><em>(b)</em> 中 S1 宕机，S5 得到 S3 跟 S4 的选票被选为了 Term 3 的 <em>Leader</em>，然后在索引为 2 的日志位置保存了新的日志条目</li>
<li><em>(c)</em> 中 S5 宕机；S1 重启并被选为 <em>Leader</em> 后继续他的复制操作。这时 Term 为 2 的日志条目被复制到了大多数的服务中，但还没被提交。
<ul>
<li><em>(d)</em> 如果这时 S1 宕机了，S5 能够通过 S2、S3、S4 的选票成为新的 <em>Leader</em>，然后用自身的日志重写其他服务的日志。</li>
<li><em>(e)</em> 如果 S1 复制了其自身的日志到大多数的服务中，并且进行了提交，那 S5 就无法赢得选举。这样的话之前的所有日志也能够正常的被提交。</li>
</ul>
</li>
</ul>
<p><img src="./Raft Note.assets/image-20200909160839866.png" style="width: 80%" ></p>
</blockquote>
<p>为了避免类似 <em>Figure 8</em> 中的问题，对于当前 Term 之前的日志条目，Raft 不会根据日志条目是否复制到大多数服务来判断他是否能提交。只会提交当前任期中产生的并且已复制到大多数服务中的日志。这样所有之前的日志只会根据 <em>Log Matching</em> 属性被间接的提交。这里有一些情况是其实 <em>Leader</em> 是能够安全的将旧日志进行提交的（比如某条日志条目已经被复制到了所有的服务中），但 Raft 选择了一种更保守的方式来保持简单性。</p>
<p>Raft 日志的提交规则的复杂性是由于 <em>Leader</em> 在复制前任 Term 的日志到其他服务时保留了日志条目的原始 Term。在一些其他的共识算法中，一个新的 <em>Leader</em> 会使用他的新 Term 来复制其前任 Term 的日志。Raft 的做法保持了日志条目的条理性，因为他们在整个过程中都保持着一致的 Term 信息。而且 Raft 中的新 <em>Leader</em> 相较于其他的算法能够传递更少的来自前任的日志信息 <em>(其他的算法需要发送那些冗余的日志来保证日志在提交时能保持正确的顺序)</em>。</p>
<a class="header" href="print.html#a543-安全性的讨论-safety-argument" id="a543-安全性的讨论-safety-argument"><h4>5.4.3 安全性的讨论 Safety Argument</h4></a>
<p>通过完整的 Raft 算法描述，我们现在能够更精确的证明 <em>Leader Completeness</em>  属性 <em>(这个讨论基于 9.2 节的安全性证明)</em>。我们假设 <em>Leader Completeness</em> 属性是不成立的，然后再推导出其中的矛盾。假设处于 Term T 的 <em>Leader~T~</em>  在他的任期期间提交了一个日志条目。但该条目并不存在于其他后续的 <em>Leader</em> 中。考虑有一个最小的 Term U &gt; T，<em>Leader~U~</em> 的日志中不存在这个日志条目。</p>
<ol>
<li>被提交的条目在 <em>Leader~U~</em> 的的选举过程中必须是缺失的，因为 <em>Leader</em> 从来不会删除会重写自身的日志</li>
<li><em>Leader~T~</em> 复制了日志到集群中的大多数服务，然后 <em>Leader~U~</em> 收到了集群中大多数服务的选票。这样，在投票者中至少会有一个服务接收了 <em>Leader~T~</em> 的日志条目并且将选票投给了 <em>Leader~U~</em> ，就像在 <em>Figure 9</em> 中展示的一样，该投票者是引发矛盾的关键。</li>
<li>该投票者必须在投票给 <em>Leader~U~</em> 之前从 <em>Leader~T~</em> 中接收了已提交的日志；否则的话他将拒绝 <em>Leader~T~</em> 所发出的 <strong>AppendEntries</strong> 请求 <em>(因为他的 currentTerm 会比 T 大</em> )*。</li>
<li>该投票者在他投票给 <em>Leader~U~</em> 后依然保存着该日志条目，因为中间的任何 <em>Leader</em> 都包含了该条目 <em>(假设)</em>。<em>Leader</em> 从来不会删除日志条目，<em>Follower</em> 也只会在跟 <em>Leader</em> 发生冲突时才删除该条目。</li>
<li>该投票者将他的选票投给了 <em>Leader~U~</em> ，因此 <em>Leader~U~</em> 的日志至少是跟投票者的一样新的。这会导致其中一个矛盾的发生。</li>
<li>首先，如果该投票者跟 <em>Leader~U~</em> 的最后一条日志的 Term 是一致的，那 <em>Leader~U~</em> 跟投票者的日志至少是具有相同长度的，所以 <em>Leader~U~</em> 的日志肯定包含了投票者的所有日志。这就是矛盾处，因为我们假定了投票者包含了 <em>Leader~U~</em> 中不存在的日志。</li>
<li>另一方面，<em>Leader~U~</em> 最新的日志的 Term 必须必高于其他投票者。再具体一点就是他必须要比 T 大，因此投票者最后一个日志条目的 Term 最少要是 T <em>(因为他包含在 Term T 期间已经提交的日志)</em> 。<em>Leader~U~</em>  的上一个 <em>Leader</em> 所创建的最后一条日志条目必须要包含在其自身的日志中 <em>(假设)</em> 。因此，根据 <em>Log Maching</em> 属性， <em>Leader~U~</em> 的日志必然也会包含该已提交的日志，这是另一个矛盾。</li>
<li>至此完成了矛盾的推导。因此所有大于 Term T 的 <em>Leader</em> 必须包含所有 Term T 所提交的日志。</li>
<li><em>Log Matching</em> 属性保证了未来的 <em>Leader</em> 会包含所有间接提交的日志，正如 <em>Figure 8(d)</em> 中的索引 2。</li>
</ol>
<p>根据 <em>Figure 3</em> 所示，我们通过 <em>Leader Completeness</em> 属性，可以证明 <em>State Machine Safety</em> 属性。如果一个服务已经将指定索引位置的日志条目应用到了自身的状态机中，则不会再有其他的服务会在相同的索引位置应用不同的日志条目。当一个服务将日志应用到自己的状态机时，他自身的日志必须是跟 <em>Leader</em> 的日志中，到该日志条目之前的日志都是一致且已提交的。现在考虑有一个服务将一个较低的 Term 中的某个索引位置的日志条目应用到了状态机中；则 <em>Log Completeness</em> 属性保证了具有更高 Term 的 <em>Leader</em> 的日志中会在该索引位置包含相同的日志条目，所以后续 Term 的服务在应用该日志条目时，能够得到相同的结果。因此，<em>State Machine Sfatety</em> 属性成立。</p>
<p>最后，Raft 需要服务们按顺序来应用日志条目。结合 <em>State Machine Sfaety</em> 属性，这意味着所有的服务都将以相同的顺序应用同样的日志到自身的状态机中。</p>
<a class="header" href="print.html#a55-follower-跟-candidate-宕机-follower-and-candidate-crashes" id="a55-follower-跟-candidate-宕机-follower-and-candidate-crashes"><h3>5.5 Follower 跟 Candidate 宕机 Follower And Candidate Crashes</h3></a>
<p>到目前为止我们关注的都是 <em>Leader</em> 的失效。 <em>Follower</em> 及 <em>Candidate</em> 的失效处理相对于 <em>Leader</em> 则要简单得多，并且他们的失效都是以相同的方式来处理的。在 <em>Follower</em> 或 <em>Candidate</em> 宕机时，发送给他们的 <strong>RequestVote</strong> 或 <strong>AppendEntries</strong> 都会失败。Raft 处理该失败的方式是无限的进行重试；如果宕机的服务重启了，那后续的 RPC 调用会返回成功。如果一个服务在处理完 RPC 请求并且返回响应之前宕机了，那他在重启之后会收到跟之前相同的请求。Raft 的 RPC 调用是具有幂等性的，所以收到相同的请求并不会造成任何问题。比如如果一个 <em>Follower</em> 收到的 <strong>AppendEntries</strong> 请求中包含了他已经存在的的日志，那他将会忽略这些重复的日志信息。</p>
<a class="header" href="print.html#a56-时序与可用性-timing-and-availability" id="a56-时序与可用性-timing-and-availability"><h3>5.6 时序与可用性 Timing And Availability</h3></a>
<p>Raft 对安全性的一个要求是不依赖于时序: 系统不能在某些事件发生的时机比预期的快或慢时产生错误的结果。然而，可用性 <em>(系统及时响应客户请求的能力)</em> 必然会依赖于时序。比如，就算消息在传递时遭遇了服务宕机导致占用比常见情况更多的时间，<em>Candidate</em> 也不会停下来太久等待选举；在没有可用 <em>Leader</em> 的情况下，Raft 将停滞不前。</p>
<p>时序在 Raft 的 <em>Leader</em> 选举中是最重要的部分。Raft 是通过下面的时序要求来满足选举及保持 <em>Leader</em> 的管理权的：
$$
broadcastTime &lt; electionTimeout &lt; MTBF
$$
在这个不等式中 $broadcastTime$ 是取了服务之间发送 RPC 请求到得到响应的平均时间；$electionTimeout$ 则是我们在 5.2 节所介绍的选举的超时时间；最后的 $MTBF$ 是单个服务失效间隔的平均时长。$broadcastTime$ 应当远小于 $electionTimeout$ ，这样 <em>Leader</em> 在赢得选举后才能比较可靠的发送心跳消息来管理其他 <em>Follower</em>；通过给予 $electionTimeout$ 一个随机的波动范围，可以降低瓜分选票的概率。而 $electionTimeout$ 应当远小于 $MTFB$ 来让系统趋于稳定。当 <em>Leader</em> 宕机时，整个系统会在 $electionTimeout$ 的这段期间处于不可用状态；我们会期望这种情况只占系统运行期间的绝小部分。</p>
<p>$broadcastTime$ 以及 $MTBF$ 是根据系统的属性所决定的，而 $electionTimeout$ 则是我们需要自己决定的。Raft 的 RPC 需要接收方能够将信息持久化到稳定的存储介质中，所以 $broadcastTime$ 可能会是在 0.5ms 到 20ms 之间，这取决于具体的存储技术。最终的结果是，$electionTimeout$ 一般会设置在 10ms 到 500ms 之间。而服务器的 $MTBF$ 则可能会在数个月甚至更长，这让我们能够很容易的满足上述的时序要求公式。</p>
<a class="header" href="print.html#a6-集群的成员变更-cluster-membership-changes" id="a6-集群的成员变更-cluster-membership-changes"><h2>6. 集群的成员变更 Cluster Membership Changes</h2></a>
<p>到现在为止我们都假设集群的配置 <em>(参与到共识算法的服务集合)</em> 是固定的。但是在实践中，这个配置信息偶尔是会产生变动的。比如某台服务器宕机了又或者我们需要改变副本的个数。尽管这能够通过下线集群的服务，更新配置文件，然后重启集群来实现，但在这期间整个集群都将处于不可用的状态。更进一步，那些需要人手动去完成的操作都会导致更高的风险。为了避免这些问题，我们选择将自动更新配置文件的机制纳入到 Raft 共识算法中。</p>
<p>为了保证变更配置文件的机制是安全的，在配置变更期间不能够出现有两个 <em>Leader</em> 共存且使用同一个 Term 的状态。不幸的是，在服务从旧配置文件转换成新配置文件的过程中是有可能产生这种不安全状态的。因为不可能在同一个时刻更新所有服务的配置，所以集群有可能在同一时刻被分割成两个包含 <em>大多数</em>  服务的小集群 <em>(如 Figure 10 所展示)</em> 。</p>
<blockquote>
<p><strong>Figure 10</strong></p>
<p>将配置文件直接切换成新的是不安全的，因为不同的服务可能会在不同的时刻进行切换。在这个例子中，集群的服务器从 3 个切换为 5 个。不幸的是在某个时刻有两个不同的服务被同时选举 为 Term 一样 <em>Leader</em> 了。其中一个的选票来自于使用旧配置 <em>(C~old~)</em> 的大多数服务 ，另外一个则来自于使用了新配置 <em>(C~new~)</em> 的大多数服务。</p>
<p><img src="./Raft Note.assets/image-20200911214140115.png" style="width: 60%"></p>
</blockquote>
<p>为了确保安全性，配置的切换需要被分成两步来实现。有多种不同的方式来实现两步完成的目标，比如有的系统会在第一步把使用旧配置的服务禁用来防止他们继续处理客户端的请求；第二步则让所有服务启用新的配置。在 Raft 中，集群中的服务首先会转换到一个我们称为 <em>Joint Consensus</em>  的过渡配置；当 <em>Joint Consensus</em> 被提交后整个系统才会转换为新的配置。 <em>Joint Consensus</em> 会同时包含旧的跟新的配置信息：</p>
<ul>
<li>日志条目会被复制到两种配置的所有的服务中</li>
<li>每种配置的服务都可能称为 <em>Leader</em></li>
<li>共识 <em>(包括选举的跟提交日志的)</em> 需要分别在新的跟旧的配置中都达成</li>
</ul>
<p><em>Joint Consensus</em> 允许在不对安全性做出妥协的情况下，让不同配置的服务们能在不同的时间点上转换配置。而且 <em>Joint Consensus</em> 允许集群在切换配置信息的同时继续为客户端提供服务。</p>
<blockquote>
<p><strong>Figure 11</strong></p>
<p>下图是关于配置变更的时间线。虚线表示了配置信息创建但未被提交，实线表示最新的配置信息被提交。 <em>Leader</em> 一开始会创建 <em>C~old,new~</em> 配置并提交到 <em>C~old~</em> 及 <em>C~new~</em> 的大多数中。在这期间不存在一个 <em>C~old~</em> 跟 <em>C~new~</em> 的能够独自做抉择的时间点。</p>
<p><img src="./Raft Note.assets/image-20200914212837616.png" style="width: 80%" ></p>
</blockquote>
<p>集群的配置信息是通过一种特殊的日志条目来保存跟传输的；<em>Figure 11</em> 说明了整个配置切换的过程。当 <em>Leader</em> 接收到要求将配置从 <em>C~old~</em> 切换到 <em>C~new~</em> 的请求时，会将配置信息保存为一个配置日志条目并使用我们前面提过的复制日志的机制将其复制到其他的服务中 <em>(即图中的 C~old,new~)</em> 。当一个服务将给配置日志条目添加到自身的日志中后，他就会使用新的配置信息来作为后续所有抉择的依据 <em>(一个服务总是使用他日志中最新的配置，无论他是否被提交)</em> 。这意味着 <em>Leader</em> 可以使用这个 <em>C~old,new~</em> 的规则去决定该日志条目何时需要被提交。如果 <em>Leader</em> 宕机，新 <em>Leader</em> 的配置会是 <em>C~old~</em> 及 <em>C~old,new~</em> 之一，这取决于赢得选举的 <em>Candidate</em> 是否接收到了 <em>C~old,new~</em> 。但不管是哪个配置，<em>C~new~</em> 在这个阶段都无法单独的做出任何抉择。</p>
<p>当 <em>C~old,new~</em> 被提交后，<em>C~old~</em> 跟 <em>C~new~</em> 都不能够单独的在得到其他人同意时做任何决定，并且 <em>Leader Completeness</em> 属性确保了只有收到了 <em>C~old,new~</em> 的服务能够被选举为 <em>Leader</em>。这时 <em>Leader</em> 就能够安全的创建一个新的 <em>C~new~</em> 日志配置条目并将其复制到集群。同样的，该日志配置会在服务接收到他的那一刻生效。当新的配置按照 <em>C~new~</em> 的规则提交后，旧的配置已经变的无关紧要了，而那些不在新配置中的服务则能够被关机了。如 <em>Figure 11</em> 所展示的，整个过程中不存在 <em>C~new~</em> 跟 <em>C~old~</em> 能够单独做抉择的时间点；这保证了整个过程的安全性。</p>
<p>关于配置切换还有三个问题需要处理。第一个问题是新服务可能不包含任何日志条目，如果他在这种状态下加入了集群，那他可能需要一段很长的时间来追赶上其他服务，在追赶的这段期间他可能无法再提交任何新的日志。为了避免这种状况，Raft 在配置切换前引出一个额外的步骤，也就是该服务会以一种没有投票资格的方式加入集群 <em>(即是 Leader 会复制日志条目给他，但不会将他们当成大多数之一)</em> 。当一个服务追赶上集群中的其他服务时，日志的切换就能够像之前所描述的继续进行了。</p>
<p>第二个问题是集群当前的 <em>Leader</em> 不一定存在于新的配置中。这种情况，<em>Leader</em> 会在提交了 <em>C~new~</em> 后将自己转换为 <em>Follower</em>。这意味着这期间 <em>(提交了 C~new~ 时)</em>   <em>Leader</em> 管理着一个不包含他自身的集群； 他复制日志条目但不会把自己当成大多数之一。<em>Leader</em> 将提交 <em>C~new~</em> 前作为过渡，因为 <em>C~new~</em> 提交是新配置信息能够独立做抉择的首个时间点 <em>(之后的选举都会在 C~new~ 中产生)</em>。在这个时间点之前，可能只会从 <em>C~old~</em> 中选出新的 <em>Leader</em>。</p>
<p>第三个问题是移除服务 <em>(即移除不在 C~new~ 中的服务)</em>。这些服务将不再受到心跳，所以他们会到达超时时间并发起新一轮的选举。他们会使用新的 Term 发送 <strong>RequestVote</strong> 请求，这会导致当前的 <em>Leader</em> 被转换为 <em>Follower</em> 状态。一个新的 <em>Leader</em> 最终会被选出来，但那些被移除的服务会继续超时然后不断的重复上述的状况，造成集群处于低可用的状态。</p>
<p>为了防止这个问题，服务们会在确认当前 <em>Leader</em> 可用时忽视 <strong>RequestVote</strong> 请求。特别的，如果一个服务在接收到 <em>Leader</em> 后的最小的超时时限接收到新的 <strong>RequestVote</strong>请求时，他不会更新 Term 跟进行投票。这不会影响正常的选举，因为每个服务在进行新一轮选举前都最少会等待最小的选举超时时间 <em>(Election Timeout)</em>。然而，他帮助集群避免被那些移除服务打断正常的服务: 如果 <em>Leader</em> 能够正常的发送心跳给集群中的服务，那他就不会被更大的 Term 废除。</p>
<a class="header" href="print.html#a7-日志压缩-log-compaction" id="a7-日志压缩-log-compaction"><h2>7. 日志压缩 Log Compaction</h2></a>
<p>Raft 的日志在处理来自客户端请求的常规的操作中会不断的增长，但在实际的系统中，我们不可能让他无止境的增长下去。当日志越来越多，他会占据更多的磁盘空间，也将需要使用更长的时间来进行恢复。如果没有一个可以用来丢弃过期/废弃日志的机制，最终会导致系统的可用性出现问题。</p>
<p>快照是一个实现日志压缩最简单的方式。使用快照，整个系统的当前状态能够被存储到存储器中，然后在这个状态之前的所有日志就可以被丢弃了。<em>Chubby</em> 及 <em>Zookeeper</em> 都使用了快照机制，接下来的章节我们将介绍 Raft 的快照实现方式。</p>
<p>增量化的日志压缩方式，如使用 日志清理 <em>([Log Clean][LogClean])</em> 或 结构化日志合并树 ([Log-Structured Merge Tree][LSM]) 的方式都是可行的。这些方式每次都只对一部分数据进行处理，所以他们能够将压缩的负载分散到各个时间段中。他们首先会挑选一个包含了很多已删除、被覆盖对象的数据区域，接着重写该区域还存活的对象使之存储的更加紧凑，接着释放该数据区域。和直接对比整个数据集相比，这需要一些新的重要的机制及用于对比日志的复杂的比较方法。日志清理可能会需要对 Raft 进行修改，状态机则可以使用 [LSM][LSM] 以同样的接口来实现快照。</p>
<blockquote>
<p><strong>Figure 12</strong></p>
<p>服务将日志中已提交的日志条目 <em>(索引值为 1 到 5 的)</em> 替换为一个新的快照。该快照保存了当前状态 <em>(示例中的变量 $x$ 及 $y$ )</em>，该快照包含的最新条目的索引值及 Term，用来表示该快照包含了日志条目索引 6 之前的日志信息。</p>
<p><img src="./Raft Note.assets/image-20200915140946977.png" style="width:70%"></p>
</blockquote>
<p><em>Figure 12</em> 展示了 Raft 快照的基本想法。每个服务都会独立，将已提交的日志条目转换为快照。大部分的工作在于如何将状态机的当前状态写入快照中。Raft 同样也包含了少量的元数据在快照中：包含的<em>最新索引值</em> <em>(last included inxex</em>) 是指快照所替换的那部分日志中的最新日志条目的索引 <em>(指最后一条应用到状态机的日志条目)</em> ；最后包含的 Term <em>(Last Included Term)</em> 即该日志条目的 Term。这些信息将用来支撑 <strong>AppendEntries</strong> 的快照之后第一条日志的一致性检查，因为该操作需要待处理日志的前置的日志条目及 Term 信息。为了能够支持 集群成员变更 <em>(第六章)</em> ，快照也会保存与 <em>最新索引值</em> 所对应的最新的一份配置信息。当服务将完成快照的构建后，他们应该删除所有在 <em>最新索引值</em> 之前的所有日志条目，当然也包括了之前的快照信息。</p>
<p>尽管服务在通常的情况下都会独立的构建快照，但 <em>Leader</em> 偶尔也会需要将快照发送给延迟较严重的 <em>Follower</em> 上。这个情况一般发生在 <em>Leader</em> 丢弃了下一条需要发送给 <em>Follower</em> 的日志时。幸运的是这种情况并不常见：<em>Follower</em> 通常会紧跟着 <em>Leader</em> 去添加日志条目。但是一个异常缓慢的 <em>Follower</em> 或者是新加入集群的服务则没办法跟上 <em>Leader</em>，<em>Leader</em> 通过在网络中发送快照的方式将这种类型的 <em>Follower</em> 状态同步上来。</p>
<p><em>Leader</em> 使用一种新的称为 <strong>InstallSnapshot</strong> 的 RPC 调用来发送快照给那些落后太多的 <em>Follower</em>；正如 <em>Figure 13</em> 所示的一样。当一个 <em>Follower</em> 接收到这种 RPC 请求时，他需要决定如何处置现有的日志条目。通常情况下快照会包含一些新的不存在于接收者日志中的信息。在这种情况下，<em>Follower</em> 会使用快照来替代他所有的日志，包括那些未提交的与快照有冲突的日志。如果 <em>Follower</em> 接收到的快照比他的日志还要旧 <em>(比如因为重复发送快照或其他错误造成)</em>，则那些包含在快照中的日志条目会被删除，而那些快照之后的日志则是合法并且会被继续保留。</p>
<blockquote>
<p><strong>Figure 13 InstallSnapshot RPC</strong></p>
<p>由 <em>Leader</em> 发起来发送快照数据块给 <em>Follower</em> 的请求，<em>Leader</em> 会按照顺序来发送这些数据块。</p>
<ul>
<li><strong>参数</strong>
<ul>
<li><em>term</em>* <em>Leader</em> 的 Term</li>
<li><strong>leaderId</strong> <em>Follower</em> 可用他来转发客户端的情况</li>
<li><strong>lastIncludedIndex</strong> 表示快照中将包含从开始到该索引对应的日志条目</li>
<li><strong>offset</strong> 数据块在快照文件的偏移量</li>
<li><strong>data[]</strong> 快照数据块的原始数据，包含从 <strong>offset</strong> 开始的数据</li>
<li><strong>done</strong> 如果快照包含的数据快已经是最后一个了，则为 <em>True</em></li>
</ul>
</li>
<li><strong>返回值</strong>
<ul>
<li><em>term</em>* 当前 Term，用于 <em>Leader</em> 更新自身的状态</li>
</ul>
</li>
<li><strong>接收者的实现</strong>
<ol>
<li>如果 <strong>term &lt; currentTerm</strong> 直接返回</li>
<li>在接收到第一个快照数据块时，建立一个新的快照文件 <em>(即 offset 为 0 时)</em></li>
<li>将数据根据 <em>offset</em> 写入到快照文件中</li>
<li>在 <em>done</em> 为 <em>False</em> 时回复或者等待更多来自快照请求的数据</li>
<li>保存快照文件，丢弃那些已存在并具有更小索引值的快照文件</li>
<li>如果存在跟快照文件具有相同索引 及 Term 的日志条目，保存在这之后的日志条目并返回</li>
<li>丢弃所有日志信息</li>
<li>使用快照的内容来重置状态机 <em>(并应用快照中包含的集群配置信息)</em></li>
</ol>
</li>
</ul>
</blockquote>
<p>这个快照机制背离了 Raft 的强 <em>Leader</em> 原则， <em>Follower</em> 在接收快照请求时不需要对 <em>Leader</em> 有任何认知。然而我们认为这个做法是值得的。<em>Leader</em> 的存在是为了帮助我们避免做出那些与一致性有冲突的决定。而在发送快照时一致性是已经被保证了的，所以不会造成任何冲突。数据依然只会从 <em>Leader</em> 流向 <em>Follower</em> ，只是 <em>Follower</em> 现在可以自己来重组他们的数据了。</p>
<p>我们考虑过一种基于 <em>Leader</em> 的方案 - 只有 <em>Leader</em> 可以创建快照，但他会要求 <em>Leader</em> 向所有的 <em>Follwer</em> 发送快照，这种做法会产生两种缺点。第一，发送快照给每个 <em>Follower</em> 会浪费网络的带宽并且拖慢处理快照的进度。每个 <em>Follower</em> 已经有了足够的信息来构建自己的快照了，相较于通过网络发送的方式，在本地构建快照的成本是远远更低的。第二，由 <em>Leader</em> 来实现是更为复杂的，比方说， <em>Leader</em> 需要发送快照给 <em>Follower</em> 的同时又并行的复制新的日志条目，以避免堵塞客户端的请求。</p>
<p>这里还存在两个会对快照的性能产生冲击的问题。第一，服务需要决定何时建立快照，如果服务建立的频率太过频繁，将浪费大量的磁盘带宽及能源；如果快照建立的频率太低，则可能对消耗大量的磁盘容量，同时也会显著的增加重启服务时重新应用日志所需的时间。一个简单的策略是在日志的尺寸到达某个固定的大小时触发快照。如果所设的这个尺寸的大小显著的大于快照期望的快照的大小，那快照对磁盘带宽的影响就会很小了。</p>
<p>第二个会影响性能的问题就是创建快照需要耗费显著的时间，但我们不希望这会造成常规的操作的延时。解决的方案是使用类似 写时复制 <em>(Copy On Write)</em> 的机制，让后续的更新操作不会影响快照的写入操作。比如说，一些使用函数式数据结构的状态机本身就支持这种特性。又或者使用操作系统提供的 写时复制机制 <em>(比如 Linux 的 Fork)</em>，能够为状态机创建一份保存在内存中的副本 <em>(我们的实现就使用了这个机制)</em> 。</p>
<a class="header" href="print.html#a8-与客户端的交互-client-interaction" id="a8-与客户端的交互-client-interaction"><h2>8. 与客户端的交互 Client Interaction</h2></a>
<p>本章将介绍客户端是如何跟 Raft 进行交互的，包括了客户端是如何确认 <em>Leader</em> 及如何支持可线性化语义 <em>([Linearizable Semantics](Linearizable Semantics))</em> 的。 这个问题是所有的一致性系统都需要面对的，Raft 的方案跟其他的系统基本类似。</p>
<p>Raft 的客户端会将所有的请求都发送给 <em>Leader</em>。当一个客户端首次启动时，他会尝试随机的连接到所有服务的其中一个，如果客户端选择的服务不是 <em>Leader</em>，该服务会拒绝客户端的请求并且返回他所知道的 <em>Leader</em> 的信息 <em>( AppendEntries 请求中就包含了 Leader 的地址信息)</em>。如果 <em>Leader</em> 宕机了，客户端的连接就会超时，这时客户端会重新随机选一个服务。</p>
<p>Raft 的目标就是实现线性一致性语义 <em>(每个操作在发出请求到响应之间只会严格的执行一次)</em>。然而，就如我们前面说的，Raft 允许执行同一个命令多次：比如一个 <em>Leader</em> 在提交日志后、响应客户端前宕机了，客户端会在新的 <em>Leader</em> 上重试该命令，导致该命令被执行多次。解决方案是客户端会给每个命令一个序列号，这样状态机记能通录最后执行命令的序列号，并将其关联到对应的响应中。如果接受的命令的序列号已经被执行了，他可以直接根据序列号找到响应并返回给客户端，从而避免重复执行该命令。</p>
<p>状态机在处理只读的操作时不需要产生任何新的日志。然而，如果不加上一些其他的保证，这些操作可能会有返回脏数据的风险，如果一个 <em>Leader</em> 在响应客户端的请求时没发现自己已经被新的 <em>Leader</em> 取代了。线性一致性的读操作不能够返回脏数据，所以 Raft 需要两个措施做到在不修改日志的前提下达到目标。第一，<em>Leader</em> 必须持有最新提交日志条目的信息，<em>Leader COmpleteness</em> 属性保证了 <em>Leader</em> 包含所有已提交的日志，但在他自己的 Term 开始时，他无法确认哪些是已提交的，为了确认这个信息，他需要在自己的 Term 中提交一条日志。Raft 的做法是在每个 <em>Leader</em> 开始他的 Term 时提交一条空白的不进行任何操作 <em>(no-op)</em> 的日志条目。第二，<em>Leader</em> 需要在处理只读的请求前先确认自己是否已被罢免 <em>(如果有新的 Leader 被选举出来，那该 Leader 的信息可能已经不是最新的了。)</em>。Raft 在响应客户端之前，通过向大多数服务发送心跳的方式来确认自己是否最新。又或者 <em>Leader</em> 可以依靠心跳机制来提供一个租约，但这又会导致一个时序上的安全问题 <em>(因为他假定时钟的误差是有限的)</em> 。</p>
<a class="header" href="print.html#a9-实现与评估-implementation-and-evaluation" id="a9-实现与评估-implementation-and-evaluation"><h2>9. 实现与评估 Implementation And Evaluation</h2></a>
<p>我们实现了  Raft  作为 RAMCloud 用来保存配置信息的复制状态机，用来帮助 RAMCloud 实现故障协调。这个 Raft 的实现包含了大约 2000 行 <em>C++</em> 代码，其中不包含测试、注释跟空行。这些代码是可以[免费获取][RaftCode]的。而且大约还有 25 个独立的开源的基于本篇论文的 Raft 实现。还有很多的公司也开发了[基于 Raft 的系统][RaftBasedSystem]。</p>
<p>本章后续的内容从三个方面来评估 Raft, 包括: 可理解性、正确性 以及 性能。</p>
<a class="header" href="print.html#a91-可理解性-understandability" id="a91-可理解性-understandability"><h3>9.1 可理解性 Understandability</h3></a>
<p>我们将 Raft 与 Paxos 作为对比来确认他的可理解性，我们在斯坦福的高级操作系统及 伯克利的 分布式计算课程的高年级及将毕业的学生中做了测试。并分别录制了 Raft 跟 Paxos 课程的视频，然后开展相同的测试。Raft 的课程包含了出了日志压缩部分的内容；Paxos 也覆盖了足够的知识来创建一个相同的复制状态机，包含了 <em>Sigle-decree Paxos</em> 跟 <em>Multi-decree Paxos</em>，更新配置及现实所需的少量优化 <em>(如 Leader 选举)</em>。这个测试了对算法的基本理解及对算法的各种边界情况进行说明。每个学生都会先看一个一个视频，做对应的测试；然后再看第二个视频做第二个测试。大约参与的一半学生先做了 Paxos 的测试，另一半则先做 Raft 的测试，这是为了避免前面第一个学习内容的经验会影响后面第二个学习内容的效果。我们比较了每个测试参与者的成绩，并以此来确认他们对 Raft 的理解是否更好。</p>
<p>我们尽可能的让 Paxos 跟 Raft 的比较更公平。这个实验用两种方式来让 Paxos 的学生有更多的优势：43 个参与的学生中有 15 个是有一些关于 Paxos 的经验的；Paxos 的视频比 Raft 的要长 14%。正如 Table 1 总结的，我们在努力降低对两者之间的倾向性。所有的材料都可供审查。</p>
<p>平均来说，参与学生的 Raft 分数要高于 Paxos 4.9 分 <em>(总分是 60 分，Raft 的是 25.7 分，Paxos 的是 20.8 分)</em>；<em>Figure 14</em> 展示了各自的分数。通过 <em>$t$-test</em> 表明这个测试的可信度有 95%，真实的 Raft 分数的分布比真实的 Paxos 的分数分数要高至少 2.5 分。</p>
<p>我们还建立了一个线性回归模型，该模型基于以下三个因素：进行的测试，学习前对 Paxos 的熟悉程度，以及学习这两个算法的顺序。该模型预测了两者之间的分数差距有 12.5 分，这要远高于之前的 4.9 分，因为有大部分的学生是在已经有 Paxos 经验的前提下参加测试的，这造成了他们具有对 Paxos 的更大优势。这个模型同时预测了对于先进行了 Paxos 测验的人来说，他们的 Raft 的得分会低于 Paxos 6.3 分。我们也无法解释，但这确实是个统计结果。</p>
<p>我们同样对进行了测试的学生进行了调查，去了解他们觉得哪个算法更易于实现跟解释；这个结果在 <em>Figure 15</em> 上展示了，具有压倒性的结果是大部分的参与者都觉得 Raft 更易于实现及解释 <em>(41 个人中有 33 个)</em>。然而，这些主观性的回答可能不比参与者的分数可靠，而且参与者可能因为我们对 Raft 更容易理解的假设而产生偏见。</p>
<p>更详细的对学习 Raft 的讨论可以在 [这里][StudyRafu] 找到。</p>
<a class="header" href="print.html#a92-正确性-correctness" id="a92-正确性-correctness"><h3>9.2 正确性 Correctness</h3></a>
<p>我们建立了一个正式的规格说明并且在第五章已经证明了其共识机制的安全性。该正式的规格将 <em>Figure 2</em> 中的总结完整清晰的用 <em>TLA+</em> 语言进行了描述。它大概有 400 行且充分的证明了相关的主题。它对于那些需要自己来实现 Raft 的人来说也是非常参考价值的。并且已经机械化的使用 <em>TLA</em> 证明了 <em>Log Completeness</em> 属性。然而这个证明所依赖的前提还没使用机械的方式进行证明 <em>(比如我们没有证明规格中的类型安全)</em>。同时我们还证明了 <em>State Machine Safe</em> 属性 及其所依赖的前提 <em>(大约用了 3500 个字)</em>。</p>
<a class="header" href="print.html#a93-性能-performance" id="a93-性能-performance"><h3>9.3 性能 Performance</h3></a>
<p>Raft 的性能跟其他如 Paxos 等一致性算法是类似的。其中 <em>Leader</em> 分发新的日志条目给其他服务是最重要的一个评测场景。Raft 通过最小化消息的传递来达成这个目标 <em>(只通过一轮消息传递给集群中的一半服务)</em>。当然，其实还能有更多的优化性能的方案。比如，可以通过支持批量及管道的方式来实现更高的吞吐量及延迟。有很多的优化方案在其他的各种算法中已经提出过；其中的大部分方案也适用于 Raft，但我们将这些都留到了以后的工作去做。</p>
<p>我们使用自己的 Raft 实现去确认 Raft 的 <em>Leader</em> 选举算法，并回答了两个问题。第一是这个选举的过程是否足够高效？第二是 <em>Leader</em> 宕机时集群能实现的最小失效时间是多长？</p>
<p>为了确认 <em>Leader</em> 的选举算法，我们不断的使集群中的 <em>Leader</em> 宕机然后统计在每次宕机跟选出新的 <em>Leader</em> 之间的间隔 <em>(在 Figure 16 中)</em>。为了制造出最糟糕的情况，可以让每个服务都具有不同长度的日志长度，让每个 <em>Candidate</em> 都没办法成为 <em>Leader</em>。更进一步的，为了鼓励脑裂的情况发生，我们的测试脚本在退出之前会用同步的方式以 <em>Leader</em> 的身份发送心跳广播 <em>(这跟 Leader 复制了一个新的日志条目后崩溃的情况是类似的)</em>。<em>Leader</em> 会均匀的在心跳的间隔中宕机，在所有的测试中这个间隔都是最小选举超时时间的一半。因此，该最小宕机时间将是最小选举超时时间的一半。</p>
<blockquote>
<p><strong>Figure 16</strong></p>
<p>The time to detect and replace a crashed leader. The top graph varies the amount of randomness in election timeouts, and the bottom graph scales the minimum election timeout. Each line represents 1000 trials (except for 100 tri- als for “150–150ms”) and corresponds to a particular choice of election timeouts; for example, “150–155ms” means that election timeouts were chosen randomly and uniformly be- tween 150ms and 155ms. The measurements were taken on a cluster of five servers with a broadcast time of roughly 15ms. Results for a cluster of nine servers are similar. ==TODO==</p>
<p><img src="./Raft Note.assets/image-20200919235458228.png" style="width: 70%" ></p>
</blockquote>
<p>在 <em>Figure 16</em> 的下半部分图表中展示了宕机的时间可以通过降低减少的耗时来实现。在超时时间为 12-24ms 时，他只花费了平均 35ms 来选出新的 <em>Leader</em> <em>(最长的一次花费了 152ms)</em>。然而，设置超过了这个点的太低的超时时间违背了 Raft 的时序要求：<em>Leader</em> 很困难在选举的超时时间内完成心跳广播。这可能导致不必要的 <em>Leader</em> 更换并降低了系统的可用性。我们建议使用较为保守的超时时间，如 150-300ms；这个超时时间能尽可能的不造成无谓的 <em>Leader</em> 更换及提供较好的可用性。</p>
<a class="header" href="print.html#a10-相关工作-related-work" id="a10-相关工作-related-work"><h2>10. 相关工作 Related Work</h2></a>
<p>我们研读了许多的关于一致性算法的公开论文，其中的大部分我们都将其归类到下面的列表中：</p>
<ul>
<li>Lamport's 对 Paxos 原始的说明 [15], 以及其他一下尝试将其解释得更清晰的论文 [16. 20, 21]</li>
<li>详尽的 Paxos 说明， 他们提供了很多确实的细节并对算法做了一些修改，来为实现提供更好基础支持 [26, 39, 13]</li>
<li>实现了一致性算法的系统，如 Chubby [2, 4]， Zookeeper [11, 12]， 以及 Spanner [6]。Chubby 跟 Spanner 并没有正式的发布其实现细节，只是他们都声明了自己是基于 Paxos 实现的。Zookeeper 的算法则有正式发布的细节，但他跟 Paxos 的实现大相庭径。</li>
<li>能够应用到 Paxos 的性能的优化方案 [18, 19, 3, 25, 1, 27]</li>
<li>Oki 及 Liskov 的 Viewstamped Replication <em>(VR)</em>，跟 Paxos 几乎同时发布的实现一致性的算法。原本的说明 [29] 是在分布式事务的协议中出现的，后来更核心的一致性协议部分则作为单独的更新发布 [22]。VR 使用了跟 Raft 一样基于 <em>Leader</em> 的实现，并且跟 Raft 有许多的共同点。</li>
</ul>
<p>Paxos 与 Raft 最大的区别在于 Raft 使用的是强 <em>Leader</em> 职位: Raft 将 <em>Leader</em> 选举作为了一致性协议中不可或缺的一部分，并且他聚焦于让 <em>Leader</em> 具有完整的功能。这个设计的结果是简化了算法从而让其易于理解。比如在 Paxos 中，<em>Leader</em> 的选举跟一致性算法是正交的: 它对于 Paxos 来说是为了优化性能，对于达成一致性不是必须的。然而这带了了一些额外的机制: Paxos 包含了为基础一致性提供支持的二次提交协议及独立的 <em>Leader</em> 选举。作为对于，Raft 将 <em>Leader</em> 纳入为一致性算法的一部分，并将其作为为实现一致性的两次提交中的第一步。这让它相对于 Paxos 来说需要更少的机制支持。</p>
<p>如 Raft、VR 跟 ZooKeeper 都是基于 <em>Leader</em> 的，因此他们都具有跟 Raft 类似的优点。但是 Raft 相较于 VR 跟 ZooKeeper 具有更少的机制，因为他为其他非 <em>Leader</em> 的类型定义了最小化的功能。比如日志条目在 Raft 中只能通过 <strong>AppendEntries</strong> 调用单向的从 <em>Leader</em> 发往其他服务。VR 中的日志条目是允许双向传递的 <em>(Leader 在选举阶段会接收来自其他服务的日志条目)</em>；这导致了一些额外的机制及复杂性。在 <em>ZooKeeper</em> 发表的论文中也说明了 <em>Leader</em> 同时会接收或发送日志条目，但其具体实现则更接近于 Raft。</p>
<p>相对于我们提到的其他基于日志复制的一致性算法，Raft 具有更少的消息类型。比如，我们统计了 VR 跟 ZooKeeper 用于基础的一致性及成员变更 <em>(不包含日志压缩及客户端的交互，这些基本是独立于算法存在的)</em> 的消息类型。VR 跟 ZooKeeper 都定义了 10 中不同的消息类型，而 Raft 只有 4 种 <em>(两种请求及两种响应)</em>。 Raft 的消息相对于其他算法的消息也更加的紧凑，但也更加的简单。因为 VR 跟 ZooKeeper 在 <em>Leader</em> 变更时都会进行日志的传输；为了对这些操作做出优化让他们更符合实际，所以一些其他的消息类型是必要的。</p>
<blockquote>
<p>Raft 的强 <em>Leader</em> 机制目的是为了简化算法，但同时也让他排除了一些性能优化方案。比如平等主义的 Paxos  <em>(EPaxos)</em> 实现能够在某些没有 <em>Leader</em> 的条件下得到更高的性能。EPaxos 充分的发挥了状态机指令的可交换性。任意的服务可以在处理其他提议的同时，在单轮通信中完成命令的提交。但是，如果在同时刻没有其他被提议的命令，EPaxos 则需要额外的一轮通信来处理命令。因为每个服务都可能会提交命令，EPaxos 可以在 WAN 网络中实现比 Raft 更好的负载均衡以达到更低的延迟性。但是这为 Paxos 的实现带来很高的复杂性。</p>
</blockquote>
<p>有许多不同的集群成员变更的提议跟实现已经在其他的算法中完成了，包括 Lamport 原本的提议，VR 及 SMART。我们在 Raft 中选择了 <em>Joint Consensus</em> ，因为他对其他一致性协议的其他部分的要求跟影响是更小。Lamport 的 $a$-based 不在 Raft 的选择中是因为他假设一致性能够在没有 <em>Leader</em> 的前提下实现。相较于 VR 跟 SMART， Raft 变更配置算法的有点在于能够在不影响常规操作的前提下完成；他们的差异在于，VR 在配置变更是需要停止所有常规的处理，SMART 则因为了 $a$-like 的方式，限制了能够处理请求的成员数量。Raft 对额外机制的要求也是低于 VR 跟 SMART。</p>
<a class="header" href="print.html#a11-总结-conclution" id="a11-总结-conclution"><h2>11. 总结 Conclution</h2></a>
<p>算法的设计总是会把正确、高效 以及/或者 简洁性作为主要的目标。虽然这些都是有价值的目标，但我们相信可理解性同样重要。其他的目标在开发人员将算法实际实现出来之前都是不现实的，而且他们在实现时必然会跟理论有所偏离。除非开发者对算法有很深入的理解跟很直观的感觉，不然在开发中实现所描述的各个属性是非常困难的。</p>
<p>在本论文中我们介绍了在分布式的一致性算法中被广为接受但让人费解的算法 Paxos，已经有无数的学生跟开发者的经历了它挑战。我们开发了一个新的算法 Raft，并且已经展示了它相较于 Paxos 具有更高的可理解性。我们还相信 Raft 为系统的构建提供了更好的基础。将可理解性作为设计的主要目标改变了我们设计 Raft 的方式；在设计期间我们发现我们复用了一些方法，比如问题分解跟简化状态控件。这些方法不只提高了 Raft 的可理解性，还让我们更容易的说服自己这些设计是正确的。</p>
<a class="header" href="print.html#a12-致谢-acknowledgments" id="a12-致谢-acknowledgments"><h2>12. 致谢 Acknowledgments</h2></a>
<p>这项学习计划需要感谢 Ali Ghodsi, David Mazie`res 跟 伯克利 CS 294-91 及 斯坦福 CS 240 同学们的支持。Scott Klemmer 帮助我们设计了学习计划。Nelson Ray 建议我们使用了一些统计分析的方法。用于学习计划的 Paxos 的幻灯片大量的参考了Lorenzo Alvisi 的成果。特别感谢 DavidMazie 'res and EzraHoch 为我们找出了 Raft 中的隐秘Bug。还有许多的人在论文跟学习计划中给予了非常有用的反馈，包括了 Ed Bugnion, Michael Chan, Hugues Evrard, Daniel Giffin, Arjun Gopalan, Jon Howell, Vimalkumar Jeyakumar, Ankita Kejriwal, Aleksandar Kracun, Amit Levy, Joel Martin, Satoshi Matsushita, Oleg Pesok, David Ramos, Robbert van Renesse, Mendel Rosenblum, Nico- las Schiper, Deian Stefan, Andrew Stone, Ryan Stutsman, David Terei, Stephen Yang, Matei Zaharia, 以及 24 位未公开名字的校对人员。特别感谢我们的引导人 Eddie Kohler。Werner Vogels 在 Twitter 上发布了早期的草稿，给予了 Raft 重要的一次展示机会。This work was supported by the Gigascale Sys- tems Research Center and the Multiscale Systems Cen- ter, two of six research centers funded under the Fo- cus Center Research Program, a Semiconductor Research Corporation program, by STARnet, a Semiconductor Re- search Corporation program sponsored by MARCO and DARPA, by the National Science Foundation under Grant No. 0963859, and by grants from Facebook, Google, Mel- lanox, NEC, NetApp, SAP, and Samsung. Diego Ongaro is supported by The Junglee Corporation Stanford Gradu- ate Fellowship。</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script src="searchindex.js" type="text/javascript" charset="utf-8"></script>
        
        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
